1. bi-directional LSTM 
2. mixture model for non-stationry time series
3. LSTM GRU tensorflow souce code 

# basic model 
# stateful training
# multivariate time series

concatenaed RNN
indiviudal RNN
residual rnn high way layers

dim_per_var 

RNN regularization, mv_cell speed up, 

mv_cell: for stacked lstm

temporal and variate attention

mv_lstmcell speed up 

time decay attention:
  variate specific decay

mv_rnn final speed-up

pure probabilistic model with variance term

dropout set-up

dropout in training and testing phase 

epoch rmse, mae 

hidden activiation added into attention


initilizaer:

   with tf.variable_scope('RNN', initializer=tf.contrib.layers.xavier_initializer()):
      outputs, state = tf.nn.dynamic_rnn(cell, ...)
      
      
normalization the whole or each batch

large regularization

learning rate update in training to handle trapped attention

Tao Lin:
   overfit in baselines
   lr re-initiliaze 

individual dropout

skewed target value distribution affects the performance a lot 


RNN dropout without memory loss

max norm regularization

--- TO DO ---
 
attention value affected by l2 regularization 
 
regularization on attention dropout 
dropout row/column mask 

point-wise loss
point-wise loss, loss reweight 


Experiments:
   synthetic data 

For Lin:
   overfit overcome

Pre-processing:
   outlier removal 
   autocorrelation order

Post-processing:
   attention recording 
   posterior attention on both training and testing data

Regularization:

   Batch norm, Layer norm, Batch Normalized Recurrent Neural Networks
   
   DropConnect
    
   Regularization of Deep Neural Networks with Spectral Dropout
   
   
Attention:
   attenion on exgoneous variable taking into account the hiddens of the target variable 

Model interpretation:
   https://phys.org/news/2017-12-climate-conditions-affect-solar-cell.html
   

Bayesian:
  
  bayesian dropout rnn + attention

  http://andymiller.github.io/2016/12/19/elbo-gradient-estimators.html#fn:2
  https://gist.github.com/VikingPenguinYT/665769ba03115b1a0888893eaf1d4f13
  https://theintelligenceofinformation.wordpress.com/2017/06/02/pydata-london-2017-bayesian-deep-learning-talk-by-andrew-rowan/
  https://gist.github.com/tokestermw/a9de2ef498a09747bbf673ddf6ea4843


REINFORCE:

memory mechanism:
   some improvements on memory moduel in LSTM 

Baseline:
   (NIPS 2016)temporal regularized time series, prophet , python automatic time series forecasting , causalimpact
   prophet 
   # basic model 
   # multivariate time series
   # exponeital weighted moving average 
   gcForest

Metrics:
    RMSE mean, standard errors, willios rank test, predicive likelihood 
    

soft and hart cutoff, impulse function approximate, 

top% accuracy in causal discovery 

model saver, training and testing data, attentions 

Computation time 

Attention mechinism:  time decay: cikm deephawkes paper 


mixed effect in temporal data: e.g. air pollution data: heterogenous 


order effect: chosen order, automatic detected order  


--- experiments:

For causal work:
classification dataset
Wafer: Sensor data collected during the manufacture of semiconductor microelectronics, where the time series are labeled as normal or abnormal.

---air

plain att: 130
sep att: 120








