1. bi-directional LSTM 
2. mixture model for non-stationry time series
3. LSTM GRU tensorflow souce code 

# basic model 
# stateful training
# multivariate time series

concatenaed RNN
indiviudal RNN
residual rnn high way layers

dim_per_var 

RNN regularization, mv_cell speed up, 

mv_cell: for stacked lstm

temporal and variate attention

mv_lstmcell speed up 

time decay attention:
  variate specific decay

mv_rnn final speed-up

pure probabilistic model with variance term

dropout set-up

dropout in training and testing phase 

epoch rmse, mae 

hidden activiation added into attention


initilizaer:

   with tf.variable_scope('RNN', initializer=tf.contrib.layers.xavier_initializer()):
      outputs, state = tf.nn.dynamic_rnn(cell, ...)
      
      
normalization the whole or each batch

large regularization

learning rate update in training to handle trapped attention

Tao Lin:
   overfit in baselines
   lr re-initiliaze 

individual dropout

skewed target value distribution affects the performance a lot 


RNN dropout without memory loss

max norm regularization + dropout

Layer norm: shared or multi-variate


For Lin:
   overfit overcome
   DUAL data no data at t 


Related work:

   ARX model 
   

--- Tuning tricks ---

Observe data to determine window size

Max-norm constraints and dropout together  

divergence point of training and validation errors as the training stopping point

Training slow down:  learning rate, max norm, 

observe tradeoff between representability and generalization ability when hyper-parameter changes 

large network -> high learning rate 


--- TO DO ---
 
LSTM with various regularization
 
attention value affected by l2 regularization 
 
regularization on attention dropout 
dropout row/column mask 

point-wise loss
point-wise loss, loss reweight 


Reference:
   feature importance

Paper write-up:
   reference check
   

Experiments:

   synthetic data 
   https://archive.ics.uci.edu/ml/datasets/SML2010
   
   layer normalization 
   Stack mv lstm
   
   mv_lstm: hidden combination 
   
   predition sample
   
   
   multi-step ahead prediction
   
   prior, poster, negative/positive weights over epoches
   
   the variable importance under different window sizes 

   logs:
      1. temporal attention, variable attention, importance
      2. weight on the output layer


Pre-processing:
   outlier removal 
   autocorrelation order

Post-processing:
   attention recording 
   posterior attention on both training and testing data
   scale-free posterior inference 
   
   select top-k variables, run prediction again
   identify significance different in prior and poster variable attention 
   


Baseline:
   
   (NIPS 2016)temporal regularized time series, prophet , python automatic time series forecasting , causalimpact
   prophet 
   # basic model 
   # multivariate time series
   # exponeital weighted moving average 
   gcForest
   An Interpretable and Sparse Neural Network Model for Nonlinear Granger Causality Discovery
   
   KDD paper MIMC
   
   structural time series
   
   Time series forecasting using a hybrid ARIMA and neural network model
   
   modeling long and short pattern in time series (sigir paper)
   
   AutoTS on R
   
   
Regularization:
   
   Maxout activiation

   DropConnect
    
   Regularization of Deep Neural Networks with Spectral Dropout
   
   Zoneout: regularizing RNNs by randomly preserving hidden activations
   
   Batch norm
   Batch Normalized Recurrent Neural Network
   Recurrent batch normalization
   
   dense regularization on max-norm constrained weights 
   dropout before output layer
   
   max-norm on weight of LSTM
   
   causal regularization 
   

Multi-layer MV-LSTM

Training and tesing importance value comparison, stablize variable importance across epoches 


Attention:

   attenion on exgoneous variable taking into account the hiddens of the target variable 
   
   sparse attention on temporal 
   

Model interpretation:
   https://phys.org/news/2017-12-climate-conditions-affect-solar-cell.html
    

Bayesian:
  
  bayesian dropout rnn + attention

  http://andymiller.github.io/2016/12/19/elbo-gradient-estimators.html#fn:2
  https://gist.github.com/VikingPenguinYT/665769ba03115b1a0888893eaf1d4f13
  https://theintelligenceofinformation.wordpress.com/2017/06/02/pydata-london-2017-bayesian-deep-learning-talk-by-andrew-rowan/
  https://gist.github.com/tokestermw/a9de2ef498a09747bbf673ddf6ea4843



Memory mechanism:
   some improvements on memory moduel in LSTM 



Metrics:
    RMSE, MAE, MAPE 
    standard errors, willios rank test, predicive likelihood 
    

soft and hart cutoff, impulse function approximate, 

top% accuracy in causal discovery 

model saver, training and testing data, attentions 

Computation time 

Attention mechinism:  time decay: cikm deephawkes paper 


mixed effect in temporal data: e.g. air pollution data: heterogenous 


order effect: chosen order, automatic detected order  


--- experiments:

For causal work:
classification dataset
Wafer: Sensor data collected during the manufacture of semiconductor microelectronics, where the time series are labeled as normal or abnormal.

---air

plain att: 130
sep att: 120








