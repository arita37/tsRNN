
---- DONE ----

1. bi-directional LSTM 
2. mixture model for non-stationry time series
3. LSTM GRU tensorflow souce code 

# basic model 
# stateful training
# multivariate time series

concatenaed RNN
indiviudal RNN
residual rnn high way layers

dim_per_var 

RNN regularization, mv_cell speed up, 

mv_cell: for stacked lstm

temporal and variate attention

mv_lstmcell speed up 

time decay attention:
  variate specific decay

mv_rnn final speed-up

pure probabilistic model with variance term

dropout set-up

dropout in training and testing phase 

epoch rmse, mae 

hidden activiation added into attention


initilizaer:

   with tf.variable_scope('RNN', initializer=tf.contrib.layers.xavier_initializer()):
      outputs, state = tf.nn.dynamic_rnn(cell, ...)
      
      
normalization the whole or each batch

large regularization

learning rate update in training to handle trapped attention

Tao Lin:
   overfit in baselines
   lr re-initiliaze 

individual dropout

skewed target value distribution affects the performance a lot 


RNN dropout without memory loss

max norm regularization + dropout

Layer norm: shared or multi-variate


For Lin:
   overfit overcome
   DUAL data no data at t 


Related work:

   ARX model 
   
pseudo likelihood

dropout on output layer

tensordot operation 

  position of variable attention
  
effect of attention regularization

 learning rate decay

   epoch log 

   synthetic data 
   https://archive.ics.uci.edu/ml/datasets/SML2010
   
   Stack mv lstm

Reading named command arguments via argparse

LSTM with various regularization

log: temporal and variable attention log in text

temporal attention sensitivity 

variable attention 

regularization on attention

learning rate decay 

regularization on attention 
dropout row/column mask 


Multi-layer MV-LSTM

Training and tesing importance value comparison, stablize variable importance across epoches 


Model interpretation:
   https://phys.org/news/2017-12-climate-conditions-affect-solar-cell.html
   

Memory mechanism:
   some improvements on memory moduel in LSTM 

  test mode

---- Tuning tricks ----

-- convergence: learning rate, batch size, network size 
-- underfitting: network size
-- overfitting: regularization strength, batch size, network size, target loss v.s. regularization


Observe data to determine window size

Max-norm constraints and dropout together  

divergence point of training and validation errors as the training stopping point

Training slow down: learning rate, max norm, batch size 

observe tradeoff between representability and generalization ability when hyper-parameter changes 

large network -> high learning rate 

   
---- TO DO ----

MULTI-TASK 

INTERPRETABILITY:

  LIME
  DeepLift
  
  SHARP
  
NORMALIZATION:
  
  batch normalization
  weight normalization

ATTENTION:
  
  multi-head attention
  global local attention regularization 
  
EXPERIMENTS:

   Retraining
   
   EM process

   Order: loss function, normalization on Y, regularization, attention 
   
   observe hidden state variation in RNN
   
   only with dropout, no dropout
   
   layer normalization 
   
   mv_lstm: hidden combination 
   
   predition sample
   
   multi-step ahead prediction
   
   prior, poster, negative/positive weights over epoches
   
   the variable importance under different window sizes 

   logs:
      1. temporal attention, variable attention, importance
      2. weight on the output layer

   y_normalization function
  
   attention:  
     
     attention regularization
     
     attenion on exgoneous variable taking into account the hiddens of the target variable 
     
     sparse attention on temporal
     
  Laplace posterior smoothing, KL-divergency and Laplace smoothing 
  
  log-sum-exp trick

  experiments: noisy input variables  
               compare with linear correlation, pearson 
                             
  optimization techniques:
               batch/layer normalization

REFERENCE: 

  https://beenkim.github.io/
  feature/variable importance
  
STATISTIC:

  https://ermongroup.github.io/cs228-notes/

  https://dandelion.eu/datamine/open-big-data/

  http://www.johnwittenauer.net/a-simple-time-series-analysis-of-the-sp-500-index/

  http://nbviewer.jupyter.org/gist/ChadFulton/5127108f4c7025ed2648

  Probabilistic Programming and Bayesian Methods for Hackers:
  http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-     
  Hackers/blob/master/Prologue/Prologue.ipynb

LOSS FUNCTION:

  corss-check varialbe ranking or network 

  distributional loss: regression 
  risk minimization 

  llk + squared loss
  quantile regression
  earth mover distance
  
  point-wise loss
  point-wise loss, loss reweight
  
 
PRE-PROCESSING:

   outlier removal 
   autocorrelation order

POST-PROCESSING:

   attention recording 
   posterior attention on both training and testing data
   scale-free posterior inference 
   
   select top-k variables, run prediction again
   identify significance different in prior and poster variable attention 

BASELINE:

   RETAIN and DUAL: instance-wise aggregate 

   Multilevel Wavelet Decomposition Network for Interpretable Time Series Analysis

   Autoregressive Convolutional Neural Networks for Asynchronous Time Series
   
   (NIPS 2016)temporal regularized time series, prophet, python automatic time series forecasting , causalimpact
   prophet 
   # basic model 
   # multivariate time series
   # exponeital weighted moving average 
   gcForest
   An Interpretable and Sparse Neural Network Model for Nonlinear Granger Causality Discovery
   
   KDD paper MIMC
   
   structural time series
   
   Time series forecasting using a hybrid ARIMA and neural network model
   
   modeling long and short pattern in time series (sigir paper)
   
   AutoTS on R
   
   
REGULARIZATION:
   
   Maxout activiation

   DropConnect
    
   Regularization of Deep Neural Networks with Spectral Dropout
   
   Zoneout: regularizing RNNs by randomly preserving hidden activations
   
   Batch norm
   Batch Normalized Recurrent Neural Network
   Recurrent batch normalization
   
   dense regularization on max-norm constrained weights 
   dropout before output layer
   
   max-norm on weight of LSTM
   
   causal regularization 
    

BAYESIAN:
  
  bayesian dropout rnn + attention

  http://andymiller.github.io/2016/12/19/elbo-gradient-estimators.html#fn:2
  https://gist.github.com/VikingPenguinYT/665769ba03115b1a0888893eaf1d4f13
  https://theintelligenceofinformation.wordpress.com/2017/06/02/pydata-london-2017-bayesian-deep-learning-talk-by-andrew-rowan/
  https://gist.github.com/tokestermw/a9de2ef498a09747bbf673ddf6ea4843


METRICES:
    RMSE, MAE, MAPE 
    standard errors, willios rank test, predicive likelihood 
    

soft and hart cutoff, impulse function approximate, 

top% accuracy in causal discovery 

model saver, training and testing data 

Computation time 


Attention mechinism:  time decay: cikm deephawkes paper 


mixed effect in temporal data: e.g. air pollution data: heterogenous 


order effect: chosen order, automatic detected order  


---- experiments: ----

For causal work:
   classification dataset
   Wafer: Sensor data collected during the manufacture of semiconductor microelectronics, where the time series are labeled as    normal or abnormal.








