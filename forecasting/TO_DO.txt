1. bi-directional LSTM 
2. mixture model for non-stationry time series
3. LSTM GRU tensorflow souce code 

# basic model 
# stateful training
# multivariate time series

concatenaed RNN
indiviudal RNN
residual rnn high way layers

dim_per_var 

RNN regularization, mv_cell speed up, 

mv_cell: for stacked lstm

temporal and variate attention

mv_lstmcell speed up 

time decay attention:
  variate specific decay

mv_rnn final speed-up

pure probabilistic model with variance term

dropout set-up

dropout in training and testing phase 

epoch rmse, mae 

hidden activiation added into attention


initilizaer:

   with tf.variable_scope('RNN', initializer=tf.contrib.layers.xavier_initializer()):
      outputs, state = tf.nn.dynamic_rnn(cell, ...)
      
      
normalization the whole or each batch

large regularization

learning rate update in training to handle trapped attention

Tao Lin:
   overfit in baselines
   lr re-initiliaze 

individual dropout

skewed target value distribution affects the performance a lot 


RNN dropout without memory loss

max norm regularization + dropout

Layer norm: shared or multi-variate


--- Tuning tricks ---

Observe data to determine window size

Max-norm constraints and dropout together  

divergence point of training and validation errors as the training stopping point

Training slow down:  learning rate, max norm, 

observe tradeoff between representability and generalization ability when hyper-parameter changes 

--- TO DO ---
 
LSTM with various regularization
 
attention value affected by l2 regularization 
 
regularization on attention dropout 
dropout row/column mask 

point-wise loss
point-wise loss, loss reweight 
                                                   

Related work:
   ARX model 

Experiments:
   synthetic data 
   https://archive.ics.uci.edu/ml/datasets/SML2010

For Lin:
   overfit overcome
   DUAL data no data at t 

Pre-processing:
   outlier removal 
   autocorrelation order

Post-processing:
   attention recording 
   posterior attention on both training and testing data


Baseline:
   (NIPS 2016)temporal regularized time series, prophet , python automatic time series forecasting , causalimpact
   prophet 
   # basic model 
   # multivariate time series
   # exponeital weighted moving average 
   gcForest
   An Interpretable and Sparse Neural Network Model for Nonlinear Granger Causality Discovery
   KDD paper MIMC
   
   structural time series
   
   
Regularization:
   
   Maxout activiation

   DropConnect
    
   Regularization of Deep Neural Networks with Spectral Dropout
   
   Zoneout: regularizing RNNs by randomly preserving hidden activations
   
   Batch norm
   Batch Normalized Recurrent Neural Network
   Recurrent batch normalization
   
   dense regularization on max-norm constrained weights 
   dropout before output layer
   
   max-norm on weight in LSTM

Multi-layer MV-LSTM


Attention:
   attenion on exgoneous variable taking into account the hiddens of the target variable 

Model interpretation:
   https://phys.org/news/2017-12-climate-conditions-affect-solar-cell.html
    

Bayesian:
  
  bayesian dropout rnn + attention

  http://andymiller.github.io/2016/12/19/elbo-gradient-estimators.html#fn:2
  https://gist.github.com/VikingPenguinYT/665769ba03115b1a0888893eaf1d4f13
  https://theintelligenceofinformation.wordpress.com/2017/06/02/pydata-london-2017-bayesian-deep-learning-talk-by-andrew-rowan/
  https://gist.github.com/tokestermw/a9de2ef498a09747bbf673ddf6ea4843


REINFORCE:

Memory mechanism:
   some improvements on memory moduel in LSTM 



Metrics:
    RMSE mean, standard errors, willios rank test, predicive likelihood 
    

soft and hart cutoff, impulse function approximate, 

top% accuracy in causal discovery 

model saver, training and testing data, attentions 

Computation time 

Attention mechinism:  time decay: cikm deephawkes paper 


mixed effect in temporal data: e.g. air pollution data: heterogenous 


order effect: chosen order, automatic detected order  


--- experiments:

For causal work:
classification dataset
Wafer: Sensor data collected during the manufacture of semiconductor microelectronics, where the time series are labeled as normal or abnormal.

---air

plain att: 130
sep att: 120








