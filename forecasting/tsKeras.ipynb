{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data processing packages\n",
    "import numpy as np   \n",
    "import pandas as pd \n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    "\n",
    "import random\n",
    "\n",
    "# machine leanring packages\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "# RMSprop, Adadelta\n",
    "from keras.regularizers import *\n",
    "from keras.initializers import *\n",
    "from keras.activations import *\n",
    "\n",
    "from utils_keras import *\n",
    "from utils_dataPrepro import *\n",
    "\n",
    "# from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.merge import *\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "\n",
    "# ---Ini:            \n",
    "#     orthogonal: https://smerity.com/articles/2016/orthogonal_init.html\n",
    "#     identity, Xavier, He's\n",
    "\n",
    "# ---Activation:     \n",
    "#     tan, sigmod by default\n",
    "\n",
    "# ---Regularization: \n",
    "#     dropout on non-recurrent connections \n",
    "#     batch normalization\n",
    "\n",
    "# unstable or not decreasing training loss: lr, representability, e.g., number of neursons, layers, etc..\n",
    "# watch out for the amount difference between regularization and loss  \n",
    "# large minibatch -> large lr\n",
    "# large network -> large lr\n",
    "\n",
    "\n",
    "# learning speed, repren, regular\n",
    "\n",
    "# TO DO:\n",
    "    \n",
    "# more component\n",
    "# muliti-input-muliti-output method\n",
    "# log transformation\n",
    "# integrate trend \n",
    "# attention for peridoic time series\n",
    "\n",
    "# multistep ahead:  multi-output rnn, a.k.a. state space model \n",
    "#                   seq2seq rnn \n",
    "\n",
    "# Stat-space model: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files_list=[\"../../dataset/dataset_ts/air_xtrain.dat\", \\\n",
    "            \"../../dataset/dataset_ts/air_xtest.dat\",\\\n",
    "            \"../../dataset/dataset_ts/air_ytrain.dat\", \\\n",
    "            \"../../dataset/dataset_ts/air_ytest.dat\"]\n",
    "\n",
    "xtrain, ytrain, xtest, ytest, tr_shape, ts_shape = \\\n",
    "prepare_train_test_data( True, files_list)\n",
    "\n",
    "\n",
    "xtrain = np.reshape( xtrain, (tr_shape[0], tr_shape[1]-1, -1) )\n",
    "ytrain = np.reshape( ytrain, (tr_shape[0], 1) ) \n",
    "xtest  = np.reshape( xtest,  (ts_shape[0], ts_shape[1]-1, -1) )\n",
    "ytest = np.reshape(  ytest,  (ts_shape[0], 1) )\n",
    "\n",
    "print np.shape(xtrain), np.shape(ytrain), np.shape(xtest), np.shape(ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_raw_diff(x):\n",
    "    cnt = len( list(x) )\n",
    "    steps = len(x[0])\n",
    "    \n",
    "    raw  = []\n",
    "    diff = []\n",
    "    last = []\n",
    "    \n",
    "    for i in xrange(cnt):\n",
    "        raw.append( [j[0] for j in x[i]] )\n",
    "        last.append( x[i][steps-1][0] )\n",
    "        diff.append( [j[1] for j in x[i]] )\n",
    "\n",
    "    return np.asarray(raw), np.asarray(diff), np.asarray(last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tr, diff_tr, last_tr = split_raw_diff(xtrain)\n",
    "raw_ts, diff_ts, last_ts = split_raw_diff(xtest)\n",
    "\n",
    "raw_tr  = np.reshape( raw_tr,  (tr_shape[0], tr_shape[1]-1, -1) )\n",
    "diff_tr = np.reshape( diff_tr, (tr_shape[0], tr_shape[1]-1, -1) )\n",
    "last_tr = np.reshape( last_tr, (tr_shape[0],  -1) )\n",
    "\n",
    "raw_ts  = np.reshape( raw_ts,  (ts_shape[0], ts_shape[1]-1, -1) )\n",
    "diff_ts = np.reshape( diff_ts, (ts_shape[0], ts_shape[1]-1, -1) )\n",
    "last_ts = np.reshape( last_ts, (ts_shape[0],  -1) )\n",
    "\n",
    "ytrain = np.reshape( ytrain, (tr_shape[0], 1) ) \n",
    "ytest = np.reshape(  ytest,  (ts_shape[0], 1) )\n",
    "\n",
    "print np.shape(raw_tr), np.shape(diff_tr), np.shape(last_tr)\n",
    "print np.shape(raw_ts), np.shape(diff_ts), np.shape(last_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plain discriminative \n",
    "\n",
    "# network set-up\n",
    "in_out_neurons = 1\n",
    "hidden_neurons = 256\n",
    "win_size = 200\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "disc_xtrain = np.reshape( xtrain, [-1, win_size, in_out_neurons] )\n",
    "disc_xtest  = np.reshape( xtest,  [-1, win_size, in_out_neurons] )\n",
    "\n",
    "disc_ytrain = ytrain\n",
    "disc_ytest  = ytest\n",
    "\n",
    "print np.shape(disc_xtrain),np.shape(disc_ytrain),np.shape(disc_xtest),\\\n",
    "np.shape(disc_ytest)\n",
    "\n",
    "# optimizer\n",
    "sgd  = SGD(lr = 0.01, momentum = 0.9, nesterov = True)\n",
    "rms  = RMSprop(lr = 0.05,  rho = 0.9, epsilon  = 1e-08, decay = 0.0)\n",
    "\n",
    "adam = Adam(lr = 0.005, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add( LSTM(hidden_neurons, input_dim = in_out_neurons, return_sequences = False, \\\n",
    "                input_shape = [batch_size, win_size, in_out_neurons ], \\\n",
    "#               input_length = win_size, \\\n",
    "#               activation ='tanh',\\\n",
    "#               dropout = 0.1,\\\n",
    "#               kernel_regularizer = l2(0.2), \n",
    "#               recurrent_regularizer = l2(0.1),\\\n",
    "#               !change!\n",
    "                kernel_initializer    = glorot_normal(), \\\n",
    "                recurrent_initializer = glorot_normal() ))\n",
    "# change: activiation\n",
    "\n",
    "# model.add( LSTM(hidden_neurons, return_sequences = False, \\\n",
    "# #               input_shape = [batch_size, win_size, in_out_neurons ], \\\n",
    "# #               input_length = win_size, \\\n",
    "#               activation ='tanh',\\\n",
    "# #               dropout = 0.1,\\\n",
    "# #               kernel_regularizer = l2(0.2), \n",
    "# #               recurrent_regularizer = l2(0.1),\\\n",
    "# #               !change!\n",
    "#                 kernel_initializer    = glorot_normal(), \\\n",
    "#                 recurrent_initializer = glorot_normal() ))\n",
    "\n",
    "\n",
    "# identity ini\n",
    "# he's ini:   he_normal()\n",
    "# xavier ini: glorot_normal()\n",
    "# orthogonal: Orthogonal()\n",
    "\n",
    "\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(128,\\\n",
    "#                   kernel_regularizer = l2(0.01), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(64, \\\n",
    "#                   kernel_regularizer = l2(0.01), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(32, \\\n",
    "#                   kernel_regularizer = l2(0.01), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(1,  \\\n",
    "#                   kernel_regularizer = l2(0.01), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"linear\"))\n",
    "\n",
    "\n",
    "# training\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = adam)\n",
    "\n",
    "\n",
    "model.fit( disc_xtrain, disc_ytrain, shuffle = True, \\\n",
    "           callbacks = [ \\\n",
    "           TestCallback( (disc_xtest, disc_ytest), (disc_xtrain, disc_ytrain) ) ], \\\n",
    "           batch_size = batch_size, epochs = 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative model\n",
    "\n",
    "# set-up\n",
    "in_out_neurons = 1\n",
    "hidden_neurons = 256\n",
    "win_size = 200    \n",
    "    \n",
    "# prepare data\n",
    "ytrain = expand_y( xtrain_df.as_matrix(), ytrain_df.as_matrix() )\n",
    "ytest  = expand_y( xtest_df.as_matrix(),  ytest_df.as_matrix()  )\n",
    "\n",
    "gen_xtrain = np.reshape( xtrain, [-1, win_size, 1] )\n",
    "gen_ytrain = np.reshape( ytrain, [-1, win_size, 1] )\n",
    "\n",
    "gen_xtest  = np.reshape( xtest, [-1, win_size, 1] )\n",
    "gen_ytest  = np.reshape( ytest, [-1, win_size, 1] )\n",
    "\n",
    "print np.shape(gen_xtrain), np.shape(gen_ytrain), np.shape(gen_xtest), np.shape(gen_ytest)\n",
    "\n",
    "\n",
    "# optimizer \n",
    "adam  = Adam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add( LSTM(hidden_neurons, input_dim = in_out_neurons, return_sequences = True, \\\n",
    "                input_length = win_size,\\\n",
    "                activation   = 'tanh',\\\n",
    "#                dropout = 0.1,\\\n",
    "#                kernel_regularizer = l2(0.15), \\\n",
    "#                recurrent_regularizer = l2(0.15), \\\n",
    "                kernel_initializer    = glorot_normal(), \\\n",
    "                recurrent_initializer = glorot_normal() ) )\n",
    "\n",
    "\n",
    "# identity ini\n",
    "# he's ini:   he_normal()\n",
    "# xavier ini: glorot_normal()\n",
    "# orthogonal: Orthogonal() \n",
    "\n",
    "\n",
    "# model.add(Dropout(0.1))\n",
    "model.add(TimeDistributed(Dense(128, activation = 'relu',\\\n",
    "#                                     kernel_regularizer = l2(0.1), \\\n",
    "                                    kernel_initializer = he_normal() ) ))\n",
    "\n",
    "model.add(TimeDistributed(Dense(64, activation = 'relu',\\\n",
    "#                                     kernel_regularizer = l2(0.01), \\\n",
    "                                    kernel_initializer = he_normal() ) ))\n",
    "\n",
    "model.add(TimeDistributed(Dense(1,  activation = 'linear',\\\n",
    "#                                     kernel_regularizer = l2(0.01), \\\n",
    "                                    kernel_initializer = he_normal() ) ))\n",
    "\n",
    "\n",
    "# training\n",
    "model.compile( loss = \"mean_squared_error\", optimizer = adam )\n",
    "\n",
    "\n",
    "model.fit( gen_xtrain, gen_ytrain, shuffle=True,  \\\n",
    "           callbacks = [ TestCallback_Generative( \\\n",
    "                         (gen_xtest, gen_ytest), (gen_xtrain, gen_ytrain) ) ], \\\n",
    "                         batch_size = 128, epochs = 500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State space model \n",
    "# with non-overlapping local data\n",
    "\n",
    "#  network set-up\n",
    "local_size = 5\n",
    "in_out_neurons = local_size\n",
    "hidden_neurons = 512\n",
    "win_size = 200/5\n",
    "\n",
    "# # validation on each epoch \n",
    "# class TestCallback_insight(Callback):\n",
    "#     def __init__(self, test_data):\n",
    "#         self.test_data = test_data\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         x, y = self.test_data\n",
    "#         loss, mse = self.model.evaluate(x, y, verbose=0)\n",
    "        \n",
    "#         py = self.model.predict(x, verbose=0)\n",
    "#         cnt = len(y)\n",
    "#         err = [ (y[i][0]-py[i][0])**2 for i in range(cnt)]\n",
    "        \n",
    "#         print('\\nTesting loss: {}, acc: {}, mean: {} \\n'.format(\\\n",
    "#                loss, sqrt(mse), mean(err) ))\n",
    "\n",
    "def expand_x_local_non_overlapping( local_size, list_data):\n",
    "    \n",
    "    cnt = len(list_data)\n",
    "    steps = len(list_data[0])\n",
    "    tmp_dta = []\n",
    "    \n",
    "    for i in range(cnt):\n",
    "        tmp_dta.append([])\n",
    "        for j in range( local_size-1, steps, local_size ):\n",
    "            tmp_dta[-1].append( list_data[i][ j-local_size+1:j+1 ] )\n",
    "    \n",
    "    return tmp_dta\n",
    "\n",
    "\n",
    "disc_xtrain = np.array( expand_x_local_non_overlapping( local_size, list(xtrain) ) )\n",
    "disc_xtest  = np.array( expand_x_local_non_overlapping( local_size, list(xtest) ) )\n",
    "\n",
    "disc_ytrain = ytrain\n",
    "disc_ytest  = ytest\n",
    "\n",
    "print np.shape(disc_xtrain), np.shape(disc_ytrain), np.shape(disc_xtest), \\\n",
    "np.shape(disc_ytest)\n",
    "\n",
    "# optimizer \n",
    "adam = Adam(lr = 0.007, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_neurons, input_dim = in_out_neurons, return_sequences = False, \\\n",
    "               input_length = win_size, \\\n",
    "               activation = 'tanh', \\\n",
    "#                dropout = 0.1,\\\n",
    "#                kernel_regularizer = l2(0.1),      recurrent_regularizer = l2(0.1), \\\n",
    "               kernel_initializer    = glorot_normal(), \\\n",
    "               recurrent_initializer = glorot_normal() ))\n",
    "# identity ini\n",
    "# he's ini:   he_normal()\n",
    "# xavier ini: glorot_normal()\n",
    "# orthogonal: Orthogonal() \n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# kernel_regularizer = l2(0.1), \\\n",
    "model.add(Dense(128,\\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(64, \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(32, \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(1,\\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"linear\"))\n",
    "\n",
    "\n",
    "# training \n",
    "model.compile(loss=\"mean_squared_error\", optimizer = adam)\n",
    "\n",
    "\n",
    "model.fit( disc_xtrain, disc_ytrain, shuffle=True, \\\n",
    "           callbacks = [ TestCallback( (disc_xtest, disc_ytest), (disc_xtrain, disc_ytrain) ) ], \\\n",
    "           batch_size = 256, epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation on each epoch \n",
    "class TestCallback_ada(Callback):\n",
    "    def __init__(self, test_data, train_data):\n",
    "        \n",
    "        self.test_data  = test_data        \n",
    "        self.train_data = train_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        raw_ts, diff_ts, last_ts, y_ts = self.test_data\n",
    "        raw_tr, diff_tr, last_tr, y_tr = self.train_data\n",
    "        \n",
    "        py_tr = self.model.predict( [raw_tr, diff_tr, last_tr], verbose=0)\n",
    "        py_ts = self.model.predict( [raw_ts, diff_ts, last_ts], verbose=0)\n",
    "        \n",
    "        size_tr = len(y_tr)\n",
    "        size_ts = len(y_ts)\n",
    "        \n",
    "        err_tr = [ (y_tr[i][0] - py_tr[i][0])**2 for i in range(size_tr) ]\n",
    "        err_ts = [ (y_ts[i][0] - py_ts[i][0])**2 for i in range(size_ts) ]\n",
    "        \n",
    "        loss = self.model.evaluate([raw_ts, diff_ts, last_ts], y_ts, verbose=0)\n",
    "        \n",
    "        with open(\"res/tsRnn_ada.txt\", \"a\") as text_file:\n",
    "            text_file.write(\"At epoch %d: loss %f, train %f, test %f\\n\" % ( epoch, loss, sqrt(mean(err_tr)),\\\n",
    "                                                                           sqrt(mean(err_ts))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--- PARAMETERS ---\n",
    "N_DIM = 1\n",
    "N_RNN = 64\n",
    "\n",
    "LR = 0.005\n",
    "\n",
    "WIN_SIZE = 200\n",
    "\n",
    "MODEL_CHECK_PERIOD = 2\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# --- Prepare ----\n",
    "\n",
    "input_raw  = Input(shape = ( WIN_SIZE -1, 1 ), dtype='float32', name='input_raw')\n",
    "input_diff = Input(shape = ( WIN_SIZE -1, 1 ), dtype='float32', name='input_diff')\n",
    "# smoothing last values\n",
    "input_last = Input(shape = ( 1, ), dtype='float32', name='input_last')\n",
    "\n",
    "with open(\"res/tsRnn_ada.txt\", \"w\") as text_file:\n",
    "    text_file.close()\n",
    "        \n",
    "# --- build the network ---    \n",
    "\n",
    "def difference(pair_of_tensors):\n",
    "    x, y = pair_of_tensors\n",
    "    return x - y\n",
    "\n",
    "def one_minus(tensor):\n",
    "    \n",
    "    return K.ones(1) - tensor\n",
    "    \n",
    "#     return [1.0-i for i in tensor]\n",
    "\n",
    "hidden_raw  = LSTM(N_RNN)( input_raw )\n",
    "output_raw  = Dense(1, activation='relu')(hidden_raw)\n",
    "\n",
    "hidden_diff = LSTM(N_RNN)( input_diff )\n",
    "output_diff = Dense(1, activation='relu')(hidden_diff)\n",
    "output_diff = Add()( [output_diff, input_last] )\n",
    "\n",
    "weight_var = Lambda(difference)([output_diff, output_raw])\n",
    "\n",
    "diff_prob = Dense(1, activation='sigmoid')( weight_var )\n",
    "weighted_diff = merge([output_diff, diff_prob], name='weighted_diff', mode='mul')\n",
    "\n",
    "raw_prob = Lambda(one_minus)([diff_prob])\n",
    "weighted_raw  = merge([output_raw, raw_prob], name='attention_raw', mode='mul')\n",
    "\n",
    "output_main = Add()( [weighted_diff, weighted_raw] )\n",
    "\n",
    "\n",
    "# --- training ---\n",
    "\n",
    "model = Model(inputs=[ input_raw, input_diff, input_last], outputs=[ output_main ])\n",
    "\n",
    "adam = Adam(lr = LR , beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "\n",
    "model.compile(optimizer = adam, loss='mean_squared_error')\n",
    "\n",
    "filepath=\"res/model/weights-{epoch:02d}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath, verbose=0, save_best_only=False, save_weights_only=False,\\\n",
    "                               period=MODEL_CHECK_PERIOD)\n",
    "\n",
    "model.fit([raw_tr, diff_tr, last_tr], [ytrain], epochs = 500, batch_size = BATCH_SIZE,\\\n",
    "          callbacks = \\\n",
    "          [TestCallback_ada( [ raw_tr, diff_tr, last_tr, ytrain ], \\\n",
    "                             [ raw_ts, diff_ts, last_ts, ytest ] ), checkpointer ])\n",
    "                  \n",
    "\n",
    "# # load weights\n",
    "# model.load_weights(\"weights.best.hdf5\")\n",
    "# # Compile model (required to make predictions)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
