{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_Find' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9ab9c3c2e953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_Find' is not defined"
     ]
    }
   ],
   "source": [
    "# An alternative to tf.nn.rnn_cell._linear function, which has been removed in Tensorfow 1.0.1\n",
    "# The highway layer is borrowed from https://github.com/mkroutikov/tf-lstm-char-cnn\n",
    "def linear(input_, output_size, scope=None):\n",
    "    '''\n",
    "    Linear map: output[k] = sum_i(Matrix[k, i] * input_[i] ) + Bias[k]\n",
    "    Args:\n",
    "    input_: a tensor or a list of 2D, batch x n, Tensors.\n",
    "    output_size: int, second dimension of W[i].\n",
    "    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "  Returns:\n",
    "    A 2D Tensor with shape [batch x output_size] equal to\n",
    "    sum_i(input_[i] * W[i]), where W[i]s are newly created matrices.\n",
    "  Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  '''\n",
    "\n",
    "    shape = input_.get_shape().as_list()\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
    "    if not shape[1]:\n",
    "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
    "    input_size = shape[1]\n",
    "\n",
    "    # Now the computation.\n",
    "    with tf.variable_scope(scope or \"SimpleLinear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [output_size, input_size], dtype=input_.dtype)\n",
    "        bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
    "\n",
    "    return tf.matmul(input_, tf.transpose(matrix)) + bias_term\n",
    "\n",
    "class myGRUCell(RNNCell):\n",
    "  \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\n",
    "  Args:\n",
    "    num_units: int, The number of units in the GRU cell.\n",
    "    activation: Nonlinearity to use.  Default: `tanh`.\n",
    "    reuse: (optional) Python boolean describing whether to reuse variables\n",
    "     in an existing scope.  If not `True`, and the existing scope already has\n",
    "     the given variables, an error is raised.\n",
    "    kernel_initializer: (optional) The initializer to use for the weight and\n",
    "    projection matrices.\n",
    "    bias_initializer: (optional) The initializer to use for the bias.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_units,\n",
    "               activation=None,\n",
    "               reuse=None,\n",
    "               kernel_initializer=None,\n",
    "               bias_initializer=None):\n",
    "    super(GRUCell, self).__init__(_reuse=reuse)\n",
    "    self._num_units = num_units\n",
    "    self._activation = activation or math_ops.tanh\n",
    "    self._kernel_initializer = kernel_initializer\n",
    "    self._bias_initializer = bias_initializer\n",
    "    self._gate_linear = None\n",
    "    self._candidate_linear = None\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  def call(self, inputs, state):\n",
    "    \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\"\n",
    "    if self._gate_linear is None:\n",
    "      bias_ones = self._bias_initializer\n",
    "      if self._bias_initializer is None:\n",
    "        bias_ones = init_ops.constant_initializer(1.0, dtype=inputs.dtype)\n",
    "      with vs.variable_scope(\"gates\"):  # Reset gate and update gate.\n",
    "        self._gate_linear = _Linear(\n",
    "            [inputs, state],\n",
    "            2 * self._num_units,\n",
    "            True,\n",
    "            bias_initializer=bias_ones,\n",
    "            kernel_initializer=self._kernel_initializer)\n",
    "\n",
    "    value = math_ops.sigmoid(self._gate_linear([inputs, state]))\n",
    "    r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)\n",
    "\n",
    "    r_state = r * state\n",
    "    if self._candidate_linear is None:\n",
    "      with vs.variable_scope(\"candidate\"):\n",
    "        self._candidate_linear = _Linear(\n",
    "            [inputs, r_state],\n",
    "            self._num_units,\n",
    "            True,\n",
    "            bias_initializer=self._bias_initializer,\n",
    "            kernel_initializer=self._kernel_initializer)\n",
    "    c = self._activation(self._candidate_linear([inputs, r_state]))\n",
    "    new_h = u * state + (1 - u) * c\n",
    "    return new_h, new_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminative\n",
    "\n",
    "class tsLSTM_discriminative():\n",
    "    \n",
    "    def __init__(self, n_lstm_dim, n_steps, n_data_dim, n_lstm_layers, session,\\\n",
    "                 lr, l2, max_norm, bool_is_stateful, n_batch_size ):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "        self.L2 =  l2\n",
    "        \n",
    "        self.N_LSTM_LAYERS = n_lstm_layers\n",
    "        self.N_LSTM_DIM    = n_lstm_dim\n",
    "        \n",
    "        self.N_STEPS    = n_steps\n",
    "        self.N_DATA_DIM = n_data_dim\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [None, self.N_STEPS, self.N_DATA_DIM])\n",
    "        self.y = tf.placeholder(tf.float32, [None, 1])\n",
    "        \n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.is_state_full = bool_is_stateful\n",
    "        self.n_batch_size = n_batch_size\n",
    "        \n",
    "        self.sess = session\n",
    "    \n",
    "        with tf.variable_scope(\"lstm\"):\n",
    "            \n",
    "#           !!change\n",
    "            lstm_cell    = tf.contrib.rnn.GRUCell( self.N_LSTM_DIM )\n",
    "#           state_is_tuple = True\n",
    "            \n",
    "            stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell]* self.N_LSTM_LAYERS,\\\n",
    "                                                       state_is_tuple=True )\n",
    "            \n",
    "            self.hiddens, self.state = tf.nn.dynamic_rnn(cell = stacked_lstm,\\\n",
    "                                                         inputs = self.x,\\\n",
    "                                                         dtype = tf.float32)\n",
    "            \n",
    "        tmp_hiddens = tf.transpose( self.hiddens, [1,0,2]  )\n",
    "        last_hidden = tmp_hiddens[-1]\n",
    "        \n",
    "        with tf.variable_scope(\"hidden\"):\n",
    "                \n",
    "#           change  orthogonal ini\n",
    "            w = tf.Variable(tf.random_normal([self.N_LSTM_DIM, 128],\\\n",
    "                            stddev=math.sqrt(2.0/self.N_LSTM_DIM)))\n",
    "            b = tf.Variable(tf.zeros( [128] ))\n",
    "            \n",
    "            self.regularization = tf.nn.l2_loss(w)\n",
    "            \n",
    "            h = tf.matmul(last_hidden, w) + b\n",
    "            h = tf.nn.relu( h )\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w = tf.Variable(tf.random_normal([128, 1],\\\n",
    "                            stddev=math.sqrt(2.0/128)))\n",
    "            b = tf.Variable(tf.zeros( [ 1 ] ))\n",
    "            \n",
    "            self.regularization += tf.nn.l2_loss(w)\n",
    "            \n",
    "            self.py = tf.matmul(h, w) + b\n",
    "    \n",
    "    def train_ini(self):  \n",
    "        \n",
    "#       !!! change\n",
    "        self.cost = tf.reduce_mean( tf.square(self.y - self.py) ) \n",
    "#     + self.L2*self.regularization\n",
    "        \n",
    "        self.optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)  \n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)  \n",
    "#         !! same lr, converge faster\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "#       in addition to ini op, initialize the state vector of RNN\n",
    "        self.sess.run( [self.init, self.state_variables] )\n",
    "        \n",
    "        \n",
    "    def train_batch(self, x_batch, y_batch, keep_prob ):\n",
    "        \n",
    "        _, c, _ = self.sess.run([self.optimizer, self.cost, self.state_update_op],\\\n",
    "                        feed_dict={self.x:x_batch,\\\n",
    "                                   self.y:y_batch,\\\n",
    "                                   self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "            \n",
    "        return c\n",
    "\n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "\n",
    "#       denormalzied RMSE  \n",
    "        self.rmse = \\\n",
    "        tf.sqrt( tf.reduce_mean( tf.square( self.y - self.py ) ) )\n",
    "        \n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, x_test, y_test, keep_prob):\n",
    "        return self.sess.run([self.rmse], feed_dict={self.x:x_test,\\\n",
    "                                                self.y:y_test,\\\n",
    "                                                self.keep_prob:keep_prob\\\n",
    "                                                })\n",
    "        \n",
    "   \n",
    "    def test(self, x_test, y_test ):\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        tmpshape = tf.shape(self.state)\n",
    "        self.sess.run( self.init )\n",
    "        \n",
    "        return self.sess.run( [ tmpshape ], \\\n",
    "                 feed_dict={self.x:x_test, self.y:y_test})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discriminative\n",
    "\n",
    "disc_xtrain = np.reshape( xtrain, [-1, 100, 1] )\n",
    "disc_xtest  = np.reshape( xtest, [-1, 100, 1] )\n",
    "\n",
    "disc_ytrain = ytrain\n",
    "disc_ytest  = ytest\n",
    "\n",
    "print np.shape(disc_xtrain), np.shape(disc_ytrain), np.shape(disc_xtest), \\\n",
    "np.shape(disc_ytest)\n",
    "\n",
    "\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_lr = 0.001\n",
    "para_lstm_dim = 256\n",
    "para_lstm_layers = 1\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_l2 = 0.1\n",
    "\n",
    "para_max_norm  = 4\n",
    "para_keep_prob = 0.8\n",
    "\n",
    "# fixed parameters\n",
    "para_data_dim = 1\n",
    "para_steps = 100\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "\n",
    "# reset the environment\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    reg = tsLSTM_discriminative(para_lstm_dim, para_steps, para_data_dim, \\\n",
    "                                para_lstm_layers, sess, \\\n",
    "                                para_lr, para_l2, para_max_norm,\\\n",
    "                                True, para_batch_size)\n",
    "    \n",
    "    reg.train_ini()\n",
    "    reg.inference_ini()\n",
    "    \n",
    "    total_cnt   = np.shape(disc_xtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx = range(total_cnt)\n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc = 0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "    \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_x = disc_xtrain[ batch_idx ]\n",
    "            batch_y = disc_ytrain[ batch_idx ]            \n",
    "            \n",
    "            tmpc += reg.train_batch( batch_x, batch_y, para_keep_prob,)\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "    \n",
    "        tmp_test_acc  = reg.inference( disc_xtest, disc_ytest,  para_keep_prob) \n",
    "        tmp_train_acc = reg.inference( disc_xtrain,disc_ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 100\n",
    "total_series_length = 50000\n",
    "truncated_backprop_length = 15\n",
    "state_size = 4\n",
    "num_classes = 2\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length\n",
    "\n",
    "def generateData():\n",
    "    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n",
    "    y = np.roll(x, echo_step)\n",
    "    y[0:echo_step] = 0\n",
    "\n",
    "    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "    y = y.reshape((batch_size, -1))\n",
    "\n",
    "    return (x, y)\n",
    "\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "init_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "\n",
    "W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\n",
    "\n",
    "# Unpack columns\n",
    "inputs_series = tf.unpack(batchX_placeholder, axis=1)\n",
    "labels_series = tf.unpack(batchY_placeholder, axis=1)\n",
    "\n",
    "# Forward pass\n",
    "current_state = init_state\n",
    "states_series = []\n",
    "for current_input in inputs_series:\n",
    "    current_input = tf.reshape(current_input, [batch_size, 1])\n",
    "    input_and_state_concatenated = tf.concat(1, [current_input, current_state])  # Increasing number of columns\n",
    "\n",
    "    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)  # Broadcasted addition\n",
    "    states_series.append(next_state)\n",
    "    current_state = next_state\n",
    "\n",
    "logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels) \\\n",
    "          for logits, labels in zip(logits_series,labels_series)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)\n",
    "\n",
    "def plot(loss_list, predictions_series, batchX, batchY):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "\n",
    "    for batch_series_idx in range(5):\n",
    "        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "        plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "        left_offset = range(truncated_backprop_length)\n",
    "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
    "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    plt.ion()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        x,y = generateData()\n",
    "        _current_state = np.zeros((batch_size, state_size))\n",
    "\n",
    "        print(\"New data, epoch\", epoch_idx)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "\n",
    "            batchX = x[:,start_idx:end_idx]\n",
    "            batchY = y[:,start_idx:end_idx]\n",
    "\n",
    "            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
    "                [total_loss, train_step, current_state, predictions_series],\n",
    "                feed_dict={\n",
    "                    batchX_placeholder:batchX,\n",
    "                    batchY_placeholder:batchY,\n",
    "                    init_state:_current_state\n",
    "                })\n",
    "\n",
    "            loss_list.append(_total_loss)\n",
    "\n",
    "            if batch_idx%100 == 0:\n",
    "                print(\"Step\",batch_idx, \"Loss\", _total_loss)\n",
    "                plot(loss_list, _predictions_series, batchX, batchY)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "view raw1-10-vanilla-rnn.py hosted with ❤ by GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generative\n",
    "\n",
    "def expand_y( x, y ):\n",
    "    cnt = len(x)\n",
    "    expand_y = []\n",
    "    \n",
    "    for i in range(cnt):\n",
    "        tmp = x[i][1:]\n",
    "        tmp = np.append( tmp, y[i][0] )\n",
    "        \n",
    "        expand_y.append( tmp )\n",
    "    \n",
    "    return np.array( expand_y )\n",
    "\n",
    "class tsLSTM_generative():\n",
    "    \n",
    "    def __init__(self, n_lstm_dim, n_steps, n_data_dim, n_lstm_layers, session,\\\n",
    "                 lr, l2, max_norm ):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "        self.L2 =  l2\n",
    "        \n",
    "        self.N_LSTM_LAYERS = n_lstm_layers\n",
    "        self.N_LSTM_DIM    = n_lstm_dim\n",
    "        \n",
    "        self.N_STEPS    = n_steps\n",
    "        self.N_DATA_DIM = n_data_dim\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [None, self.N_STEPS, self.N_DATA_DIM])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_STEPS])\n",
    "        self.test_y    = tf.placeholder(tf.float32, [None, 1]\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        with tf.variable_scope(\"lstm\"):\n",
    "            \n",
    "            lstm_cell    = tf.contrib.rnn.GRUCell(self.N_LSTM_DIM)\n",
    "#           , state_is_tuple = True\n",
    "            \n",
    "            stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell]* self.N_LSTM_LAYERS,\\\n",
    "                                                      state_is_tuple=True )\n",
    "            \n",
    "            self.hiddens, self.state = tf.nn.dynamic_rnn(cell = stacked_lstm, \\\n",
    "                                                         inputs = self.x,\\\n",
    "                                                    dtype = tf.float32)\n",
    "            \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w = tf.Variable( tf.random_normal([self.N_LSTM_DIM, 1],\\\n",
    "                             stddev=math.sqrt(2.0/self.N_LSTM_DIM)) )\n",
    "            \n",
    "            b = tf.Variable(tf.zeros( [ 1 ] ))\n",
    "            \n",
    "            \n",
    "            train_h  =  tf.reshape( self.hiddens, [ -1, self.N_LSTM_DIM ] )\n",
    "            train_py =  tf.matmul( train_h, w ) + b\n",
    "            self.py  =  tf.reshape( train_py, [-1, self.N_STEPS ] ) \n",
    "            \n",
    "            \n",
    "            test_h = tf.transpose( self.hiddens, [1,0,2] )\n",
    "            self.test_py = tf.matmul( test_h[-1], w ) + b\n",
    "        \n",
    "\n",
    "    def train_ini(self):\n",
    "        \n",
    "        self.cost = tf.nn.l2_loss( self.y - self.py )\n",
    "        \n",
    "        self.optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "        \n",
    "    def train_batch(self, x_batch, y_batch, keep_prob ):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer, self.cost],\\\n",
    "                        feed_dict={self.x:x_batch,\\\n",
    "                                   self.y:y_batch,\\\n",
    "                                   self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "            \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "\n",
    "#       denormalzied RMSE\n",
    "        self.rmse = tf.sqrt( tf.reduce_mean(\\\n",
    "                             tf.square( self.test_y - self.test_py ) ) )\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, x_test, y_test, keep_prob):\n",
    "        return sess.run([self.rmse], feed_dict={self.x:x_test,\\\n",
    "                                                self.test_y:y_test,\\\n",
    "                                                self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "   \n",
    "    def test(self, x_test, y_test):\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        tmpshape  = tf.shape(self.state)\n",
    "        sess.run( self.init )\n",
    "        \n",
    "        return sess.run( [ tmpshape ], \\\n",
    "                         feed_dict={self.x:x_test, self.y:y_test})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 100, 10):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generative\n",
    "\n",
    "gen_ytrain_test =  ytrain\n",
    "gen_ytrain = expand_y( xtrain, ytrain )\n",
    "gen_ytest  = ytest\n",
    "\n",
    "gen_xtrain = np.reshape( xtrain, [-1, 100, 1] )\n",
    "gen_xtest  = np.reshape( xtest, [-1, 100, 1] )\n",
    "\n",
    "print np.shape(gen_xtrain), np.shape(gen_ytrain), np.shape(gen_xtest), np.shape(gen_ytest)\n",
    "\n",
    "\n",
    "\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_lr = 0.01\n",
    "para_lstm_dim = 256\n",
    "para_lstm_layers = 1\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_l2 = 0.1\n",
    "\n",
    "para_max_norm = 4\n",
    "para_keep_prob = 0.8\n",
    "\n",
    "# fixed parameters\n",
    "para_data_dim = 1\n",
    "para_steps = 100\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "# clean the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    reg = tsLSTM_generative( para_lstm_dim, para_steps, para_data_dim, \\\n",
    "                                para_lstm_layers, sess, \\\n",
    "                                para_lr, para_l2, para_max_norm)\n",
    "    \n",
    "    reg.train_ini()\n",
    "    reg.inference_ini()\n",
    "    \n",
    "    total_cnt   = np.shape(gen_xtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx = range(total_cnt)\n",
    "    \n",
    " \n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc = 0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "    \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_x = gen_xtrain[ batch_idx ]\n",
    "            batch_y = gen_ytrain[ batch_idx ]            \n",
    "            \n",
    "            tmpc += reg.train_batch( batch_x, batch_y, para_keep_prob,)\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "    \n",
    "        tmp_test_acc  = reg.inference( gen_xtest, gen_ytest,       para_keep_prob) \n",
    "        tmp_train_acc = reg.inference( gen_xtrain,gen_ytrain_test, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
