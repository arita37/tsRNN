{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discriminative\n",
    "\n",
    "class tsLSTM_discriminative():\n",
    "    \n",
    "    def __init__(self, n_lstm_dim, n_steps, n_data_dim, n_lstm_layers, session,\\\n",
    "                 lr, l2, max_norm ):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "        self.L2 =  l2\n",
    "        \n",
    "        self.N_LSTM_LAYERS = n_lstm_layers\n",
    "        self.N_LSTM_DIM    = n_lstm_dim\n",
    "        \n",
    "        self.N_STEPS    = n_steps\n",
    "        self.N_DATA_DIM = n_data_dim\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [None, self.N_STEPS, self.N_DATA_DIM])\n",
    "        self.y = tf.placeholder(tf.float32, [None, 1])\n",
    "        \n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        with tf.variable_scope(\"lstm\"):\n",
    "            \n",
    "#           !!change\n",
    "            lstm_cell    = tf.contrib.rnn.GRUCell(self.N_LSTM_DIM, activation = tf.nn.relu)\n",
    "#           state_is_tuple = True\n",
    "            \n",
    "            stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell]* self.N_LSTM_LAYERS,\\\n",
    "                                                      state_is_tuple=True )\n",
    "            \n",
    "            self.hiddens, self.state = tf.nn.dynamic_rnn(cell = stacked_lstm,\\\n",
    "                                                         inputs = self.x,\\\n",
    "                                                         dtype = tf.float32)\n",
    "            \n",
    "        tmp_hiddens = tf.transpose( self.hiddens, [1,0,2]  )\n",
    "        last_hidden = tmp_hiddens[-1]\n",
    "        \n",
    "        with tf.variable_scope(\"hidden\"):\n",
    "                \n",
    "#           change  orthogonal ini\n",
    "            w = tf.Variable(tf.random_normal([self.N_LSTM_DIM, 128],\\\n",
    "                            stddev=math.sqrt(2.0/self.N_LSTM_DIM)))\n",
    "            b = tf.Variable(tf.zeros( [128] ))\n",
    "            \n",
    "            self.regularization = tf.nn.l2_loss(w)\n",
    "            \n",
    "            h = tf.matmul(last_hidden, w) + b\n",
    "            h = tf.nn.relu( h )\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w = tf.Variable(tf.random_normal([128, 1],\\\n",
    "                            stddev=math.sqrt(2.0/128)))\n",
    "            b = tf.Variable(tf.zeros( [ 1 ] ))\n",
    "            \n",
    "            self.regularization += tf.nn.l2_loss(w)\n",
    "            self.py = tf.matmul(h, w) + b\n",
    "        \n",
    "    \n",
    "    def train_ini(self):\n",
    "#       !!! change\n",
    "        self.cost = tf.reduce_mean( tf.quare(self.y - self.py) ) + self.L2*self.regularization\n",
    "        \n",
    "        self.optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)  \n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)  \n",
    "#         !! same lr, converge faster\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "        \n",
    "    def train_batch(self, x_batch, y_batch, keep_prob ):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer, self.cost],\\\n",
    "                        feed_dict={self.x:x_batch,\\\n",
    "                                   self.y:y_batch,\\\n",
    "                                   self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "            \n",
    "        return c\n",
    "\n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "\n",
    "#       denormalzied RMSE  \n",
    "        self.rmse = \\\n",
    "        tf.sqrt( tf.reduce_mean( tf.square( self.y - self.py ) ) )\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, x_test, y_test, keep_prob):\n",
    "        return sess.run([self.rmse], feed_dict={self.x:x_test,\\\n",
    "                                                self.y:y_test,\\\n",
    "                                                self.keep_prob:keep_prob\\\n",
    "                                                })\n",
    "        \n",
    "   \n",
    "    def test(self, x_test, y_test ):\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        tmpshape = tf.shape(self.state)\n",
    "        sess.run( self.init )\n",
    "        \n",
    "        return sess.run( [ tmpshape ], \\\n",
    "                 feed_dict={self.x:x_test, self.y:y_test})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discriminative\n",
    "\n",
    "disc_xtrain = np.reshape( xtrain, [-1, 100, 1] )\n",
    "disc_xtest  = np.reshape( xtest, [-1, 100, 1] )\n",
    "\n",
    "disc_ytrain = ytrain\n",
    "disc_ytest  = ytest\n",
    "\n",
    "print np.shape(disc_xtrain), np.shape(disc_ytrain), np.shape(disc_xtest), \\\n",
    "np.shape(disc_ytest)\n",
    "\n",
    "\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_lr = 0.001\n",
    "para_lstm_dim = 256\n",
    "para_lstm_layers = 1\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_l2 = 0.1\n",
    "\n",
    "para_max_norm  = 4\n",
    "para_keep_prob = 0.8\n",
    "\n",
    "#  fixed parameters\n",
    "para_data_dim = 1\n",
    "para_steps = 100\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    reg = tsLSTM_discriminative( para_lstm_dim, para_steps, para_data_dim, \\\n",
    "                                para_lstm_layers, sess, \\\n",
    "                                para_lr, para_l2, para_max_norm)\n",
    "    \n",
    "    reg.train_ini()\n",
    "    reg.inference_ini()\n",
    "    \n",
    "    total_cnt   = np.shape(disc_xtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx = range(total_cnt)\n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc = 0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "    \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_x = disc_xtrain[ batch_idx ]\n",
    "            batch_y = disc_ytrain[ batch_idx ]            \n",
    "            \n",
    "            tmpc += reg.train_batch( batch_x, batch_y, para_keep_prob,)\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "    \n",
    "        tmp_test_acc  = reg.inference( disc_xtest, disc_ytest,  para_keep_prob) \n",
    "        tmp_train_acc = reg.inference( disc_xtrain,disc_ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generative\n",
    "\n",
    "def expand_y( x, y ):\n",
    "    cnt = len(x)\n",
    "    expand_y = []\n",
    "    \n",
    "    for i in range(cnt):\n",
    "        tmp = x[i][1:]\n",
    "        tmp = np.append( tmp, y[i][0] )\n",
    "        \n",
    "        expand_y.append( tmp )\n",
    "    \n",
    "    return np.array( expand_y )\n",
    "\n",
    "class tsLSTM_generative():\n",
    "    \n",
    "    def __init__(self, n_lstm_dim, n_steps, n_data_dim, n_lstm_layers, session,\\\n",
    "                 lr, l2, max_norm ):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "        self.L2 =  l2\n",
    "        \n",
    "        self.N_LSTM_LAYERS = n_lstm_layers\n",
    "        self.N_LSTM_DIM    = n_lstm_dim\n",
    "        \n",
    "        self.N_STEPS    = n_steps\n",
    "        self.N_DATA_DIM = n_data_dim\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [None, self.N_STEPS, self.N_DATA_DIM])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_STEPS])\n",
    "        self.test_y    = tf.placeholder(tf.float32, [None, 1]\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        with tf.variable_scope(\"lstm\"):\n",
    "            \n",
    "            lstm_cell    = tf.contrib.rnn.GRUCell(self.N_LSTM_DIM)\n",
    "#           , state_is_tuple = True\n",
    "            \n",
    "            stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell]* self.N_LSTM_LAYERS,\\\n",
    "                                                      state_is_tuple=True )\n",
    "            \n",
    "            self.hiddens, self.state = tf.nn.dynamic_rnn(cell = stacked_lstm, \\\n",
    "                                                         inputs = self.x,\\\n",
    "                                                    dtype = tf.float32)\n",
    "            \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w = tf.Variable( tf.random_normal([self.N_LSTM_DIM, 1],\\\n",
    "                             stddev=math.sqrt(2.0/self.N_LSTM_DIM)) )\n",
    "            \n",
    "            b = tf.Variable(tf.zeros( [ 1 ] ))\n",
    "            \n",
    "            \n",
    "            train_h  =  tf.reshape( self.hiddens, [ -1, self.N_LSTM_DIM ] )\n",
    "            train_py =  tf.matmul( train_h, w ) + b\n",
    "            self.py  =  tf.reshape( train_py, [-1, self.N_STEPS ] ) \n",
    "            \n",
    "            \n",
    "            test_h = tf.transpose( self.hiddens, [1,0,2] )\n",
    "            self.test_py = tf.matmul( test_h[-1], w ) + b\n",
    "        \n",
    "\n",
    "    def train_ini(self):\n",
    "        \n",
    "        self.cost = tf.nn.l2_loss( self.y - self.py )\n",
    "        \n",
    "        self.optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "        \n",
    "    def train_batch(self, x_batch, y_batch, keep_prob ):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer, self.cost],\\\n",
    "                        feed_dict={self.x:x_batch,\\\n",
    "                                   self.y:y_batch,\\\n",
    "                                   self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "            \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "\n",
    "#       denormalzied RMSE\n",
    "        self.rmse = tf.sqrt( tf.reduce_mean(\\\n",
    "                             tf.square( self.test_y - self.test_py ) ) )\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, x_test, y_test, keep_prob):\n",
    "        return sess.run([self.rmse], feed_dict={self.x:x_test,\\\n",
    "                                                self.test_y:y_test,\\\n",
    "                                                self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "   \n",
    "    def test(self, x_test, y_test):\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        tmpshape  = tf.shape(self.state)\n",
    "        sess.run( self.init )\n",
    "        \n",
    "        return sess.run( [ tmpshape ], \\\n",
    "                         feed_dict={self.x:x_test, self.y:y_test})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generative\n",
    "\n",
    "gen_ytrain_test =  ytrain\n",
    "gen_ytrain = expand_y( xtrain, ytrain )\n",
    "gen_ytest  = ytest\n",
    "\n",
    "gen_xtrain = np.reshape( xtrain, [-1, 100, 1] )\n",
    "gen_xtest  = np.reshape( xtest, [-1, 100, 1] )\n",
    "\n",
    "print np.shape(gen_xtrain), np.shape(gen_ytrain), np.shape(gen_xtest), np.shape(gen_ytest)\n",
    "\n",
    "\n",
    "\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_lr = 0.01\n",
    "para_lstm_dim = 256\n",
    "para_lstm_layers = 1\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_l2 = 0.1\n",
    "\n",
    "para_max_norm = 4\n",
    "para_keep_prob = 0.8\n",
    "\n",
    "# fixed parameters\n",
    "para_data_dim = 1\n",
    "para_steps = 100\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "# clean the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    reg = tsLSTM_generative( para_lstm_dim, para_steps, para_data_dim, \\\n",
    "                                para_lstm_layers, sess, \\\n",
    "                                para_lr, para_l2, para_max_norm)\n",
    "    \n",
    "    reg.train_ini()\n",
    "    reg.inference_ini()\n",
    "    \n",
    "    total_cnt   = np.shape(gen_xtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx = range(total_cnt)\n",
    "    \n",
    " \n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc = 0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "    \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_x = gen_xtrain[ batch_idx ]\n",
    "            batch_y = gen_ytrain[ batch_idx ]            \n",
    "            \n",
    "            tmpc += reg.train_batch( batch_x, batch_y, para_keep_prob,)\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "    \n",
    "        tmp_test_acc  = reg.inference( gen_xtest, gen_ytest,       para_keep_prob) \n",
    "        tmp_train_acc = reg.inference( gen_xtrain,gen_ytrain_test, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
