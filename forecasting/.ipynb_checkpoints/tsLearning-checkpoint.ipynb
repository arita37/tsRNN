{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# baseline\n",
    "\n",
    "#  GP\n",
    "#  S-ARIMA\n",
    "#  SVR\n",
    "\n",
    "\n",
    "#  RF\n",
    "#  GBT\n",
    "#  xgboosted \n",
    "\n",
    "\n",
    "# bayeisan regression\n",
    "\n",
    "# LSTM:\n",
    "#   1. initialization\n",
    "#   2. batch normalization\n",
    "#   3. weight normalization\n",
    "#   4. variable length\n",
    "#   5. attention mechanism \n",
    "\n",
    "# LSTM:  discrimitive, generative\n",
    "# perodic in data\n",
    "# attention \n",
    "\n",
    "# http://bugra.github.io/work/notes/2014-04-26/outlier-detection-markov-chain-monte-carlo-via-pymc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "#  lstm regularization\n",
    "#  skewness of dependent variable \n",
    "\n",
    "# http://smerity.com/articles/2016/orthogonal_init.html\n",
    "\n",
    "\n",
    "\n",
    "# RMSE, MAE, MAPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# data processing packages\n",
    "import numpy as np   \n",
    "import pandas as pd \n",
    "\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "from scipy import *\n",
    " \n",
    "%matplotlib inline    \n",
    "import matplotlib as mplt\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.style.use('ggplot')\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "\n",
    "# machine leanring packages\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from utils_keras import *\n",
    "from utils_dataPrepro import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features:\n",
    "    \n",
    "# Feature based approach: Here the time series are mapped to another, \n",
    "#     possibly lower dimensional, representation. \n",
    "#     This means that the feature extraction algorithm calculates characteristics\n",
    "#     such as the average or maximal value of the time series. The features are then \n",
    "#     passed as a feature matrix to a \"normal\" machine learning such as a neural network, \n",
    "#     random forest or support vector machine. This approach has the advantage of a better\n",
    "#     explainability of the results. Further it enables us to use a well developed theory of\n",
    "# supervised machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# show the training errors as well \n",
    "# mixture of expert\n",
    "# Gaussian process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3800, 200) (3800,) (1300, 200) (1300,)\n"
     ]
    }
   ],
   "source": [
    "# # stock data\n",
    "\n",
    "files_list=[\"../../dataset/dataset_ts/stock_xtrain.dat\", \\\n",
    "            \"../../dataset/dataset_ts/stock_xtest.dat\",\\\n",
    "            \"../../dataset/dataset_ts/stock_ytrain.dat\", \\\n",
    "            \"../../dataset/dataset_ts/stock_ytest.dat\"]\n",
    "\n",
    "xtrain, ytrain, xtest, ytest = \\\n",
    "prepare_train_test_data( False, files_list)\n",
    "\n",
    "\n",
    "print np.shape(xtrain), np.shape(ytrain), np.shape(xtest), np.shape(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--test-- (1297, 200) (8300, 200)\n",
      "(8300, 200) (8300,) (1297, 200) (1297,)\n"
     ]
    }
   ],
   "source": [
    "# # power data\n",
    "\n",
    "files_list=[\"../../dataset/dataset_ts/power_xtrain.dat\", \\\n",
    "            \"../../dataset/dataset_ts/power_xtest.dat\",\\\n",
    "            \"../../dataset/dataset_ts/power_ytrain.dat\", \\\n",
    "            \"../../dataset/dataset_ts/power_ytest.dat\"]\n",
    "\n",
    "xtrain, ytrain, xtest, ytest = \\\n",
    "prepare_train_test_data( False, files_list)\n",
    "\n",
    "print np.shape(xtrain), np.shape(ytrain), np.shape(xtest), np.shape(ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3840, 200) (3840,) (960, 200) (960,)\n"
     ]
    }
   ],
   "source": [
    "# # air-quality data\n",
    "\n",
    "files_list=[\"../../dataset/dataset_ts/air_xtrain.dat\", \\\n",
    "            \"../../dataset/dataset_ts/air_xtest.dat\",\\\n",
    "            \"../../dataset/dataset_ts/air_ytrain.dat\", \\\n",
    "            \"../../dataset/dataset_ts/air_ytest.dat\"]\n",
    "\n",
    "\n",
    "xtrain, ytrain, xtest, ytest,_,_ = \\\n",
    "prepare_train_test_data( False, files_list)\n",
    "\n",
    "print np.shape(xtrain), np.shape(ytrain), np.shape(xtest), np.shape(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBT\n",
    "\n",
    "# https://www.analyticsvidhya.com/blog/2016/02/\n",
    "# complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "#----Boosting parameters:\n",
    "#   learnning rate: 0.05 - 0.2\n",
    "#   n_estimators: 40-70\n",
    "\n",
    "#----Tree parameters:\n",
    "#   max_depth: 3-10\n",
    "#   max_leaf_nodes\n",
    "#   num_samples_split: 0.5-1% of total number \n",
    "#   min_samples_leaf\n",
    "#   max_features\n",
    "\n",
    "#   subsample: 0.8\n",
    "#   min_weight_fraction_leaf\n",
    "\n",
    "#----Order of tuning: max_depth and num_samples_split, min_samples_leaf, max_features\n",
    "\n",
    "def gbt_n_estimatior(maxnum, X, Y, xtest, ytest, fix_lr ):\n",
    "    \n",
    "    tmpy = Y.reshape( (len(Y),) )\n",
    "    score = []\n",
    "    \n",
    "    cnt = len(xtest)\n",
    "    \n",
    "    for i in range(10,maxnum+1,10):\n",
    "        clf = GradientBoostingRegressor(n_estimators = i,learning_rate = fix_lr)\n",
    "        \n",
    "        clf.fit( X, tmpy )\n",
    "        \n",
    "        pytest = clf.predict(xtest)\n",
    "\n",
    "        score.append(\\\n",
    "        (i, sqrt(mean([( pytest[i]-ytest[i] )**2 for i in range(cnt) ]))) )\n",
    "    \n",
    "    return min(score, key = lambda x: x[1]), score\n",
    "\n",
    "\n",
    "def gbt_tree_para( X, Y, xtest, ytest, depth_range, fix_lr, fix_n_est):\n",
    "    \n",
    "    tmpy = Y.reshape( (len(Y),) )\n",
    "    score = []\n",
    "    \n",
    "    cnt = len(xtest)\n",
    "    \n",
    "    for i in depth_range:\n",
    "        \n",
    "        clf = GradientBoostingRegressor(n_estimators = fix_n_est,learning_rate = fix_lr,\\\n",
    "                                        max_depth = i )\n",
    "        \n",
    "        clf.fit( X, tmpy )\n",
    "        \n",
    "        pytest = clf.predict(xtest)\n",
    "\n",
    "        score.append(\\\n",
    "        (i, sqrt(mean([( pytest[i]-ytest[i] )**2 for i in range(cnt) ]))) )\n",
    "    \n",
    "    return min(score, key = lambda x: x[1]), score\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimator, RMSE: (10, 200.74424935334872)\n",
      "depth, RMSE: (3, 250.03334104971421)\n"
     ]
    }
   ],
   "source": [
    "# GBT performance\n",
    "\n",
    "fix_lr = 0.25\n",
    "\n",
    "n_err, n_err_list = gbt_n_estimatior(301, xtrain, ytrain, xtest, ytest, fix_lr)\n",
    "\n",
    "print \"n_estimator, RMSE:\", n_err\n",
    "\n",
    "depth_err, depth_err_list = gbt_tree_para( xtrain, ytrain, xtest, ytest, range(3,16), fix_lr, n_err[0] )\n",
    "\n",
    "print \"depth, RMSE:\", depth_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GBT test\n",
    "# clf = GradientBoostingRegressor(n_estimators=20, learning_rate=0.25, max_depth = 3)\n",
    "        \n",
    "# clf.fit( xtrain, ytrain )\n",
    "        \n",
    "# pytest = clf.predict(xtest)\n",
    "\n",
    "# err = []\n",
    "# for i in range(len(pytest)):\n",
    "#     err.append( ytest[i] - pytest[i] )\n",
    "    \n",
    "\n",
    "# print mean(err), var(err), sqrt(mean([i**2 for i in err]))\n",
    "\n",
    "# print zip(ytrain, pytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoosted\n",
    "\n",
    "# https://www.analyticsvidhya.com/blog/2016/03/\n",
    "#     complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "\n",
    "#----General Parameters\n",
    "\n",
    "#   eta(learning rate): 0.05 - 0.3\n",
    "#   number of rounds: \n",
    "\n",
    "#----Booster Parameters\n",
    "\n",
    "#   max_depth 3-10\n",
    "#   max_leaf_nodes\n",
    "#   gamma: mininum loss reduction\n",
    "#   min_child_weight: 1 by default\n",
    "\n",
    "#   max_delta_step: not needed in general, for unbalance in logistic regression\n",
    "#   subsample: 0.5-1\n",
    "#   colsample_bytree: 0.5-1\n",
    "#   colsample_bylevel: \n",
    "#   lambda: l2 regularization \n",
    "#   alpha: l1 regularization\n",
    "#   scale_pos_weight: >>1, for high class imbalance\n",
    "\n",
    "# Learning Task Parameters\n",
    "\n",
    "\n",
    "def xgt_n_depth( lr, max_depth, max_round, X, Y, xtest, ytest ):\n",
    "    \n",
    "    score = []\n",
    "    xg_train = xgb.DMatrix(xtrain, label = ytrain)\n",
    "    xg_test  = xgb.DMatrix(xtest,  label = ytest)\n",
    "\n",
    "# setup parameters for xgboost\n",
    "    param = {}\n",
    "# use softmax multi-class classification\n",
    "    param['objective'] = \"reg:linear\" \n",
    "#   'multi:softmax'\n",
    "    \n",
    "# scale weight of positive examples\n",
    "    param['eta'] = lr\n",
    "    param['max_depth'] = 0\n",
    "    param['silent'] = 1\n",
    "    param['nthread'] = 8\n",
    "#     param['num_class'] = 8\n",
    "#     param['gamma']\n",
    "    \n",
    "    for depth_trial in range(2, max_depth):\n",
    "        for num_round_trial in range(2, max_round):\n",
    "            \n",
    "            param['max_depth'] = depth_trial\n",
    "            bst  = xgb.train( param, xg_train, num_round_trial )\n",
    "            pred = bst.predict( xg_test )\n",
    "            \n",
    "            tmp_accur = sqrt(mean( [(pred[i] - ytest[i])**2 for i in range(len(ytest))] )) \n",
    "            \n",
    "            score.append( (depth_trial, num_round_trial, tmp_accur) )\n",
    "            \n",
    "    return min(score, key = lambda x: x[2]), score\n",
    "\n",
    "\n",
    "def xgt_l2( fix_lr, fix_depth, fix_round, X, Y, xtest, ytest, l2_range ):\n",
    "    \n",
    "    score = []\n",
    "    xg_train = xgb.DMatrix(xtrain, label = ytrain)\n",
    "    xg_test  = xgb.DMatrix(xtest,  label = ytest)\n",
    "\n",
    "# setup parameters for xgboost\n",
    "    param = {}\n",
    "# use softmax multi-class classification\n",
    "    param['objective'] = 'reg:linear' \n",
    "#   'multi:softmax'\n",
    "    \n",
    "# scale weight of positive examples\n",
    "    param['eta'] = fix_lr\n",
    "    param['max_depth'] = fix_depth\n",
    "    param['silent'] = 1\n",
    "    param['nthread'] = 8\n",
    "#     param['num_class'] = 8\n",
    "    \n",
    "    param['lambda'] = 0.0\n",
    "#     param['alpha']\n",
    "    \n",
    "    \n",
    "    for l2_trial in l2_range:\n",
    "        \n",
    "        param['lambda'] = l2_trial\n",
    "        \n",
    "        bst = xgb.train(param, xg_train, fix_round )\n",
    "        pred = bst.predict( xg_test )\n",
    "            \n",
    "        tmp_accur = sqrt(mean( [(pred[i] - ytest[i])**2 for i in range(len(ytest))] )) \n",
    "            \n",
    "        score.append( (l2_trial, tmp_accur) )\n",
    "            \n",
    "    return min(score, key = lambda x: x[1]), score\n",
    "\n",
    "    \n",
    "#  def xgt_l1 for very high dimensional features    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoosted performance\n",
    "fix_lr = 0.2\n",
    "\n",
    "n_depth_err, n_depth_err_list = xgt_n_depth( fix_lr, 16, 51, xtrain, ytrain, xtest, ytest)\n",
    "\n",
    "print \" depth, number of rounds, RMSE:\", n_depth_err\n",
    "\n",
    "l2_err, l2_err_list = xgt_l2( fix_lr, n_depth_err[0], n_depth_err[1], xtrain, ytrain, xtest, ytest,\\\n",
    "                    [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000])\n",
    "\n",
    "print \" l2, RMSE:\", l2_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random forest\n",
    "\n",
    "# max_features:\n",
    "# n_estimators\n",
    "# max_depth\n",
    "\n",
    "def rf_n_depth_estimatior(maxnum, maxdep, X, Y, xtest, ytest ):\n",
    "        \n",
    "    tmpy = Y\n",
    "    score = []\n",
    "    \n",
    "    cnt = len(xtest)\n",
    "        \n",
    "    for n_trial in range(10,maxnum+1,10):\n",
    "        for dep_trial in range(2, maxdep+1):\n",
    "            \n",
    "            clf = RandomForestRegressor(n_estimators = n_trial, max_depth = dep_trial, max_features = \"sqrt\")\n",
    "            clf.fit( X, tmpy )\n",
    "        \n",
    "            pytest = clf.predict(xtest)\n",
    "            \n",
    "            score.append(\\\n",
    "            (n_trial, dep_trial, sqrt(mean([( pytest[i]-ytest[i] )**2 for i in range(cnt) ]))) )\n",
    "    \n",
    "    return min(score, key = lambda x: x[2]), score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimator, RMSE: (120, 24, 167.40678819053835)\n"
     ]
    }
   ],
   "source": [
    "# Random forest performance\n",
    "\n",
    "n_err, n_err_list = rf_n_depth_estimatior( 130, 25, xtrain, ytrain, xtest, ytest )\n",
    "\n",
    "print \"n_estimator, RMSE:\", n_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960,) (960, 200)\n",
      "156.576531538\n"
     ]
    }
   ],
   "source": [
    "# Bayesian regression\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "bayesian_reg = linear_model.BayesianRidge( normalize=True, fit_intercept=True )\n",
    "bayesian_reg.fit(xtrain, ytrain)\n",
    "\n",
    "# BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
    "#        fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
    "#        normalize=False, tol=0.001, verbose=False)\n",
    "\n",
    "y_pred = bayesian_reg.predict ( xtest )\n",
    "\n",
    "print np.shape(ytest), np.shape(xtest)\n",
    "print sqrt(mean( [(ytest[i]-ytmp)**2 for i,ytmp in enumerate( y_pred )] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960,)\n",
      "(0.1, 0.7, 156.1420367565249)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:14: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n"
     ]
    }
   ],
   "source": [
    "# ElasticNet\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "def enet_alpha_l1(alpha_range, l1_range, xtrain, ytrain, xtest, ytest):\n",
    "    \n",
    "    print np.shape(ytest)\n",
    "\n",
    "    res = []\n",
    "    for i in alpha_range:\n",
    "        for j in l1_range:\n",
    "            \n",
    "            enet = ElasticNet(alpha = i, l1_ratio = j)\n",
    "            enet.fit(xtrain, ytrain)\n",
    "            \n",
    "            y_pred = enet.predict( xtest )\n",
    "            \n",
    "            res.append( (i,j, sqrt(mean([(ytest[i]-ytmp)**2 for i,ytmp in enumerate(y_pred)])) ) )\n",
    "    \n",
    "    return min(res, key = lambda x:x[2]), res\n",
    "\n",
    "err_min, err_list = enet_alpha_l1( [0, 0.001, 0.01, 0.1, 1] , [0.7] , xtrain, ytrain, xtest, ytest)\n",
    "# 0  0.01 0.1 1 10 100\n",
    "print err_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_dic = { 'stock':0,\\\n",
    "             \"power\":1,\\\n",
    "             \"air\":2,\\\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-2a73a0b7d9fc>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-2a73a0b7d9fc>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    [i+100 if i[1] == 3 else i] for i in enumerate(a)\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a= [1,2,3,4]\n",
    "\n",
    " for i in enumerate(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
