{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# data processing packages\n",
    "import numpy as np   \n",
    "import pandas as pd \n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    " \n",
    "%matplotlib inline    \n",
    "import matplotlib as mplt\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.style.use('ggplot')\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "\n",
    "# machine leanring packages\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation, Dropout, Reshape, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "# RMSprop, Adadelta\n",
    "from keras.regularizers import *\n",
    "from keras.initializers import *\n",
    "from keras.activations import *\n",
    "\n",
    "from utils_keras import *\n",
    "from utils_dataPrepro import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notes:\n",
    "\n",
    "# ---Ini:            \n",
    "#     orthogonal: https://smerity.com/articles/2016/orthogonal_init.html\n",
    "#     identity, Xavier, He's\n",
    "\n",
    "# ---Activation:     \n",
    "#     tan, sigmod by default\n",
    "\n",
    "# ---Regularization: \n",
    "#     dropout on non-recurrent connections \n",
    "#     batch normalization\n",
    "\n",
    "# unstable or not decreasing training loss: lr, representability, e.g., number of neursons, layers, etc..\n",
    "# watch out for the amount difference between regularization and loss  \n",
    "# large minibatch -> large lr\n",
    "# large network -> large lr\n",
    "\n",
    "\n",
    "# learning speed, repren, regular\n",
    "\n",
    "# TO DO:\n",
    "    \n",
    "# more component\n",
    "# muliti-input-muliti-output method\n",
    "# log transformation\n",
    "# integrate trend \n",
    "# attention for peridoic time series\n",
    "\n",
    "# multistep ahead:  multi-output rnn\n",
    "#                   seq2seq rnn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_train_test_data( steps, bool_add_trend, xtrain_df, xtest_df, ytrain_df, ytest_df):\n",
    "    \n",
    "    PARA_STEPS = steps\n",
    "    PARA_ADD_TREND = bool_add_trend\n",
    "    \n",
    "    # integrate trends\n",
    "    if PARA_ADD_TREND == True:\n",
    "        \n",
    "        trend_xtrain = expand_x_trend( xtrain_df.as_matrix() )\n",
    "        trend_xtest  = expand_x_trend( xtest_df.as_matrix() )\n",
    "    \n",
    "        tmp_xtrain = np.reshape( trend_xtrain, [-1, (PARA_STEPS-1)*2 ] )\n",
    "        tmp_xtest  = np.reshape( trend_xtest,  [-1, (PARA_STEPS-1)*2 ] )\n",
    "    \n",
    "        xtrain_df = pd.DataFrame( tmp_xtrain )\n",
    "        xtest_df  = pd.DataFrame( tmp_xtest )\n",
    "    \n",
    "#   normalize x in training and testing datasets\n",
    "        xtest = conti_normalization_test_dta(xtest_df, xtrain_df)\n",
    "        xtrain= conti_normalization_train_dta(xtrain_df)\n",
    "\n",
    "#   trend enhanced\n",
    "        xtest  = np.reshape( xtest,  [-1, (PARA_STEPS-1), 2 ] )\n",
    "        xtrain = np.reshape( xtrain, [-1, (PARA_STEPS-1), 2 ] )\n",
    "        \n",
    "    else:\n",
    "#   normalize x in training and testing datasets\n",
    "        xtest = conti_normalization_test_dta(xtest_df, xtrain_df)\n",
    "        xtrain= conti_normalization_train_dta(xtrain_df)\n",
    "\n",
    "        ytrain = ytrain_df.as_matrix()\n",
    "        ytest  = ytest_df.as_matrix()\n",
    "        \n",
    "    return xtrain, ytrain, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7837, 200) (7837, 1) (1960, 200) (1960, 1)\n"
     ]
    }
   ],
   "source": [
    "# power data\n",
    "\n",
    "files_list=[\"../dataset/dataset_ts/power_xtrain.csv\", \\\n",
    "            \"../dataset/dataset_ts/power_xtest.csv\",\\\n",
    "            \"../dataset/dataset_ts/power_ytrain.csv\", \\\n",
    "            \"../dataset/dataset_ts/power_ytest.csv\"]\n",
    "\n",
    "\n",
    "xtrain_df =pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df  =pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df =pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df  =pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "xtrain, ytrain, xtest, ytest = prepare_train_test_data( 200, False, xtrain_df, xtest_df, ytrain_df, ytest_df)\n",
    "    \n",
    "print np.shape(xtrain), np.shape(ytrain), np.shape(xtest), np.shape(ytest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stock data\n",
    "\n",
    "files_list=[\"../dataset/dataset_ts/stock_xtrain.csv\", \\\n",
    "            \"../dataset/dataset_ts/stock_xtest.csv\",\\\n",
    "            \"../dataset/dataset_ts/stock_ytrain.csv\", \\\n",
    "            \"../dataset/dataset_ts/stock_ytest.csv\"]\n",
    "\n",
    "xtrain_df =pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df  =pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df =pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df  =pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "xtrain, ytrain, xtest, ytest = prepare_train_test_data( 200, False)\n",
    "    \n",
    "print np.shape(xtrain), np.shape(ytrain), np.shape(xtest), np.shape(ytest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7837, 200, 1) (7837, 1) (1960, 200, 1) (1960, 1)\n",
      "Epoch 1/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 57422.7632 \n",
      "Testing loss: 54719.2331952, test_err:233.921421246, train_err:233.956974336 \n",
      "\n",
      "7837/7837 [==============================] - 328s - loss: 57375.7122   \n",
      "Epoch 2/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 47239.3576 \n",
      "Testing loss: 34364.2757095, test_err:185.37603733, train_err:185.749099944 \n",
      "\n",
      "7837/7837 [==============================] - 350s - loss: 46997.8752   \n",
      "Epoch 3/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 17238.7789 \n",
      "Testing loss: 1936.06891492, test_err:44.0007830769, train_err:44.9969576227 \n",
      "\n",
      "7837/7837 [==============================] - 311s - loss: 16949.7931   \n",
      "Epoch 4/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 467.1182 \n",
      "Testing loss: 109.942876886, test_err:10.4853642038, train_err:10.7013243748 \n",
      "\n",
      "7837/7837 [==============================] - 311s - loss: 460.5099   \n",
      "Epoch 5/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 29.8627 \n",
      "Testing loss: 14.0175560737, test_err:3.74400148419, train_err:3.71765596781 \n",
      "\n",
      "7837/7837 [==============================] - 318s - loss: 29.5384   \n",
      "Epoch 6/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 11.0896 \n",
      "Testing loss: 9.34239057327, test_err:3.0565325376, train_err:3.06588834561 \n",
      "\n",
      "7837/7837 [==============================] - 320s - loss: 11.0721   \n",
      "Epoch 7/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 7.3083 \n",
      "Testing loss: 5.16177198668, test_err:2.27195323374, train_err:2.3217261142 \n",
      "\n",
      "7837/7837 [==============================] - 322s - loss: 7.2753   \n",
      "Epoch 8/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 5.0271 \n",
      "Testing loss: 4.57659708043, test_err:2.13929818744, train_err:2.18877758241 \n",
      "\n",
      "7837/7837 [==============================] - 320s - loss: 5.0352   \n",
      "Epoch 9/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 4.5873 \n",
      "Testing loss: 4.17099874968, test_err:2.04230245225, train_err:2.08459367923 \n",
      "\n",
      "7837/7837 [==============================] - 320s - loss: 4.5899   \n",
      "Epoch 10/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 4.0021 \n",
      "Testing loss: 3.29788751213, test_err:1.8160090939, train_err:1.84163952648 \n",
      "\n",
      "7837/7837 [==============================] - 314s - loss: 3.9924   \n",
      "Epoch 11/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 2.3882\n",
      "Testing loss: 1.86563050917, test_err:1.36588077954, train_err:1.3642038472 \n",
      "\n",
      "7837/7837 [==============================] - 279s - loss: 2.3693   \n",
      "Epoch 12/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 1.5703\n",
      "Testing loss: 1.43344057385, test_err:1.19726371777, train_err:1.17835188205 \n",
      "\n",
      "7837/7837 [==============================] - 273s - loss: 1.5620   \n",
      "Epoch 13/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 1.2721\n",
      "Testing loss: 1.218785583, test_err:1.10398636114, train_err:1.07555287475 \n",
      "\n",
      "7837/7837 [==============================] - 271s - loss: 1.2657   \n",
      "Epoch 14/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 1.1622\n",
      "Testing loss: 1.30076853572, test_err:1.14051025454, train_err:1.10463059948 \n",
      "\n",
      "7837/7837 [==============================] - 268s - loss: 1.1636   \n",
      "Epoch 15/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 1.0973\n",
      "Testing loss: 1.1111237616, test_err:1.05409879552, train_err:1.03302064748 \n",
      "\n",
      "7837/7837 [==============================] - 270s - loss: 1.0971   \n",
      "Epoch 16/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 1.0368\n",
      "Testing loss: 1.07029038668, test_err:1.03454828603, train_err:1.00342359952 \n",
      "\n",
      "7837/7837 [==============================] - 268s - loss: 1.0335   \n",
      "Epoch 17/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.9781\n",
      "Testing loss: 1.00477703761, test_err:1.00238541136, train_err:0.978959139492 \n",
      "\n",
      "7837/7837 [==============================] - 267s - loss: 0.9824   \n",
      "Epoch 18/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.9486\n",
      "Testing loss: 0.963700757951, test_err:0.981682685467, train_err:0.953510767052 \n",
      "\n",
      "7837/7837 [==============================] - 269s - loss: 0.9497   \n",
      "Epoch 19/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.9065\n",
      "Testing loss: 0.932573616991, test_err:0.96569872477, train_err:0.933300247823 \n",
      "\n",
      "7837/7837 [==============================] - 268s - loss: 0.9075   \n",
      "Epoch 20/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.8668\n",
      "Testing loss: 0.913909680989, test_err:0.95598585166, train_err:0.929122816423 \n",
      "\n",
      "7837/7837 [==============================] - 265s - loss: 0.8662   \n",
      "Epoch 21/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.8483\n",
      "Testing loss: 0.899963485951, test_err:0.948663688312, train_err:0.915690239975 \n",
      "\n",
      "7837/7837 [==============================] - 265s - loss: 0.8472   \n",
      "Epoch 22/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.8303\n",
      "Testing loss: 0.879625834981, test_err:0.93788378237, train_err:0.904441951366 \n",
      "\n",
      "7837/7837 [==============================] - 262s - loss: 0.8323   \n",
      "Epoch 23/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.8543\n",
      "Testing loss: 0.961022743887, test_err:0.980317187908, train_err:0.941632394365 \n",
      "\n",
      "7837/7837 [==============================] - 263s - loss: 0.8535   \n",
      "Epoch 24/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.8494\n",
      "Testing loss: 0.903501654158, test_err:0.950526161211, train_err:0.917944672364 \n",
      "\n",
      "7837/7837 [==============================] - 265s - loss: 0.8484   \n",
      "Epoch 25/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.8214\n",
      "Testing loss: 0.934043705828, test_err:0.966459316588, train_err:0.952854803717 \n",
      "\n",
      "7837/7837 [==============================] - 264s - loss: 0.8184   \n",
      "Epoch 26/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.8061\n",
      "Testing loss: 0.848438676036, test_err:0.921108069908, train_err:0.894558885843 \n",
      "\n",
      "7837/7837 [==============================] - 265s - loss: 0.8067   \n",
      "Epoch 27/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7910\n",
      "Testing loss: 0.843330421496, test_err:0.918330675992, train_err:0.893635366479 \n",
      "\n",
      "7837/7837 [==============================] - 264s - loss: 0.7904   \n",
      "Epoch 28/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7887\n",
      "Testing loss: 0.84566104631, test_err:0.919597812948, train_err:0.888067462176 \n",
      "\n",
      "7837/7837 [==============================] - 264s - loss: 0.7888   \n",
      "Epoch 29/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7692 \n",
      "Testing loss: 0.81413501939, test_err:0.902294374095, train_err:0.872242152837 \n",
      "\n",
      "7837/7837 [==============================] - 295s - loss: 0.7732   \n",
      "Epoch 30/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7699\n",
      "Testing loss: 0.814012953213, test_err:0.902226635318, train_err:0.868907303504 \n",
      "\n",
      "7837/7837 [==============================] - 273s - loss: 0.7675   \n",
      "Epoch 31/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7579\n",
      "Testing loss: 0.845297158008, test_err:0.91940041148, train_err:0.879802247884 \n",
      "\n",
      "7837/7837 [==============================] - 276s - loss: 0.7596   \n",
      "Epoch 32/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7760\n",
      "Testing loss: 0.836768306761, test_err:0.914749182573, train_err:0.883588303879 \n",
      "\n",
      "7837/7837 [==============================] - 283s - loss: 0.7766   \n",
      "Epoch 33/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7618\n",
      "Testing loss: 0.795508478612, test_err:0.891912579349, train_err:0.860184910904 \n",
      "\n",
      "7837/7837 [==============================] - 279s - loss: 0.7640   \n",
      "Epoch 34/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.7631 \n",
      "Testing loss: 0.788172368371, test_err:0.887790502256, train_err:0.856726074906 \n",
      "\n",
      "7837/7837 [==============================] - 326s - loss: 0.7615   \n",
      "Epoch 35/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7432 \n",
      "Testing loss: 0.810222249128, test_err:0.900124147925, train_err:0.864717153193 \n",
      "\n",
      "7837/7837 [==============================] - 285s - loss: 0.7436   \n",
      "Epoch 36/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7359\n",
      "Testing loss: 0.779444439314, test_err:0.882861674148, train_err:0.852930237091 \n",
      "\n",
      "7837/7837 [==============================] - 259s - loss: 0.7355   \n",
      "Epoch 37/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7344\n",
      "Testing loss: 0.785522809564, test_err:0.886297341568, train_err:0.85216163419 \n",
      "\n",
      "7837/7837 [==============================] - 282s - loss: 0.7342   \n",
      "Epoch 38/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7330\n",
      "Testing loss: 0.786377684924, test_err:0.886779383725, train_err:0.861301762903 \n",
      "\n",
      "7837/7837 [==============================] - 278s - loss: 0.7307   \n",
      "Epoch 39/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7401\n",
      "Testing loss: 0.807940866996, test_err:0.898855362134, train_err:0.86111091993 \n",
      "\n",
      "7837/7837 [==============================] - 280s - loss: 0.7372   \n",
      "Epoch 40/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7321\n",
      "Testing loss: 0.76966104848, test_err:0.877303219042, train_err:0.853297080423 \n",
      "\n",
      "7837/7837 [==============================] - 273s - loss: 0.7297   \n",
      "Epoch 41/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7261\n",
      "Testing loss: 0.779573999376, test_err:0.882935040394, train_err:0.848387340249 \n",
      "\n",
      "7837/7837 [==============================] - 271s - loss: 0.7251   \n",
      "Epoch 42/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7267\n",
      "Testing loss: 0.75849655745, test_err:0.870916965597, train_err:0.841385516183 \n",
      "\n",
      "7837/7837 [==============================] - 272s - loss: 0.7239   \n",
      "Epoch 43/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7172\n",
      "Testing loss: 0.789667885644, test_err:0.888633384404, train_err:0.855214598703 \n",
      "\n",
      "7837/7837 [==============================] - 276s - loss: 0.7148   \n",
      "Epoch 44/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7245\n",
      "Testing loss: 0.799219282793, test_err:0.893991156651, train_err:0.857815956167 \n",
      "\n",
      "7837/7837 [==============================] - 274s - loss: 0.7256   \n",
      "Epoch 45/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7303\n",
      "Testing loss: 0.767866835546, test_err:0.876280939735, train_err:0.854924954447 \n",
      "\n",
      "7837/7837 [==============================] - 278s - loss: 0.7301   \n",
      "Epoch 46/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7127\n",
      "Testing loss: 0.76038210392, test_err:0.871999719982, train_err:0.846381417636 \n",
      "\n",
      "7837/7837 [==============================] - 272s - loss: 0.7160   \n",
      "Epoch 47/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7041\n",
      "Testing loss: 0.757565747475, test_err:0.870382475231, train_err:0.838028612591 \n",
      "\n",
      "7837/7837 [==============================] - 278s - loss: 0.7049   \n",
      "Epoch 48/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7039\n",
      "Testing loss: 0.785591099457, test_err:0.886334559097, train_err:0.861904254383 \n",
      "\n",
      "7837/7837 [==============================] - 278s - loss: 0.7047   \n",
      "Epoch 49/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7089\n",
      "Testing loss: 0.768871433881, test_err:0.876853629808, train_err:0.85305871287 \n",
      "\n",
      "7837/7837 [==============================] - 277s - loss: 0.7075   \n",
      "Epoch 50/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7082\n",
      "Testing loss: 0.7356219389, test_err:0.857684178458, train_err:0.831105568875 \n",
      "\n",
      "7837/7837 [==============================] - 279s - loss: 0.7108   \n",
      "Epoch 51/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6992\n",
      "Testing loss: 0.760086088521, test_err:0.871828751308, train_err:0.848224884037 \n",
      "\n",
      "7837/7837 [==============================] - 273s - loss: 0.7013   \n",
      "Epoch 52/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7072\n",
      "Testing loss: 0.742702751987, test_err:0.861802096604, train_err:0.837514200125 \n",
      "\n",
      "7837/7837 [==============================] - 275s - loss: 0.7058   \n",
      "Epoch 53/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.7073\n",
      "Testing loss: 0.755161347681, test_err:0.869000262341, train_err:0.836029014524 \n",
      "\n",
      "7837/7837 [==============================] - 275s - loss: 0.7042   \n",
      "Epoch 54/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6918\n",
      "Testing loss: 0.742104860958, test_err:0.861455124822, train_err:0.835727425453 \n",
      "\n",
      "7837/7837 [==============================] - 278s - loss: 0.6918   \n",
      "Epoch 55/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6920\n",
      "Testing loss: 0.736096428122, test_err:0.857960799165, train_err:0.828923766497 \n",
      "\n",
      "7837/7837 [==============================] - 275s - loss: 0.6917   \n",
      "Epoch 56/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6970\n",
      "Testing loss: 0.723762803905, test_err:0.850742542949, train_err:0.826942529564 \n",
      "\n",
      "7837/7837 [==============================] - 277s - loss: 0.6970   \n",
      "Epoch 57/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6872\n",
      "Testing loss: 0.728153271821, test_err:0.853319337253, train_err:0.830050937517 \n",
      "\n",
      "7837/7837 [==============================] - 278s - loss: 0.6897   \n",
      "Epoch 58/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6900\n",
      "Testing loss: 0.729613355958, test_err:0.8541736902, train_err:0.825957552606 \n",
      "\n",
      "7837/7837 [==============================] - 278s - loss: 0.6888   \n",
      "Epoch 59/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6835\n",
      "Testing loss: 0.759741393401, test_err:0.871633154052, train_err:0.847622224825 \n",
      "\n",
      "7837/7837 [==============================] - 276s - loss: 0.6825   \n",
      "Epoch 60/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6919\n",
      "Testing loss: 0.723311694787, test_err:0.850477690134, train_err:0.829597948415 \n",
      "\n",
      "7837/7837 [==============================] - 278s - loss: 0.6931   \n",
      "Epoch 61/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6825\n",
      "Testing loss: 0.750652239274, test_err:0.866402408649, train_err:0.833587202617 \n",
      "\n",
      "7837/7837 [==============================] - 277s - loss: 0.6850   \n",
      "Epoch 62/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6861\n",
      "Testing loss: 0.810741276887, test_err:0.900413945406, train_err:0.877268371926 \n",
      "\n",
      "7837/7837 [==============================] - 277s - loss: 0.6877   \n",
      "Epoch 63/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6938\n",
      "Testing loss: 0.735064330393, test_err:0.857358770852, train_err:0.827024419475 \n",
      "\n",
      "7837/7837 [==============================] - 281s - loss: 0.6917   \n",
      "Epoch 64/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6993\n",
      "Testing loss: 0.750582738312, test_err:0.866361560985, train_err:0.853025262311 \n",
      "\n",
      "7837/7837 [==============================] - 277s - loss: 0.6978   \n",
      "Epoch 65/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6946\n",
      "Testing loss: 0.733568786602, test_err:0.856486728931, train_err:0.8285263965 \n",
      "\n",
      "7837/7837 [==============================] - 280s - loss: 0.6913   \n",
      "Epoch 66/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6866\n",
      "Testing loss: 0.710943779167, test_err:0.843174836884, train_err:0.818271498731 \n",
      "\n",
      "7837/7837 [==============================] - 278s - loss: 0.6852   \n",
      "Epoch 67/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6808\n",
      "Testing loss: 0.720367580774, test_err:0.848744623587, train_err:0.830519080133 \n",
      "\n",
      "7837/7837 [==============================] - 279s - loss: 0.6840   \n",
      "Epoch 68/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6910\n",
      "Testing loss: 0.725097034902, test_err:0.851526432693, train_err:0.822866363466 \n",
      "\n",
      "7837/7837 [==============================] - 278s - loss: 0.6880   \n",
      "Epoch 69/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6824\n",
      "Testing loss: 0.706750285139, test_err:0.840684475169, train_err:0.817265260283 \n",
      "\n",
      "7837/7837 [==============================] - 279s - loss: 0.6814   \n",
      "Epoch 70/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6795\n",
      "Testing loss: 0.736535553543, test_err:0.858216472287, train_err:0.841189022733 \n",
      "\n",
      "7837/7837 [==============================] - 280s - loss: 0.6820   \n",
      "Epoch 71/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6886\n",
      "Testing loss: 0.705066650011, test_err:0.83968250611, train_err:0.816166453615 \n",
      "\n",
      "7837/7837 [==============================] - 291s - loss: 0.6870   \n",
      "Epoch 72/500\n",
      "7680/7837 [============================>.] - ETA: 3s - loss: 0.6823 \n",
      "Testing loss: 0.712332947887, test_err:0.843997893241, train_err:0.821530995018 \n",
      "\n",
      "7837/7837 [==============================] - 348s - loss: 0.6796   \n",
      "Epoch 73/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6717 \n",
      "Testing loss: 0.725974025045, test_err:0.852041364176, train_err:0.824348498586 \n",
      "\n",
      "7837/7837 [==============================] - 419s - loss: 0.6719   \n",
      "Epoch 74/500\n",
      "7680/7837 [============================>.] - ETA: 5s - loss: 0.6715 \n",
      "Testing loss: 0.710157899954, test_err:0.842709047961, train_err:0.818391539447 \n",
      "\n",
      "7837/7837 [==============================] - 478s - loss: 0.6722   \n",
      "Epoch 75/500\n",
      "7680/7837 [============================>.] - ETA: 6s - loss: 0.7066 \n",
      "Testing loss: 0.726277192271, test_err:0.852219429777, train_err:0.836096740497 \n",
      "\n",
      "7837/7837 [==============================] - 468s - loss: 0.7060   \n",
      "Epoch 76/500\n",
      "7680/7837 [============================>.] - ETA: 5s - loss: 0.6977 \n",
      "Testing loss: 0.705867247192, test_err:0.840158584351, train_err:0.81859037638 \n",
      "\n",
      "7837/7837 [==============================] - 467s - loss: 0.6959   \n",
      "Epoch 77/500\n",
      "7680/7837 [============================>.] - ETA: 6s - loss: 0.6741 \n",
      "Testing loss: 0.716886480974, test_err:0.846691682696, train_err:0.818551006869 \n",
      "\n",
      "7837/7837 [==============================] - 558s - loss: 0.6741   \n",
      "Epoch 78/500\n",
      "7680/7837 [============================>.] - ETA: 8s - loss: 0.6824 \n",
      "Testing loss: 0.717796784761, test_err:0.847229849767, train_err:0.827152717804 \n",
      "\n",
      "7837/7837 [==============================] - 731s - loss: 0.6807   \n",
      "Epoch 79/500\n",
      "7680/7837 [============================>.] - ETA: 8s - loss: 0.6794 \n",
      "Testing loss: 0.706374766632, test_err:0.840461019555, train_err:0.814210128434 \n",
      "\n",
      "7837/7837 [==============================] - 730s - loss: 0.6760   \n",
      "Epoch 80/500\n",
      "7680/7837 [============================>.] - ETA: 8s - loss: 0.6781 \n",
      "Testing loss: 0.697984222733, test_err:0.835454823243, train_err:0.813074371858 \n",
      "\n",
      "7837/7837 [==============================] - 732s - loss: 0.6802   \n",
      "Epoch 81/500\n",
      "7680/7837 [============================>.] - ETA: 8s - loss: 0.6692 \n",
      "Testing loss: 0.70133132594, test_err:0.837455643681, train_err:0.81429055285 \n",
      "\n",
      "7837/7837 [==============================] - 750s - loss: 0.6683   \n",
      "Epoch 82/500\n",
      "7680/7837 [============================>.] - ETA: 8s - loss: 0.6664 \n",
      "Testing loss: 0.703594751504, test_err:0.838805121063, train_err:0.814680880806 \n",
      "\n",
      "7837/7837 [==============================] - 741s - loss: 0.6692   \n",
      "Epoch 83/500\n",
      "7680/7837 [============================>.] - ETA: 7s - loss: 0.6702 \n",
      "Testing loss: 0.720713694242, test_err:0.84894864619, train_err:0.819820720297 \n",
      "\n",
      "7837/7837 [==============================] - 659s - loss: 0.6676   \n",
      "Epoch 84/500\n",
      "7680/7837 [============================>.] - ETA: 8s - loss: 0.6792 \n",
      "Testing loss: 0.715750227169, test_err:0.846020515216, train_err:0.818536332055 \n",
      "\n",
      "7837/7837 [==============================] - 733s - loss: 0.6777   \n",
      "Epoch 85/500\n",
      "7680/7837 [============================>.] - ETA: 8s - loss: 0.6686 \n",
      "Testing loss: 0.696339990168, test_err:0.834469994997, train_err:0.815201356756 \n",
      "\n",
      "7837/7837 [==============================] - 737s - loss: 0.6680   \n",
      "Epoch 86/500\n",
      "7680/7837 [============================>.] - ETA: 8s - loss: 0.6728 \n",
      "Testing loss: 0.692084254294, test_err:0.83191606011, train_err:0.812197830907 \n",
      "\n",
      "7837/7837 [==============================] - 704s - loss: 0.6702   \n",
      "Epoch 87/500\n",
      "7680/7837 [============================>.] - ETA: 7s - loss: 0.6667 \n",
      "Testing loss: 0.712685366066, test_err:0.844206974057, train_err:0.830499730726 \n",
      "\n",
      "7837/7837 [==============================] - 633s - loss: 0.6676   \n",
      "Epoch 88/500\n",
      "7680/7837 [============================>.] - ETA: 7s - loss: 0.6744 \n",
      "Testing loss: 0.698374632427, test_err:0.835688147831, train_err:0.81694635826 \n",
      "\n",
      "7837/7837 [==============================] - 683s - loss: 0.6757   \n",
      "Epoch 89/500\n",
      "7680/7837 [============================>.] - ETA: 7s - loss: 0.6705 \n",
      "Testing loss: 0.704813651163, test_err:0.839531715057, train_err:0.812802714085 \n",
      "\n",
      "7837/7837 [==============================] - 674s - loss: 0.6704   \n",
      "Epoch 90/500\n",
      "7680/7837 [============================>.] - ETA: 7s - loss: 0.6635 \n",
      "Testing loss: 0.689875707578, test_err:0.830587667793, train_err:0.81086317938 \n",
      "\n",
      "7837/7837 [==============================] - 680s - loss: 0.6655   \n",
      "Epoch 91/500\n",
      "7680/7837 [============================>.] - ETA: 7s - loss: 0.6660 \n",
      "Testing loss: 0.69969905031, test_err:0.836480251023, train_err:0.820650196711 \n",
      "\n",
      "7837/7837 [==============================] - 635s - loss: 0.6649   \n",
      "Epoch 92/500\n",
      "7680/7837 [============================>.] - ETA: 7s - loss: 0.6832 \n",
      "Testing loss: 0.692715007675, test_err:0.832295205979, train_err:0.808730692596 \n",
      "\n",
      "7837/7837 [==============================] - 664s - loss: 0.6815   \n",
      "Epoch 93/500\n",
      "7680/7837 [============================>.] - ETA: 7s - loss: 0.6693 \n",
      "Testing loss: 0.701432669406, test_err:0.837515549462, train_err:0.811147113436 \n",
      "\n",
      "7837/7837 [==============================] - 696s - loss: 0.6684   \n",
      "Epoch 94/500\n",
      "7680/7837 [============================>.] - ETA: 8s - loss: 0.6655 \n",
      "Testing loss: 0.688615325032, test_err:0.829828650784, train_err:0.812053643084 \n",
      "\n",
      "7837/7837 [==============================] - 770s - loss: 0.6665   \n",
      "Epoch 95/500\n",
      "7680/7837 [============================>.] - ETA: 8s - loss: 0.6730 \n",
      "Testing loss: 0.701369553683, test_err:0.837477659348, train_err:0.8111313759 \n",
      "\n",
      "7837/7837 [==============================] - 614s - loss: 0.6729   \n",
      "Epoch 96/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6655 \n",
      "Testing loss: 0.712230264654, test_err:0.843938327215, train_err:0.821420747867 \n",
      "\n",
      "7837/7837 [==============================] - 332s - loss: 0.6637   \n",
      "Epoch 97/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6599 \n",
      "Testing loss: 0.687629487563, test_err:0.829234789388, train_err:0.807236247456 \n",
      "\n",
      "7837/7837 [==============================] - 334s - loss: 0.6591   \n",
      "Epoch 98/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6562 \n",
      "Testing loss: 0.687841420028, test_err:0.829362425943, train_err:0.806154473302 \n",
      "\n",
      "7837/7837 [==============================] - 334s - loss: 0.6534   \n",
      "Epoch 99/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6737 \n",
      "Testing loss: 0.703114102325, test_err:0.838519310283, train_err:0.811356224456 \n",
      "\n",
      "7837/7837 [==============================] - 334s - loss: 0.6737   \n",
      "Epoch 100/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6605 \n",
      "Testing loss: 0.685768783092, test_err:0.828111950964, train_err:0.806596819906 \n",
      "\n",
      "7837/7837 [==============================] - 339s - loss: 0.6609   \n",
      "Epoch 101/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6590 \n",
      "Testing loss: 0.705106637186, test_err:0.839705881395, train_err:0.813470535595 \n",
      "\n",
      "7837/7837 [==============================] - 334s - loss: 0.6583   \n",
      "Epoch 102/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6843 \n",
      "Testing loss: 0.682869356749, test_err:0.826359307794, train_err:0.806175586387 \n",
      "\n",
      "7837/7837 [==============================] - 336s - loss: 0.6831   \n",
      "Epoch 103/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6748 \n",
      "Testing loss: 0.722273777212, test_err:0.849865420218, train_err:0.833285046276 \n",
      "\n",
      "7837/7837 [==============================] - 332s - loss: 0.6769   \n",
      "Epoch 104/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6712 \n",
      "Testing loss: 0.680979891942, test_err:0.825215154637, train_err:0.804917945929 \n",
      "\n",
      "7837/7837 [==============================] - 333s - loss: 0.6727   \n",
      "Epoch 105/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6665 \n",
      "Testing loss: 0.73022446827, test_err:0.854531649809, train_err:0.827734712549 \n",
      "\n",
      "7837/7837 [==============================] - 334s - loss: 0.6646   \n",
      "Epoch 106/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6876 \n",
      "Testing loss: 0.714940884649, test_err:0.845541874355, train_err:0.817902075528 \n",
      "\n",
      "7837/7837 [==============================] - 329s - loss: 0.6846   \n",
      "Epoch 107/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6624 \n",
      "Testing loss: 0.680517824572, test_err:0.824935100851, train_err:0.80624418127 \n",
      "\n",
      "7837/7837 [==============================] - 336s - loss: 0.6620   \n",
      "Epoch 108/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6502 \n",
      "Testing loss: 0.685265401918, test_err:0.827807514734, train_err:0.806160910108 \n",
      "\n",
      "7837/7837 [==============================] - 340s - loss: 0.6557   \n",
      "Epoch 109/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6666 \n",
      "Testing loss: 0.698290897146, test_err:0.83563793304, train_err:0.819784859305 \n",
      "\n",
      "7837/7837 [==============================] - 344s - loss: 0.6654   \n",
      "Epoch 110/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6604 \n",
      "Testing loss: 0.679007587384, test_err:0.824019165609, train_err:0.804911462292 \n",
      "\n",
      "7837/7837 [==============================] - 333s - loss: 0.6647   \n",
      "Epoch 111/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6591 \n",
      "Testing loss: 0.719588945836, test_err:0.848284854652, train_err:0.827408744379 \n",
      "\n",
      "7837/7837 [==============================] - 339s - loss: 0.6583   \n",
      "Epoch 112/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6736 \n",
      "Testing loss: 0.708733560844, test_err:0.841863412868, train_err:0.829409162885 \n",
      "\n",
      "7837/7837 [==============================] - 348s - loss: 0.6714   \n",
      "Epoch 113/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6603 \n",
      "Testing loss: 0.68353752433, test_err:0.826763352292, train_err:0.811634265337 \n",
      "\n",
      "7837/7837 [==============================] - 327s - loss: 0.6617   \n",
      "Epoch 114/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6639 \n",
      "Testing loss: 0.699717859833, test_err:0.836491364823, train_err:0.809422743847 \n",
      "\n",
      "7837/7837 [==============================] - 335s - loss: 0.6632   \n",
      "Epoch 115/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6686 \n",
      "Testing loss: 0.721735392785, test_err:0.849550255726, train_err:0.820385950613 \n",
      "\n",
      "7837/7837 [==============================] - 343s - loss: 0.6659   \n",
      "Epoch 116/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6630 \n",
      "Testing loss: 0.719675998055, test_err:0.848338698954, train_err:0.831462460895 \n",
      "\n",
      "7837/7837 [==============================] - 337s - loss: 0.6621   \n",
      "Epoch 117/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6552 \n",
      "Testing loss: 0.696022499094, test_err:0.834280849945, train_err:0.817647248304 \n",
      "\n",
      "7837/7837 [==============================] - 336s - loss: 0.6599   \n",
      "Epoch 118/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6656 \n",
      "Testing loss: 0.739064621682, test_err:0.859689516369, train_err:0.834762089737 \n",
      "\n",
      "7837/7837 [==============================] - 337s - loss: 0.6694   \n",
      "Epoch 119/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6945 \n",
      "Testing loss: 0.687581034096, test_err:0.829205980012, train_err:0.806852233683 \n",
      "\n",
      "7837/7837 [==============================] - 338s - loss: 0.6947   \n",
      "Epoch 120/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6784 \n",
      "Testing loss: 0.680380259485, test_err:0.824852183575, train_err:0.802906446878 \n",
      "\n",
      "7837/7837 [==============================] - 333s - loss: 0.6763   \n",
      "Epoch 121/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6521 \n",
      "Testing loss: 0.683084056572, test_err:0.826489604911, train_err:0.805928006689 \n",
      "\n",
      "7837/7837 [==============================] - 336s - loss: 0.6523   \n",
      "Epoch 122/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6538 \n",
      "Testing loss: 0.722704016919, test_err:0.850120676454, train_err:0.821610935207 \n",
      "\n",
      "7837/7837 [==============================] - 349s - loss: 0.6535   \n",
      "Epoch 123/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6579 \n",
      "Testing loss: 0.672424584019, test_err:0.820015075502, train_err:0.80101108585 \n",
      "\n",
      "7837/7837 [==============================] - 339s - loss: 0.6581   \n",
      "Epoch 124/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6975 \n",
      "Testing loss: 0.72764742715, test_err:0.853022711818, train_err:0.822736091781 \n",
      "\n",
      "7837/7837 [==============================] - 335s - loss: 0.6958   \n",
      "Epoch 125/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6647 \n",
      "Testing loss: 0.684947243029, test_err:0.827615502896, train_err:0.802576188406 \n",
      "\n",
      "7837/7837 [==============================] - 328s - loss: 0.6672   \n",
      "Epoch 126/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6517 \n",
      "Testing loss: 0.701115143786, test_err:0.837325702823, train_err:0.812445133372 \n",
      "\n",
      "7837/7837 [==============================] - 339s - loss: 0.6502   \n",
      "Epoch 127/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6570 \n",
      "Testing loss: 0.711374706152, test_err:0.843430452519, train_err:0.814390291049 \n",
      "\n",
      "7837/7837 [==============================] - 338s - loss: 0.6554   \n",
      "Epoch 128/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6626 \n",
      "Testing loss: 0.672691758068, test_err:0.820177839716, train_err:0.802203985668 \n",
      "\n",
      "7837/7837 [==============================] - 338s - loss: 0.6623   \n",
      "Epoch 129/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6544 \n",
      "Testing loss: 0.672766398897, test_err:0.820223393145, train_err:0.802504343327 \n",
      "\n",
      "7837/7837 [==============================] - 335s - loss: 0.6524   \n",
      "Epoch 130/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6489 \n",
      "Testing loss: 0.70428806811, test_err:0.839218186286, train_err:0.814225666799 \n",
      "\n",
      "7837/7837 [==============================] - 339s - loss: 0.6492   \n",
      "Epoch 131/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6516 \n",
      "Testing loss: 0.681932642265, test_err:0.825792182866, train_err:0.81033709457 \n",
      "\n",
      "7837/7837 [==============================] - 334s - loss: 0.6504   \n",
      "Epoch 132/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6499 \n",
      "Testing loss: 0.722049735517, test_err:0.849733542557, train_err:0.829774155743 \n",
      "\n",
      "7837/7837 [==============================] - 340s - loss: 0.6506   \n",
      "Epoch 133/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6514 \n",
      "Testing loss: 0.685633964198, test_err:0.828030333681, train_err:0.803478701711 \n",
      "\n",
      "7837/7837 [==============================] - 333s - loss: 0.6505   \n",
      "Epoch 134/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6556 \n",
      "Testing loss: 0.677329198925, test_err:0.822999795426, train_err:0.800296399075 \n",
      "\n",
      "7837/7837 [==============================] - 341s - loss: 0.6546   \n",
      "Epoch 135/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6681 \n",
      "Testing loss: 0.712654677702, test_err:0.84418748316, train_err:0.81819308323 \n",
      "\n",
      "7837/7837 [==============================] - 338s - loss: 0.6707   \n",
      "Epoch 136/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6506 \n",
      "Testing loss: 0.73930607207, test_err:0.859829010966, train_err:0.844973413277 \n",
      "\n",
      "7837/7837 [==============================] - 334s - loss: 0.6535   \n",
      "Epoch 137/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6704 \n",
      "Testing loss: 0.670399127931, test_err:0.818779169315, train_err:0.797446972303 \n",
      "\n",
      "7837/7837 [==============================] - 329s - loss: 0.6703   \n",
      "Epoch 138/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6504 \n",
      "Testing loss: 0.670994720167, test_err:0.819142446612, train_err:0.797621826834 \n",
      "\n",
      "7837/7837 [==============================] - 336s - loss: 0.6523   \n",
      "Epoch 139/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6471 \n",
      "Testing loss: 0.730452986639, test_err:0.854666871388, train_err:0.826955116943 \n",
      "\n",
      "7837/7837 [==============================] - 337s - loss: 0.6472   \n",
      "Epoch 140/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6841 \n",
      "Testing loss: 0.709385724457, test_err:0.842250592283, train_err:0.813754560786 \n",
      "\n",
      "7837/7837 [==============================] - 338s - loss: 0.6832   \n",
      "Epoch 141/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6569 \n",
      "Testing loss: 0.673520337076, test_err:0.820683567057, train_err:0.803662362923 \n",
      "\n",
      "7837/7837 [==============================] - 337s - loss: 0.6563   \n",
      "Epoch 142/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6775 \n",
      "Testing loss: 0.695307086925, test_err:0.833851140514, train_err:0.823595620852 \n",
      "\n",
      "7837/7837 [==============================] - 329s - loss: 0.6806   \n",
      "Epoch 143/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6453 \n",
      "Testing loss: 0.743599650325, test_err:0.862322520685, train_err:0.835940102101 \n",
      "\n",
      "7837/7837 [==============================] - 336s - loss: 0.6478   \n",
      "Epoch 144/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6858 \n",
      "Testing loss: 0.694982851038, test_err:0.833657356945, train_err:0.809572303456 \n",
      "\n",
      "7837/7837 [==============================] - 333s - loss: 0.6849   \n",
      "Epoch 145/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6549 \n",
      "Testing loss: 0.685314900291, test_err:0.827837790499, train_err:0.813979756165 \n",
      "\n",
      "7837/7837 [==============================] - 338s - loss: 0.6562   \n",
      "Epoch 146/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6574 \n",
      "Testing loss: 0.675443356378, test_err:0.82185376868, train_err:0.797612830138 \n",
      "\n",
      "7837/7837 [==============================] - 334s - loss: 0.6557   \n",
      "Epoch 147/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6657 \n",
      "Testing loss: 0.677021666692, test_err:0.822812755598, train_err:0.805581818324 \n",
      "\n",
      "7837/7837 [==============================] - 341s - loss: 0.6679   \n",
      "Epoch 148/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6660 \n",
      "Testing loss: 0.700968794433, test_err:0.837238781081, train_err:0.810971753442 \n",
      "\n",
      "7837/7837 [==============================] - 333s - loss: 0.6649   \n",
      "Epoch 149/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6509 \n",
      "Testing loss: 0.714029898449, test_err:0.845002370855, train_err:0.820092929268 \n",
      "\n",
      "7837/7837 [==============================] - 337s - loss: 0.6503   \n",
      "Epoch 150/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6546 \n",
      "Testing loss: 0.670758460006, test_err:0.818998415501, train_err:0.797582932391 \n",
      "\n",
      "7837/7837 [==============================] - 333s - loss: 0.6577   \n",
      "Epoch 151/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6750 \n",
      "Testing loss: 0.675878436225, test_err:0.822118247515, train_err:0.806596211806 \n",
      "\n",
      "7837/7837 [==============================] - 341s - loss: 0.6744   \n",
      "Epoch 152/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6560 \n",
      "Testing loss: 0.685396433606, test_err:0.827886185323, train_err:0.814329205435 \n",
      "\n",
      "7837/7837 [==============================] - 334s - loss: 0.6552   \n",
      "Epoch 153/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6627 \n",
      "Testing loss: 0.692948706539, test_err:0.832435605391, train_err:0.820267365305 \n",
      "\n",
      "7837/7837 [==============================] - 342s - loss: 0.6617   \n",
      "Epoch 154/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6746 \n",
      "Testing loss: 0.744431853781, test_err:0.862804765279, train_err:0.832029516262 \n",
      "\n",
      "7837/7837 [==============================] - 340s - loss: 0.6767   \n",
      "Epoch 155/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6695 \n",
      "Testing loss: 0.774072000445, test_err:0.87981554341, train_err:0.85298722958 \n",
      "\n",
      "7837/7837 [==============================] - 340s - loss: 0.6714   \n",
      "Epoch 156/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6470 \n",
      "Testing loss: 0.663336691564, test_err:0.81445483583, train_err:0.797065924487 \n",
      "\n",
      "7837/7837 [==============================] - 329s - loss: 0.6518   \n",
      "Epoch 157/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6476 \n",
      "Testing loss: 0.67934860356, test_err:0.824226299865, train_err:0.813504739379 \n",
      "\n",
      "7837/7837 [==============================] - 335s - loss: 0.6470   \n",
      "Epoch 158/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6472 \n",
      "Testing loss: 0.697048956034, test_err:0.834895441629, train_err:0.807667381186 \n",
      "\n",
      "7837/7837 [==============================] - 337s - loss: 0.6478   \n",
      "Epoch 159/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6389 \n",
      "Testing loss: 0.692656756907, test_err:0.832260917876, train_err:0.820466457912 \n",
      "\n",
      "7837/7837 [==============================] - 343s - loss: 0.6426   \n",
      "Epoch 160/500\n",
      "7680/7837 [============================>.] - ETA: 4s - loss: 0.6409 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:34: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:34: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=False, recurrent_initializer=<keras.ini..., kernel_initializer=<keras.ini..., input_shape=(None, 1))`\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-850ea712a389>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdisc_xtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc_ytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m            \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m            \u001b[0mTestCallback\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdisc_xtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc_ytest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdisc_xtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc_ytrain\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m            \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    864\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1502\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1504\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1174\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guo/nn_work/tsRNN/utils_keras.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0merr_ts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpy_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_ts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_ts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         print('\\nTesting loss: {}, test_err:{}, train_err:{} \\n'.format(\\\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[0;32m    893\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m                                    sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[0;32m   1546\u001b[0m         return self._test_loop(f, ins,\n\u001b[0;32m   1547\u001b[0m                                \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1548\u001b[1;33m                                verbose=verbose)\n\u001b[0m\u001b[0;32m   1549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_test_loop\u001b[1;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1270\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1272\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1273\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    987\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    988\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 989\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mp\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m                                                 self, node)\n\u001b[0m\u001b[0;32m    979\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/guo/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/scan_perform/mod.cpp:6946)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/tensor/type.pyc\u001b[0m in \u001b[0;36mvalue_zeros\u001b[1;34m(self, shape)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mvalue_zeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m         \"\"\"\n\u001b[0;32m    553\u001b[0m         \u001b[0mCreate\u001b[0m \u001b[0man\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mof\u001b[0m \u001b[1;36m0\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# plain discriminative \n",
    "\n",
    "# network set-up\n",
    "in_out_neurons = 1\n",
    "hidden_neurons = 128\n",
    "win_size = 200\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "disc_xtrain = np.reshape( xtrain, [-1, win_size, 1] )\n",
    "disc_xtest  = np.reshape( xtest, [-1, win_size, 1] )\n",
    "\n",
    "disc_ytrain = ytrain\n",
    "disc_ytest  = ytest\n",
    "\n",
    "print np.shape(disc_xtrain),np.shape(disc_ytrain),np.shape(disc_xtest),\\\n",
    "np.shape(disc_ytest)\n",
    "\n",
    "# optimizer\n",
    "sgd  = SGD(lr = 0.01, momentum = 0.9, nesterov = True)\n",
    "rms  = RMSprop(lr = 0.05,  rho = 0.9, epsilon  = 1e-08, decay = 0.0)\n",
    "\n",
    "\n",
    "adam = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add( LSTM(hidden_neurons, input_dim = in_out_neurons, return_sequences = False, \\\n",
    "                input_shape = [batch_size, win_size, in_out_neurons ], \\\n",
    "#               input_length = win_size, \\\n",
    "#               activation ='tanh',\\\n",
    "#               dropout = 0.1,\\\n",
    "#               kernel_regularizer = l2(0.2), \n",
    "#               recurrent_regularizer = l2(0.1),\\\n",
    "#               !change!\n",
    "                kernel_initializer    = glorot_normal(), \\\n",
    "                recurrent_initializer = glorot_normal() ))\n",
    "# change: activiation\n",
    "\n",
    "# model.add( LSTM(hidden_neurons, return_sequences = False, \\\n",
    "# #               input_shape = [batch_size, win_size, in_out_neurons ], \\\n",
    "# #               input_length = win_size, \\\n",
    "# #               activation ='tanh',\\\n",
    "# #               dropout = 0.1,\\\n",
    "# #               kernel_regularizer = l2(0.2), \n",
    "# #               recurrent_regularizer = l2(0.1),\\\n",
    "# #               !change!\n",
    "#                 kernel_initializer    = glorot_normal(), \\\n",
    "#                 recurrent_initializer = glorot_normal() ))\n",
    "\n",
    "\n",
    "# identity ini\n",
    "# he's ini:   he_normal()\n",
    "# xavier ini: glorot_normal()\n",
    "# orthogonal: Orthogonal()\n",
    "\n",
    "\n",
    "# model.add(Dropout(0.1))\n",
    "model.add(Dense(64,\\\n",
    "#                   kernel_regularizer = l2(0.1), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(32, \\\n",
    "#                   kernel_regularizer = l2(0.01), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(16, \\\n",
    "#                   kernel_regularizer = l2(0.01), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(1,  \\\n",
    "#                   kernel_regularizer = l2(0.005), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"linear\"))\n",
    "\n",
    "\n",
    "# training\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = adam)\n",
    "\n",
    "\n",
    "model.fit( disc_xtrain, disc_ytrain, shuffle = True, \\\n",
    "           callbacks = [ \\\n",
    "           TestCallback( (disc_xtest, disc_ytest), (disc_xtrain, disc_ytrain) ) ], \\\n",
    "           batch_size = batch_size, epochs = 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generative model\n",
    "\n",
    "# set-up\n",
    "in_out_neurons = 1\n",
    "hidden_neurons = 256\n",
    "win_size = 200    \n",
    "    \n",
    "# prepare data\n",
    "ytrain = expand_y( xtrain_df.as_matrix(), ytrain_df.as_matrix() )\n",
    "ytest  = expand_y( xtest_df.as_matrix(),  ytest_df.as_matrix()  )\n",
    "\n",
    "gen_xtrain = np.reshape( xtrain, [-1, win_size, 1] )\n",
    "gen_ytrain = np.reshape( ytrain, [-1, win_size, 1] )\n",
    "\n",
    "gen_xtest  = np.reshape( xtest, [-1, win_size, 1] )\n",
    "gen_ytest  = np.reshape( ytest, [-1, win_size, 1] )\n",
    "\n",
    "print np.shape(gen_xtrain), np.shape(gen_ytrain), np.shape(gen_xtest), np.shape(gen_ytest)\n",
    "\n",
    "\n",
    "# optimizer \n",
    "adam  = Adam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add( LSTM(hidden_neurons, input_dim = in_out_neurons, return_sequences = True, \\\n",
    "                input_length = win_size,\\\n",
    "                activation   = 'tanh',\\\n",
    "#                dropout = 0.1,\\\n",
    "#                kernel_regularizer = l2(0.15), \\\n",
    "#                recurrent_regularizer = l2(0.15), \\\n",
    "                kernel_initializer    = glorot_normal(), \\\n",
    "                recurrent_initializer = glorot_normal() ) )\n",
    "\n",
    "\n",
    "# identity ini\n",
    "# he's ini:   he_normal()\n",
    "# xavier ini: glorot_normal()\n",
    "# orthogonal: Orthogonal() \n",
    "\n",
    "\n",
    "# model.add(Dropout(0.1))\n",
    "model.add(TimeDistributed(Dense(128, activation = 'relu',\\\n",
    "#                                     kernel_regularizer = l2(0.1), \\\n",
    "                                    kernel_initializer = he_normal() ) ))\n",
    "\n",
    "model.add(TimeDistributed(Dense(64, activation = 'relu',\\\n",
    "#                                     kernel_regularizer = l2(0.01), \\\n",
    "                                    kernel_initializer = he_normal() ) ))\n",
    "\n",
    "model.add(TimeDistributed(Dense(1,  activation = 'linear',\\\n",
    "#                                     kernel_regularizer = l2(0.01), \\\n",
    "                                    kernel_initializer = he_normal() ) ))\n",
    "\n",
    "\n",
    "# training\n",
    "model.compile( loss = \"mean_squared_error\", optimizer = adam )\n",
    "\n",
    "\n",
    "model.fit( gen_xtrain, gen_ytrain, shuffle=True,  \\\n",
    "           callbacks = [ TestCallback_Generative( \\\n",
    "                         (gen_xtest, gen_ytest), (gen_xtrain, gen_ytrain) ) ], \\\n",
    "                         batch_size = 128, epochs = 500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# discriminative with non-overlapping local data\n",
    "\n",
    "#  network set-up\n",
    "local_size = 5\n",
    "in_out_neurons = local_size\n",
    "hidden_neurons = 512\n",
    "win_size = 200/5\n",
    "\n",
    "# validation on each epoch \n",
    "class TestCallback_insight(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, mse = self.model.evaluate(x, y, verbose=0)\n",
    "        \n",
    "        py = self.model.predict(x, verbose=0)\n",
    "        cnt = len(y)\n",
    "        err = [ (y[i][0]-py[i][0])**2 for i in range(cnt)]\n",
    "        \n",
    "        print('\\nTesting loss: {}, acc: {}, mean: {} \\n'.format(\\\n",
    "               loss, sqrt(mse), mean(err) ))\n",
    "\n",
    "def expand_x_local_non_overlapping( local_size, list_data):\n",
    "    \n",
    "    cnt = len(list_data)\n",
    "    steps = len(list_data[0])\n",
    "    tmp_dta = []\n",
    "    \n",
    "    for i in range(cnt):\n",
    "        tmp_dta.append([])\n",
    "        for j in range( local_size-1, steps, local_size ):\n",
    "            tmp_dta[-1].append( list_data[i][ j-local_size+1:j+1 ] )\n",
    "    \n",
    "    return tmp_dta\n",
    "\n",
    "\n",
    "disc_xtrain = np.array( expand_x_local_non_overlapping( local_size, list(xtrain) ) )\n",
    "disc_xtest  = np.array( expand_x_local_non_overlapping( local_size, list(xtest) ) )\n",
    "\n",
    "disc_ytrain = ytrain\n",
    "disc_ytest  = ytest\n",
    "\n",
    "print np.shape(disc_xtrain), np.shape(disc_ytrain), np.shape(disc_xtest), \\\n",
    "np.shape(disc_ytest)\n",
    "\n",
    "# optimizer \n",
    "adam = Adam(lr = 0.007, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_neurons, input_dim = in_out_neurons, return_sequences = False, \\\n",
    "               input_length = win_size, \\\n",
    "               activation = 'tanh', \\\n",
    "#                dropout = 0.1,\\\n",
    "#                kernel_regularizer = l2(0.1),      recurrent_regularizer = l2(0.1), \\\n",
    "               kernel_initializer    = glorot_normal(), \\\n",
    "               recurrent_initializer = glorot_normal() ))\n",
    "# identity ini\n",
    "# he's ini:   he_normal()\n",
    "# xavier ini: glorot_normal()\n",
    "# orthogonal: Orthogonal() \n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# kernel_regularizer = l2(0.1), \\\n",
    "model.add(Dense(128,\\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(64, \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(32, \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(1,\\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"linear\"))\n",
    "\n",
    "\n",
    "# training \n",
    "model.compile(loss=\"mean_squared_error\", optimizer = adam)\n",
    "\n",
    "\n",
    "model.fit( disc_xtrain, disc_ytrain, shuffle=True, \\\n",
    "           callbacks = [ TestCallback( (disc_xtest, disc_ytest), (disc_xtrain, disc_ytrain) ) ], \\\n",
    "           batch_size = 256, epochs = 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
