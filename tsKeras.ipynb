{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# data processing packages\n",
    "import numpy as np   \n",
    "import pandas as pd \n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    " \n",
    "%matplotlib inline    \n",
    "import matplotlib as mplt\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.style.use('ggplot')\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "\n",
    "# machine leanring packages\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation, Dropout, Reshape, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "# RMSprop, Adadelta\n",
    "from keras.regularizers import *\n",
    "from keras.initializers import *\n",
    "from keras.activations import *\n",
    "\n",
    "from utils_keras import *\n",
    "from utils_dataPrepro import *\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notes:\n",
    "\n",
    "# ---Ini:            \n",
    "#     orthogonal: https://smerity.com/articles/2016/orthogonal_init.html\n",
    "#     identity, Xavier, He's\n",
    "\n",
    "# ---Activation:     \n",
    "#     tan, sigmod by default\n",
    "\n",
    "# ---Regularization: \n",
    "#     dropout on non-recurrent connections \n",
    "#     batch normalization\n",
    "\n",
    "# unstable or not decreasing training loss: lr, representability, e.g., number of neursons, layers, etc..\n",
    "# watch out for the amount difference between regularization and loss  \n",
    "# large minibatch -> large lr\n",
    "# large network -> large lr\n",
    "\n",
    "\n",
    "# learning speed, repren, regular\n",
    "\n",
    "# TO DO:\n",
    "    \n",
    "# more component\n",
    "# muliti-input-muliti-output method\n",
    "# log transformation\n",
    "# integrate trend \n",
    "# attention for peridoic time series\n",
    "\n",
    "# multistep ahead:  multi-output rnn\n",
    "#                   seq2seq rnn \n",
    "\n",
    "# Stat-space model: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3040, 200) (3040, 1) (760, 200) (760, 1)\n"
     ]
    }
   ],
   "source": [
    "# stock data\n",
    "\n",
    "files_list=[\"../dataset/dataset_ts/stock_xtrain.dat\", \\\n",
    "            \"../dataset/dataset_ts/stock_xtest.dat\",\\\n",
    "            \"../dataset/dataset_ts/stock_ytrain.dat\", \\\n",
    "            \"../dataset/dataset_ts/stock_ytest.dat\"]\n",
    "\n",
    "xtrain_df = pd.DataFrame( np.load(files_list[0]) )\n",
    "xtest_df  = pd.DataFrame( np.load(files_list[1]) )\n",
    "ytrain_df = pd.DataFrame( np.load(files_list[2]) )\n",
    "ytest_df  = pd.DataFrame( np.load(files_list[3]) )\n",
    "\n",
    "\n",
    "# files_list=[\"../dataset/dataset_ts/power_xtrain.csv\", \\\n",
    "#             \"../dataset/dataset_ts/power_xtest.csv\",\\\n",
    "#             \"../dataset/dataset_ts/power_ytrain.csv\", \\\n",
    "#             \"../dataset/dataset_ts/power_ytest.csv\"]\n",
    "\n",
    "# xtrain_df = pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "# xtest_df  = pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "# ytrain_df = pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "# ytest_df  = pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "xtrain, ytrain, xtest, ytest = prepare_train_test_data( 200, False, xtrain_df, xtest_df, ytrain_df, ytest_df)\n",
    "    \n",
    "print np.shape(xtrain), np.shape(ytrain), np.shape(xtest), np.shape(ytest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3040, 200, 1) (3040, 1) (760, 200, 1) (760, 1)\n",
      "Epoch 1/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 1692548.9432\n",
      "Testing loss: 1274001.30658, test_err:1128.7166653, train_err:1134.52304366 \n",
      "\n",
      "3040/3040 [==============================] - 238s - loss: 1670629.6632    \n",
      "Epoch 2/500\n",
      "2816/3040 [==========================>...] - ETA: 10s - loss: 577618.1882\n",
      "Testing loss: 170779.337007, test_err:413.25455469, train_err:408.130061653 \n",
      "\n",
      "3040/3040 [==============================] - 216s - loss: 549117.8102    \n",
      "Epoch 3/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 93828.6282\n",
      "Testing loss: 56495.6588405, test_err:237.688153522, train_err:246.989565854 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 91936.7611    \n",
      "Epoch 4/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 56690.6946\n",
      "Testing loss: 51846.9339227, test_err:227.699219889, train_err:224.548743943 \n",
      "\n",
      "3040/3040 [==============================] - 226s - loss: 55688.9144    \n",
      "Epoch 5/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 49910.5433\n",
      "Testing loss: 48206.0161595, test_err:219.558685799, train_err:224.026274127 \n",
      "\n",
      "3040/3040 [==============================] - 224s - loss: 49886.3930    \n",
      "Epoch 6/500\n",
      "2816/3040 [==========================>...] - ETA: 10s - loss: 50798.1669\n",
      "Testing loss: 51004.1605674, test_err:225.841009962, train_err:236.137836201 \n",
      "\n",
      "3040/3040 [==============================] - 218s - loss: 51434.1927    \n",
      "Epoch 7/500\n",
      "2816/3040 [==========================>...] - ETA: 10s - loss: 64793.1992\n",
      "Testing loss: 56458.3134457, test_err:237.609572897, train_err:249.654592157 \n",
      "\n",
      "3040/3040 [==============================] - 216s - loss: 65188.2971    \n",
      "Epoch 8/500\n",
      "2816/3040 [==========================>...] - ETA: 10s - loss: 71863.8217\n",
      "Testing loss: 78311.0651316, test_err:279.841142722, train_err:292.861521957 \n",
      "\n",
      "3040/3040 [==============================] - 218s - loss: 71799.5103    \n",
      "Epoch 9/500\n",
      "2816/3040 [==========================>...] - ETA: 10s - loss: 81830.1342\n",
      "Testing loss: 70895.6825658, test_err:266.262425299, train_err:279.919904416 \n",
      "\n",
      "3040/3040 [==============================] - 213s - loss: 82223.9578    \n",
      "Epoch 10/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 74921.0110\n",
      "Testing loss: 58966.7945313, test_err:242.83079866, train_err:255.226393942 \n",
      "\n",
      "3040/3040 [==============================] - 221s - loss: 74250.5961    \n",
      "Epoch 11/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 56946.6744\n",
      "Testing loss: 40867.1847451, test_err:202.156349526, train_err:212.248130214 \n",
      "\n",
      "3040/3040 [==============================] - 239s - loss: 55798.4954    \n",
      "Epoch 12/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 61293.6461\n",
      "Testing loss: 25243.6459498, test_err:158.882490736, train_err:167.556436959 \n",
      "\n",
      "3040/3040 [==============================] - 262s - loss: 61687.7296    \n",
      "Epoch 13/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 33416.6999\n",
      "Testing loss: 13630.8905325, test_err:116.751421503, train_err:121.290209795 \n",
      "\n",
      "3040/3040 [==============================] - 262s - loss: 32196.0400    \n",
      "Epoch 14/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 9081.5775\n",
      "Testing loss: 3104.67259971, test_err:55.7196081051, train_err:58.587057687 \n",
      "\n",
      "3040/3040 [==============================] - 254s - loss: 8743.1926    \n",
      "Epoch 15/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 1758.7204\n",
      "Testing loss: 686.957874178, test_err:26.2098899162, train_err:27.6792614576 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 1693.5572    \n",
      "Epoch 16/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 632.8336\n",
      "Testing loss: 579.187985711, test_err:24.0663087391, train_err:24.5565357417 \n",
      "\n",
      "3040/3040 [==============================] - 259s - loss: 618.2510    \n",
      "Epoch 17/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 447.5005\n",
      "Testing loss: 340.703533293, test_err:18.4581524077, train_err:19.9240643771 \n",
      "\n",
      "3040/3040 [==============================] - 263s - loss: 441.3128    \n",
      "Epoch 18/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 382.0725\n",
      "Testing loss: 341.403486392, test_err:18.4770960118, train_err:19.638733139 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 381.2650    \n",
      "Epoch 19/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 396.4724\n",
      "Testing loss: 336.856852963, test_err:18.3536620326, train_err:19.3194968205 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 392.6052    \n",
      "Epoch 20/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 360.3428\n",
      "Testing loss: 300.360037392, test_err:17.3309079476, train_err:18.5202595949 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 361.8175    \n",
      "Epoch 21/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 358.8409\n",
      "Testing loss: 293.676522827, test_err:17.1369947837, train_err:18.5751374238 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 358.8745    \n",
      "Epoch 22/500\n",
      "2816/3040 [==========================>...] - ETA: 14s - loss: 370.6037\n",
      "Testing loss: 340.708367277, test_err:18.4582795798, train_err:19.8246469958 \n",
      "\n",
      "3040/3040 [==============================] - 316s - loss: 364.5654    \n",
      "Epoch 23/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 371.8998\n",
      "Testing loss: 297.516356619, test_err:17.2486502153, train_err:18.6642209509 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 364.2591    \n",
      "Epoch 24/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 348.4544\n",
      "Testing loss: 302.473144049, test_err:17.391762412, train_err:18.8954536892 \n",
      "\n",
      "3040/3040 [==============================] - 247s - loss: 346.5957    \n",
      "Epoch 25/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 348.6552\n",
      "Testing loss: 275.510203151, test_err:16.5985002127, train_err:17.9856424271 \n",
      "\n",
      "3040/3040 [==============================] - 249s - loss: 344.1412    \n",
      "Epoch 26/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 320.4709\n",
      "Testing loss: 270.442692004, test_err:16.4451443588, train_err:17.7395431133 \n",
      "\n",
      "3040/3040 [==============================] - 246s - loss: 325.3525    \n",
      "Epoch 27/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 317.2515\n",
      "Testing loss: 315.641042609, test_err:17.7662696164, train_err:19.1790962751 \n",
      "\n",
      "3040/3040 [==============================] - 247s - loss: 323.5113    \n",
      "Epoch 28/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 333.4323\n",
      "Testing loss: 270.093091302, test_err:16.434514549, train_err:17.8477787017 \n",
      "\n",
      "3040/3040 [==============================] - 248s - loss: 330.8723    \n",
      "Epoch 29/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 327.6919\n",
      "Testing loss: 293.345355706, test_err:17.1273289348, train_err:18.5104208889 \n",
      "\n",
      "3040/3040 [==============================] - 245s - loss: 331.3874    \n",
      "Epoch 30/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 319.9462\n",
      "Testing loss: 268.978842806, test_err:16.4005759141, train_err:17.7608538274 \n",
      "\n",
      "3040/3040 [==============================] - 245s - loss: 317.5853    \n",
      "Epoch 31/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 319.2015\n",
      "Testing loss: 271.450907336, test_err:16.4757634274, train_err:17.73400733 \n",
      "\n",
      "3040/3040 [==============================] - 245s - loss: 319.5326    \n",
      "Epoch 32/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 316.9760\n",
      "Testing loss: 272.506402427, test_err:16.5077793854, train_err:17.6043482255 \n",
      "\n",
      "3040/3040 [==============================] - 244s - loss: 310.6991    \n",
      "Epoch 33/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 315.5329\n",
      "Testing loss: 261.889252994, test_err:16.1829928295, train_err:17.4351874594 \n",
      "\n",
      "3040/3040 [==============================] - 243s - loss: 320.8214    \n",
      "Epoch 34/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 316.6218\n",
      "Testing loss: 258.320943411, test_err:16.0723673608, train_err:17.3628512556 \n",
      "\n",
      "3040/3040 [==============================] - 242s - loss: 314.0412    \n",
      "Epoch 35/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 296.4598\n",
      "Testing loss: 356.231642231, test_err:18.8740928431, train_err:19.6556726104 \n",
      "\n",
      "3040/3040 [==============================] - 242s - loss: 300.6423    \n",
      "Epoch 36/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 327.6816\n",
      "Testing loss: 264.045454326, test_err:16.2494774672, train_err:17.4840357931 \n",
      "\n",
      "3040/3040 [==============================] - 241s - loss: 326.0461    \n",
      "Epoch 37/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 325.2726\n",
      "Testing loss: 302.336480552, test_err:17.3878088961, train_err:18.6845229414 \n",
      "\n",
      "3040/3040 [==============================] - 241s - loss: 325.1007    \n",
      "Epoch 38/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 320.6729\n",
      "Testing loss: 265.976442839, test_err:16.3087713427, train_err:17.6506384589 \n",
      "\n",
      "3040/3040 [==============================] - 239s - loss: 324.7185    \n",
      "Epoch 39/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 300.2908\n",
      "Testing loss: 249.372314132, test_err:15.7915292121, train_err:17.0095576119 \n",
      "\n",
      "3040/3040 [==============================] - 239s - loss: 302.5138    \n",
      "Epoch 40/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 314.4506\n",
      "Testing loss: 350.977474494, test_err:18.7343693626, train_err:19.4895718274 \n",
      "\n",
      "3040/3040 [==============================] - 240s - loss: 316.5998    \n",
      "Epoch 41/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 361.2545\n",
      "Testing loss: 296.971144024, test_err:17.2328598399, train_err:18.5186294116 \n",
      "\n",
      "3040/3040 [==============================] - 239s - loss: 371.4470    \n",
      "Epoch 42/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 316.9725\n",
      "Testing loss: 287.317977423, test_err:16.9504471683, train_err:18.0728004667 \n",
      "\n",
      "3040/3040 [==============================] - 240s - loss: 318.9074    \n",
      "Epoch 43/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 350.5060\n",
      "Testing loss: 332.247406809, test_err:18.2276465271, train_err:19.465971547 \n",
      "\n",
      "3040/3040 [==============================] - 239s - loss: 347.2694    \n",
      "Epoch 44/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 338.4307\n",
      "Testing loss: 278.776427259, test_err:16.6965875372, train_err:17.6787180425 \n",
      "\n",
      "3040/3040 [==============================] - 241s - loss: 341.9740    \n",
      "Epoch 45/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 305.2902\n",
      "Testing loss: 272.806606092, test_err:16.5168618015, train_err:17.8383204742 \n",
      "\n",
      "3040/3040 [==============================] - 242s - loss: 308.8174    \n",
      "Epoch 46/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 302.7911\n",
      "Testing loss: 298.698898476, test_err:17.2829032808, train_err:18.249950756 \n",
      "\n",
      "3040/3040 [==============================] - 245s - loss: 304.9058    \n",
      "Epoch 47/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 326.7969\n",
      "Testing loss: 317.916178011, test_err:17.8301906569, train_err:19.1142816466 \n",
      "\n",
      "3040/3040 [==============================] - 251s - loss: 328.0458    \n",
      "Epoch 48/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 308.0563\n",
      "Testing loss: 253.068229595, test_err:15.9081241017, train_err:16.9829820284 \n",
      "\n",
      "3040/3040 [==============================] - 253s - loss: 306.9351    \n",
      "Epoch 49/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 300.5569\n",
      "Testing loss: 250.791484311, test_err:15.8364045789, train_err:16.9746370026 \n",
      "\n",
      "3040/3040 [==============================] - 240s - loss: 294.6446    \n",
      "Epoch 50/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 310.6704\n",
      "Testing loss: 241.706154432, test_err:15.5468974996, train_err:16.7104871813 \n",
      "\n",
      "3040/3040 [==============================] - 245s - loss: 319.4418    \n",
      "Epoch 51/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 290.6121\n",
      "Testing loss: 271.764120162, test_err:16.4852803493, train_err:17.7090773269 \n",
      "\n",
      "3040/3040 [==============================] - 245s - loss: 287.9890    \n",
      "Epoch 52/500\n",
      "2816/3040 [==========================>...] - ETA: 15s - loss: 288.2199\n",
      "Testing loss: 261.091080194, test_err:16.1583192063, train_err:17.353957389 \n",
      "\n",
      "3040/3040 [==============================] - 338s - loss: 291.6590    \n",
      "Epoch 53/500\n",
      "2816/3040 [==========================>...] - ETA: 17s - loss: 308.7866\n",
      "Testing loss: 301.659586696, test_err:17.3683393587, train_err:18.2318179145 \n",
      "\n",
      "3040/3040 [==============================] - 338s - loss: 309.5412    \n",
      "Epoch 54/500\n",
      "2816/3040 [==========================>...] - ETA: 15s - loss: 315.5919\n",
      "Testing loss: 326.355602064, test_err:18.065313925, train_err:19.2975000366 \n",
      "\n",
      "3040/3040 [==============================] - 303s - loss: 310.0220    \n",
      "Epoch 55/500\n",
      "2816/3040 [==========================>...] - ETA: 17s - loss: 307.3914\n",
      "Testing loss: 236.646093429, test_err:15.383305927, train_err:16.5116706974 \n",
      "\n",
      "3040/3040 [==============================] - 392s - loss: 305.0727    \n",
      "Epoch 56/500\n",
      "2816/3040 [==========================>...] - ETA: 17s - loss: 300.2018\n",
      "Testing loss: 243.460392681, test_err:15.6032195263, train_err:16.7571065834 \n",
      "\n",
      "3040/3040 [==============================] - 382s - loss: 308.4647    \n",
      "Epoch 57/500\n",
      "2816/3040 [==========================>...] - ETA: 16s - loss: 330.8917\n",
      "Testing loss: 268.09029284, test_err:16.3734567008, train_err:17.600025807 \n",
      "\n",
      "3040/3040 [==============================] - 352s - loss: 326.8484    \n",
      "Epoch 58/500\n",
      "2816/3040 [==========================>...] - ETA: 21s - loss: 282.4741\n",
      "Testing loss: 234.040931139, test_err:15.2983965749, train_err:16.4890839295 \n",
      "\n",
      "3040/3040 [==============================] - 429s - loss: 287.1842    \n",
      "Epoch 59/500\n",
      "2816/3040 [==========================>...] - ETA: 23s - loss: 278.8339\n",
      "Testing loss: 242.213474314, test_err:15.563205915, train_err:16.7490070674 \n",
      "\n",
      "3040/3040 [==============================] - 422s - loss: 279.0070    \n",
      "Epoch 60/500\n",
      "2816/3040 [==========================>...] - ETA: 17s - loss: 283.7250\n",
      "Testing loss: 235.719530768, test_err:15.3531651192, train_err:16.406863545 \n",
      "\n",
      "3040/3040 [==============================] - 358s - loss: 283.1107    \n",
      "Epoch 61/500\n",
      "2816/3040 [==========================>...] - ETA: 20s - loss: 302.2320\n",
      "Testing loss: 239.949089934, test_err:15.4902886475, train_err:16.5838680775 \n",
      "\n",
      "3040/3040 [==============================] - 434s - loss: 300.6438    \n",
      "Epoch 62/500\n",
      "2816/3040 [==========================>...] - ETA: 22s - loss: 281.8936\n",
      "Testing loss: 266.127594476, test_err:16.3134355442, train_err:17.2591719378 \n",
      "\n",
      "3040/3040 [==============================] - 465s - loss: 283.1522    \n",
      "Epoch 63/500\n",
      "2816/3040 [==========================>...] - ETA: 14s - loss: 313.2643\n",
      "Testing loss: 263.460214394, test_err:16.2314429937, train_err:17.290952373 \n",
      "\n",
      "3040/3040 [==============================] - 301s - loss: 311.6946    \n",
      "Epoch 64/500\n",
      "2816/3040 [==========================>...] - ETA: 16s - loss: 290.1951\n",
      "Testing loss: 240.107424927, test_err:15.4953925524, train_err:16.465816801 \n",
      "\n",
      "3040/3040 [==============================] - 347s - loss: 298.6897    \n",
      "Epoch 65/500\n",
      "2816/3040 [==========================>...] - ETA: 18s - loss: 379.1737\n",
      "Testing loss: 268.947269962, test_err:16.3996196811, train_err:17.6082806913 \n",
      "\n",
      "3040/3040 [==============================] - 419s - loss: 373.5812    \n",
      "Epoch 66/500\n",
      "2816/3040 [==========================>...] - ETA: 18s - loss: 294.0103\n",
      "Testing loss: 265.448193038, test_err:16.2925705367, train_err:17.3271394173 \n",
      "\n",
      "3040/3040 [==============================] - 351s - loss: 290.0542    \n",
      "Epoch 67/500\n",
      "2816/3040 [==========================>...] - ETA: 16s - loss: 318.2517\n",
      "Testing loss: 419.583224005, test_err:20.4837039462, train_err:21.0473439238 \n",
      "\n",
      "3040/3040 [==============================] - 340s - loss: 325.4650    \n",
      "Epoch 68/500\n",
      "2816/3040 [==========================>...] - ETA: 17s - loss: 319.1539\n",
      "Testing loss: 232.514609889, test_err:15.248431278, train_err:16.506182407 \n",
      "\n",
      "3040/3040 [==============================] - 357s - loss: 314.0335    \n",
      "Epoch 69/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 286.1419\n",
      "Testing loss: 295.992332699, test_err:17.2044140468, train_err:18.0185936559 \n",
      "\n",
      "3040/3040 [==============================] - 250s - loss: 286.4080    \n",
      "Epoch 70/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 276.0303\n",
      "Testing loss: 255.782144326, test_err:15.9931954067, train_err:17.176638128 \n",
      "\n",
      "3040/3040 [==============================] - 239s - loss: 280.5886    \n",
      "Epoch 71/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 303.1132\n",
      "Testing loss: 228.082028038, test_err:15.1023857191, train_err:16.2028487298 \n",
      "\n",
      "3040/3040 [==============================] - 234s - loss: 304.6370    \n",
      "Epoch 72/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 350.9553\n",
      "Testing loss: 597.563200941, test_err:24.445084502, train_err:25.0349576551 \n",
      "\n",
      "3040/3040 [==============================] - 239s - loss: 346.1269    \n",
      "Epoch 73/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 447.2020\n",
      "Testing loss: 307.357423481, test_err:17.5316017939, train_err:18.2132091687 \n",
      "\n",
      "3040/3040 [==============================] - 238s - loss: 441.4635    \n",
      "Epoch 74/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 287.6760\n",
      "Testing loss: 250.574413741, test_err:15.8295424607, train_err:16.7923824691 \n",
      "\n",
      "3040/3040 [==============================] - 240s - loss: 286.5944    \n",
      "Epoch 75/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 269.3803\n",
      "Testing loss: 302.914442524, test_err:17.4044314473, train_err:18.2312088169 \n",
      "\n",
      "3040/3040 [==============================] - 246s - loss: 270.0354    \n",
      "Epoch 76/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 321.6463\n",
      "Testing loss: 275.084371466, test_err:16.5856652526, train_err:17.6047408688 \n",
      "\n",
      "3040/3040 [==============================] - 240s - loss: 319.3184    \n",
      "Epoch 77/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 324.1215\n",
      "Testing loss: 235.136047685, test_err:15.3341479521, train_err:16.3671643467 \n",
      "\n",
      "3040/3040 [==============================] - 244s - loss: 331.4238    \n",
      "Epoch 78/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 279.6389\n",
      "Testing loss: 277.77174273, test_err:16.6664753471, train_err:17.7921736445 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 280.7824    \n",
      "Epoch 79/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 290.2765\n",
      "Testing loss: 234.508824238, test_err:15.3136755329, train_err:16.4507474955 \n",
      "\n",
      "3040/3040 [==============================] - 241s - loss: 295.7295    \n",
      "Epoch 80/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 295.8880\n",
      "Testing loss: 327.062494218, test_err:18.084876491, train_err:18.8479456491 \n",
      "\n",
      "3040/3040 [==============================] - 238s - loss: 291.6832    \n",
      "Epoch 81/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 324.5666\n",
      "Testing loss: 229.863500013, test_err:15.1612546615, train_err:16.1953209049 \n",
      "\n",
      "3040/3040 [==============================] - 236s - loss: 322.2859    \n",
      "Epoch 82/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 266.8899\n",
      "Testing loss: 224.133745053, test_err:14.971095295, train_err:16.0541713255 \n",
      "\n",
      "3040/3040 [==============================] - 237s - loss: 271.6040    \n",
      "Epoch 83/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 283.2018\n",
      "Testing loss: 334.336737382, test_err:18.2848503456, train_err:19.3408146195 \n",
      "\n",
      "3040/3040 [==============================] - 359s - loss: 289.7786    \n",
      "Epoch 84/500\n",
      "2816/3040 [==========================>...] - ETA: 24s - loss: 340.3971\n",
      "Testing loss: 244.463683118, test_err:15.6353216218, train_err:16.7865409587 \n",
      "\n",
      "3040/3040 [==============================] - 556s - loss: 346.8395    \n",
      "Epoch 85/500\n",
      "2816/3040 [==========================>...] - ETA: 22s - loss: 314.0639\n",
      "Testing loss: 382.401296836, test_err:19.5551015009, train_err:20.1059184526 \n",
      "\n",
      "3040/3040 [==============================] - 407s - loss: 310.5340    \n",
      "Epoch 86/500\n",
      "2816/3040 [==========================>...] - ETA: 19s - loss: 320.7493\n",
      "Testing loss: 312.677904952, test_err:17.6827144457, train_err:18.4565227944 \n",
      "\n",
      "3040/3040 [==============================] - 421s - loss: 321.0826    \n",
      "Epoch 87/500\n",
      "2816/3040 [==========================>...] - ETA: 19s - loss: 285.4413\n",
      "Testing loss: 231.890909456, test_err:15.2279550484, train_err:16.4149592189 \n",
      "\n",
      "3040/3040 [==============================] - 364s - loss: 285.3717    \n",
      "Epoch 88/500\n",
      "2816/3040 [==========================>...] - ETA: 16s - loss: 272.1994\n",
      "Testing loss: 231.330773765, test_err:15.2095723189, train_err:16.3179435882 \n",
      "\n",
      "3040/3040 [==============================] - 315s - loss: 276.1619    \n",
      "Epoch 89/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 290.8751\n",
      "Testing loss: 265.81465695, test_err:16.30384206, train_err:17.151569918 \n",
      "\n",
      "3040/3040 [==============================] - 251s - loss: 287.8049    \n",
      "Epoch 90/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 289.4127\n",
      "Testing loss: 238.639746736, test_err:15.4479817286, train_err:16.4026676627 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 286.7821    \n",
      "Epoch 91/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 339.2299\n",
      "Testing loss: 223.00069548, test_err:14.9332145833, train_err:15.9762007052 \n",
      "\n",
      "3040/3040 [==============================] - 252s - loss: 354.1712    \n",
      "Epoch 92/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 424.0273\n",
      "Testing loss: 217.367051777, test_err:14.7433718442, train_err:15.8305440662 \n",
      "\n",
      "3040/3040 [==============================] - 253s - loss: 422.2694    \n",
      "Epoch 93/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 313.8224\n",
      "Testing loss: 220.527426308, test_err:14.8501595, train_err:15.9765234964 \n",
      "\n",
      "3040/3040 [==============================] - 253s - loss: 318.3196    \n",
      "Epoch 94/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 286.8583\n",
      "Testing loss: 251.643788308, test_err:15.8632710344, train_err:16.9123647107 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 286.2469    \n",
      "Epoch 95/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 271.7205\n",
      "Testing loss: 403.512960334, test_err:20.0876458757, train_err:20.7642179554 \n",
      "\n",
      "3040/3040 [==============================] - 251s - loss: 276.4912    \n",
      "Epoch 96/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 347.1903\n",
      "Testing loss: 232.393061748, test_err:15.2444534091, train_err:16.1575648703 \n",
      "\n",
      "3040/3040 [==============================] - 252s - loss: 335.5215    \n",
      "Epoch 97/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 265.0939\n",
      "Testing loss: 225.293548745, test_err:15.0097883036, train_err:16.0178556979 \n",
      "\n",
      "3040/3040 [==============================] - 249s - loss: 262.1731    \n",
      "Epoch 98/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 289.5418\n",
      "Testing loss: 236.328016904, test_err:15.3729755909, train_err:16.3537341382 \n",
      "\n",
      "3040/3040 [==============================] - 249s - loss: 289.8910    \n",
      "Epoch 99/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 259.2746\n",
      "Testing loss: 222.374008982, test_err:14.9122161282, train_err:15.9732495215 \n",
      "\n",
      "3040/3040 [==============================] - 252s - loss: 259.9484    \n",
      "Epoch 100/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 270.0227\n",
      "Testing loss: 244.661149838, test_err:15.6416300307, train_err:16.5134755336 \n",
      "\n",
      "3040/3040 [==============================] - 253s - loss: 273.2320    \n",
      "Epoch 101/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 290.9821\n",
      "Testing loss: 338.515531199, test_err:18.3988203263, train_err:19.0988273938 \n",
      "\n",
      "3040/3040 [==============================] - 253s - loss: 290.0163    \n",
      "Epoch 102/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 292.0656\n",
      "Testing loss: 253.486802432, test_err:15.921284173, train_err:16.9805948231 \n",
      "\n",
      "3040/3040 [==============================] - 251s - loss: 291.8089    \n",
      "Epoch 103/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 290.7734\n",
      "Testing loss: 275.385250694, test_err:16.5947122379, train_err:17.442524435 \n",
      "\n",
      "3040/3040 [==============================] - 253s - loss: 291.1913    \n",
      "Epoch 104/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 257.1716\n",
      "Testing loss: 218.964539779, test_err:14.7974542166, train_err:15.8518689906 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 255.1073    \n",
      "Epoch 105/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 269.6096\n",
      "Testing loss: 221.120535439, test_err:14.8701252776, train_err:15.931773704 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 271.1098    \n",
      "Epoch 106/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 288.7408\n",
      "Testing loss: 239.489580496, test_err:15.4754587089, train_err:16.5215036473 \n",
      "\n",
      "3040/3040 [==============================] - 260s - loss: 290.9499    \n",
      "Epoch 107/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 269.9902\n",
      "Testing loss: 213.521096962, test_err:14.6123579343, train_err:15.6168027643 \n",
      "\n",
      "3040/3040 [==============================] - 252s - loss: 265.3942    \n",
      "Epoch 108/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 252.1140\n",
      "Testing loss: 247.182603856, test_err:15.7220282992, train_err:16.5784499423 \n",
      "\n",
      "3040/3040 [==============================] - 253s - loss: 258.1148    \n",
      "Epoch 109/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 262.3836\n",
      "Testing loss: 215.142655061, test_err:14.6677379421, train_err:15.6602445901 \n",
      "\n",
      "3040/3040 [==============================] - 258s - loss: 272.9669    \n",
      "Epoch 110/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 303.7194\n",
      "Testing loss: 209.891067666, test_err:14.4876156437, train_err:15.5645769142 \n",
      "\n",
      "3040/3040 [==============================] - 257s - loss: 302.2202    \n",
      "Epoch 111/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 282.3262\n",
      "Testing loss: 322.838637503, test_err:17.9676925184, train_err:18.7880113258 \n",
      "\n",
      "3040/3040 [==============================] - 257s - loss: 289.1066    \n",
      "Epoch 112/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 321.5097\n",
      "Testing loss: 293.241609433, test_err:17.1242767401, train_err:17.8667238 \n",
      "\n",
      "3040/3040 [==============================] - 261s - loss: 317.7220    \n",
      "Epoch 113/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 304.0337\n",
      "Testing loss: 476.732297556, test_err:21.83423961, train_err:22.3105329509 \n",
      "\n",
      "3040/3040 [==============================] - 263s - loss: 302.7627    \n",
      "Epoch 114/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 394.8575\n",
      "Testing loss: 281.977820627, test_err:16.7921697589, train_err:17.6573664279 \n",
      "\n",
      "3040/3040 [==============================] - 254s - loss: 389.8558    \n",
      "Epoch 115/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 268.8102\n",
      "Testing loss: 212.51650495, test_err:14.5779520439, train_err:15.7554121819 \n",
      "\n",
      "3040/3040 [==============================] - 260s - loss: 267.7577    \n",
      "Epoch 116/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 265.7096\n",
      "Testing loss: 218.666701789, test_err:14.787391364, train_err:15.8878661297 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 261.4660    \n",
      "Epoch 117/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 294.2459\n",
      "Testing loss: 237.287740446, test_err:15.4041576277, train_err:16.501411204 \n",
      "\n",
      "3040/3040 [==============================] - 268s - loss: 288.6063    \n",
      "Epoch 118/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 250.2844\n",
      "Testing loss: 263.791414763, test_err:16.2416364766, train_err:17.1353197245 \n",
      "\n",
      "3040/3040 [==============================] - 272s - loss: 250.6912    \n",
      "Epoch 119/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 265.4528\n",
      "Testing loss: 219.833258378, test_err:14.8267712241, train_err:15.7923277933 \n",
      "\n",
      "3040/3040 [==============================] - 261s - loss: 268.6276    \n",
      "Epoch 120/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 284.2914\n",
      "Testing loss: 212.025377535, test_err:14.5610871624, train_err:15.5318539923 \n",
      "\n",
      "3040/3040 [==============================] - 257s - loss: 281.8959    \n",
      "Epoch 121/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 294.0574\n",
      "Testing loss: 244.427080817, test_err:15.6341486692, train_err:16.472026089 \n",
      "\n",
      "3040/3040 [==============================] - 264s - loss: 288.2211    \n",
      "Epoch 122/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 255.2252\n",
      "Testing loss: 237.452019942, test_err:15.4094658048, train_err:16.4127572567 \n",
      "\n",
      "3040/3040 [==============================] - 280s - loss: 255.4564    \n",
      "Epoch 123/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 277.6253\n",
      "Testing loss: 269.941927619, test_err:16.429894762, train_err:17.3699588464 \n",
      "\n",
      "3040/3040 [==============================] - 281s - loss: 283.5741    \n",
      "Epoch 124/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 429.2300\n",
      "Testing loss: 468.51002133, test_err:21.6450566093, train_err:22.2816131832 \n",
      "\n",
      "3040/3040 [==============================] - 284s - loss: 414.2278    \n",
      "Epoch 125/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 343.8518\n",
      "Testing loss: 210.564464368, test_err:14.5108381792, train_err:15.5899268213 \n",
      "\n",
      "3040/3040 [==============================] - 282s - loss: 341.9379    \n",
      "Epoch 126/500\n",
      "2816/3040 [==========================>...] - ETA: 14s - loss: 312.9745\n",
      "Testing loss: 231.903264176, test_err:15.2283772679, train_err:16.2631695943 \n",
      "\n",
      "3040/3040 [==============================] - 285s - loss: 315.1726    \n",
      "Epoch 127/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 285.7507\n",
      "Testing loss: 213.407479377, test_err:14.6084764238, train_err:15.6623885367 \n",
      "\n",
      "3040/3040 [==============================] - 286s - loss: 279.0977    \n",
      "Epoch 128/500\n",
      "2816/3040 [==========================>...] - ETA: 14s - loss: 246.0263\n",
      "Testing loss: 206.950173468, test_err:14.3857596181, train_err:15.4298381901 \n",
      "\n",
      "3040/3040 [==============================] - 301s - loss: 241.0437    \n",
      "Epoch 129/500\n",
      "2816/3040 [==========================>...] - ETA: 15s - loss: 263.0966\n",
      "Testing loss: 223.764674217, test_err:14.9587726562, train_err:15.9927630297 \n",
      "\n",
      "3040/3040 [==============================] - 304s - loss: 266.5111    \n",
      "Epoch 130/500\n",
      "2816/3040 [==========================>...] - ETA: 14s - loss: 245.7156\n",
      "Testing loss: 206.636491956, test_err:14.374852604, train_err:15.3994956831 \n",
      "\n",
      "3040/3040 [==============================] - 273s - loss: 245.0951    \n",
      "Epoch 131/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 248.1358\n",
      "Testing loss: 212.687277061, test_err:14.5838045061, train_err:15.7020422933 \n",
      "\n",
      "3040/3040 [==============================] - 239s - loss: 252.7325    \n",
      "Epoch 132/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 241.2361\n",
      "Testing loss: 287.094496556, test_err:16.9438487996, train_err:17.6797589843 \n",
      "\n",
      "3040/3040 [==============================] - 236s - loss: 245.1701    \n",
      "Epoch 133/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 335.3828\n",
      "Testing loss: 255.8003789, test_err:15.9937455116, train_err:16.8638854693 \n",
      "\n",
      "3040/3040 [==============================] - 238s - loss: 329.4473    \n",
      "Epoch 134/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 314.4650\n",
      "Testing loss: 337.551839407, test_err:18.3725545451, train_err:19.0949834285 \n",
      "\n",
      "3040/3040 [==============================] - 236s - loss: 307.3603    \n",
      "Epoch 135/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 273.0430\n",
      "Testing loss: 296.746691895, test_err:17.2263095508, train_err:18.0535372237 \n",
      "\n",
      "3040/3040 [==============================] - 248s - loss: 272.2464    \n",
      "Epoch 136/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 264.6696\n",
      "Testing loss: 204.526851293, test_err:14.3012906076, train_err:15.384312595 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 265.4593    \n",
      "Epoch 137/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 256.7335\n",
      "Testing loss: 208.291532978, test_err:14.4323043261, train_err:15.4817648802 \n",
      "\n",
      "3040/3040 [==============================] - 252s - loss: 259.2648    \n",
      "Epoch 138/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 256.9840\n",
      "Testing loss: 208.338708175, test_err:14.4339391406, train_err:15.4525811575 \n",
      "\n",
      "3040/3040 [==============================] - 252s - loss: 251.6297    \n",
      "Epoch 139/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 249.2402\n",
      "Testing loss: 209.743227105, test_err:14.4825168129, train_err:15.5342989629 \n",
      "\n",
      "3040/3040 [==============================] - 251s - loss: 249.5937    \n",
      "Epoch 140/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 262.5494\n",
      "Testing loss: 221.43525744, test_err:14.8806951712, train_err:15.906195801 \n",
      "\n",
      "3040/3040 [==============================] - 251s - loss: 264.7369    \n",
      "Epoch 141/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 321.7947\n",
      "Testing loss: 413.706466193, test_err:20.3397444756, train_err:20.9594328997 \n",
      "\n",
      "3040/3040 [==============================] - 252s - loss: 318.0762    \n",
      "Epoch 142/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 421.6267\n",
      "Testing loss: 219.926820293, test_err:14.8299174745, train_err:15.8404312797 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 414.6646    \n",
      "Epoch 143/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 370.9266\n",
      "Testing loss: 259.263703517, test_err:16.1016839035, train_err:16.9988335182 \n",
      "\n",
      "3040/3040 [==============================] - 259s - loss: 365.1933    \n",
      "Epoch 144/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 356.5023\n",
      "Testing loss: 267.410622045, test_err:16.3527003137, train_err:17.297329958 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 349.2829    \n",
      "Epoch 145/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 297.8813\n",
      "Testing loss: 260.470234279, test_err:16.1390919095, train_err:16.8938589765 \n",
      "\n",
      "3040/3040 [==============================] - 257s - loss: 294.8264    \n",
      "Epoch 146/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 243.9988\n",
      "Testing loss: 273.040657606, test_err:16.5239433874, train_err:17.3858930096 \n",
      "\n",
      "3040/3040 [==============================] - 260s - loss: 248.7802    \n",
      "Epoch 147/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 256.1473\n",
      "Testing loss: 202.716175682, test_err:14.237840774, train_err:15.3146923174 \n",
      "\n",
      "3040/3040 [==============================] - 253s - loss: 259.6643    \n",
      "Epoch 148/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 257.3243\n",
      "Testing loss: 210.770747938, test_err:14.5179485532, train_err:15.5226089742 \n",
      "\n",
      "3040/3040 [==============================] - 260s - loss: 257.7230    \n",
      "Epoch 149/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 244.7497\n",
      "Testing loss: 208.214196135, test_err:14.429633659, train_err:15.4318223999 \n",
      "\n",
      "3040/3040 [==============================] - 253s - loss: 253.1336    \n",
      "Epoch 150/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 262.6402\n",
      "Testing loss: 365.407272178, test_err:19.1156406847, train_err:19.8386284411 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 269.3289    \n",
      "Epoch 151/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 278.6683\n",
      "Testing loss: 202.487485223, test_err:14.2298065362, train_err:15.281235256 \n",
      "\n",
      "3040/3040 [==============================] - 257s - loss: 275.2560    \n",
      "Epoch 152/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 241.5932\n",
      "Testing loss: 204.789588848, test_err:14.3104777651, train_err:15.29504512 \n",
      "\n",
      "3040/3040 [==============================] - 259s - loss: 242.1672    \n",
      "Epoch 153/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 297.6957\n",
      "Testing loss: 275.384698165, test_err:16.5947305915, train_err:17.4301076249 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 298.7800    \n",
      "Epoch 154/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 273.7998\n",
      "Testing loss: 219.303254298, test_err:14.8088983838, train_err:15.745261465 \n",
      "\n",
      "3040/3040 [==============================] - 254s - loss: 273.0629    \n",
      "Epoch 155/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 243.5421\n",
      "Testing loss: 228.918524009, test_err:15.1300547919, train_err:16.2443625655 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 244.7701    \n",
      "Epoch 156/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 305.0319\n",
      "Testing loss: 229.142795121, test_err:15.1374731911, train_err:16.1102477272 \n",
      "\n",
      "3040/3040 [==============================] - 257s - loss: 302.9647    \n",
      "Epoch 157/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 292.4086\n",
      "Testing loss: 202.4318989, test_err:14.2278526995, train_err:15.268385789 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 289.2606    \n",
      "Epoch 158/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 272.2239\n",
      "Testing loss: 225.386984092, test_err:15.0128975939, train_err:16.0390498928 \n",
      "\n",
      "3040/3040 [==============================] - 259s - loss: 283.5411    \n",
      "Epoch 159/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 314.3247\n",
      "Testing loss: 345.516612163, test_err:18.5880745121, train_err:19.2508740968 \n",
      "\n",
      "3040/3040 [==============================] - 257s - loss: 317.6773    \n",
      "Epoch 160/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 300.7546\n",
      "Testing loss: 217.219507318, test_err:14.7383720683, train_err:15.6329865181 \n",
      "\n",
      "3040/3040 [==============================] - 252s - loss: 295.1813    \n",
      "Epoch 161/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 240.3385\n",
      "Testing loss: 211.597709736, test_err:14.5463925064, train_err:15.6596516726 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 240.3879    \n",
      "Epoch 162/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 298.7157\n",
      "Testing loss: 271.120487093, test_err:16.4657328278, train_err:17.3537579977 \n",
      "\n",
      "3040/3040 [==============================] - 257s - loss: 304.6067    \n",
      "Epoch 163/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 271.6926\n",
      "Testing loss: 210.939027405, test_err:14.5237410999, train_err:15.4791119528 \n",
      "\n",
      "3040/3040 [==============================] - 257s - loss: 271.6705    \n",
      "Epoch 164/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 236.2665\n",
      "Testing loss: 215.12472221, test_err:14.6671247755, train_err:15.7121719301 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 239.5322    \n",
      "Epoch 165/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 261.3200\n",
      "Testing loss: 365.55505596, test_err:19.1194828076, train_err:19.7446863255 \n",
      "\n",
      "3040/3040 [==============================] - 259s - loss: 264.7053    \n",
      "Epoch 166/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 333.1153\n",
      "Testing loss: 270.209955637, test_err:16.4380530256, train_err:17.4787727371 \n",
      "\n",
      "3040/3040 [==============================] - 258s - loss: 342.8038    \n",
      "Epoch 167/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 267.8585\n",
      "Testing loss: 203.808272994, test_err:14.2761434868, train_err:15.3897877562 \n",
      "\n",
      "3040/3040 [==============================] - 260s - loss: 266.8661    \n",
      "Epoch 168/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 296.7332\n",
      "Testing loss: 366.702625797, test_err:19.1494615119, train_err:19.9637919079 \n",
      "\n",
      "3040/3040 [==============================] - 254s - loss: 310.1616    \n",
      "Epoch 169/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 301.6715\n",
      "Testing loss: 218.611100207, test_err:14.7854994521, train_err:15.7950359921 \n",
      "\n",
      "3040/3040 [==============================] - 254s - loss: 295.9363    \n",
      "Epoch 170/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 246.6541\n",
      "Testing loss: 224.607219495, test_err:14.9868954796, train_err:16.0034682563 \n",
      "\n",
      "3040/3040 [==============================] - 257s - loss: 244.3356    \n",
      "Epoch 171/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 243.8561\n",
      "Testing loss: 211.333782477, test_err:14.5373202834, train_err:15.6578567072 \n",
      "\n",
      "3040/3040 [==============================] - 257s - loss: 245.7398    \n",
      "Epoch 172/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 249.6468\n",
      "Testing loss: 232.709921024, test_err:15.2548247684, train_err:16.2224966696 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 253.8759    \n",
      "Epoch 173/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 255.0016\n",
      "Testing loss: 200.357901563, test_err:14.1547824577, train_err:15.2412972647 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 261.9625    \n",
      "Epoch 174/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 241.2905\n",
      "Testing loss: 201.408580579, test_err:14.1918492466, train_err:15.2847528134 \n",
      "\n",
      "3040/3040 [==============================] - 258s - loss: 241.9681    \n",
      "Epoch 175/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 284.9534\n",
      "Testing loss: 202.538507401, test_err:14.2315998264, train_err:15.2419096033 \n",
      "\n",
      "3040/3040 [==============================] - 257s - loss: 280.2840    \n",
      "Epoch 176/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 272.0269\n",
      "Testing loss: 227.642877117, test_err:15.0878308796, train_err:16.0411495725 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 268.6222    \n",
      "Epoch 177/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 254.5228\n",
      "Testing loss: 194.899437031, test_err:13.9606384428, train_err:15.0347391528 \n",
      "\n",
      "3040/3040 [==============================] - 259s - loss: 254.5582    \n",
      "Epoch 178/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 251.4311\n",
      "Testing loss: 215.4095308, test_err:14.6768308562, train_err:15.5994889014 \n",
      "\n",
      "3040/3040 [==============================] - 260s - loss: 249.0208    \n",
      "Epoch 179/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 293.3625\n",
      "Testing loss: 198.684112629, test_err:14.0955347645, train_err:15.1746439718 \n",
      "\n",
      "3040/3040 [==============================] - 260s - loss: 288.0222    \n",
      "Epoch 180/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 292.8005\n",
      "Testing loss: 284.545709068, test_err:16.8684775565, train_err:17.8013511138 \n",
      "\n",
      "3040/3040 [==============================] - 260s - loss: 288.4335    \n",
      "Epoch 181/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 252.4166\n",
      "Testing loss: 197.082303017, test_err:14.0386013888, train_err:15.0620367662 \n",
      "\n",
      "3040/3040 [==============================] - 259s - loss: 249.5976    \n",
      "Epoch 182/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 270.5668\n",
      "Testing loss: 247.14780418, test_err:15.7209192629, train_err:16.5921245022 \n",
      "\n",
      "3040/3040 [==============================] - 253s - loss: 270.6023    \n",
      "Epoch 183/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 267.7140\n",
      "Testing loss: 234.60550256, test_err:15.3168396873, train_err:16.2857255287 \n",
      "\n",
      "3040/3040 [==============================] - 254s - loss: 261.9883    \n",
      "Epoch 184/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 285.5447\n",
      "Testing loss: 193.412514456, test_err:13.9072815709, train_err:14.9476928499 \n",
      "\n",
      "3040/3040 [==============================] - 258s - loss: 278.6861    \n",
      "Epoch 185/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 283.9055\n",
      "Testing loss: 308.554603336, test_err:17.5657512222, train_err:18.3715029363 \n",
      "\n",
      "3040/3040 [==============================] - 258s - loss: 287.6149    \n",
      "Epoch 186/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 286.9610\n",
      "Testing loss: 271.027049336, test_err:16.4629095835, train_err:17.372428343 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 288.1219    \n",
      "Epoch 187/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 278.5211\n",
      "Testing loss: 217.635951153, test_err:14.7525022059, train_err:15.6920290628 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 274.0956    \n",
      "Epoch 188/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 230.4356\n",
      "Testing loss: 219.850863166, test_err:14.8273855418, train_err:15.7812203326 \n",
      "\n",
      "3040/3040 [==============================] - 259s - loss: 230.7750    \n",
      "Epoch 189/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 230.1685\n",
      "Testing loss: 327.497318629, test_err:18.0969137168, train_err:18.772747037 \n",
      "\n",
      "3040/3040 [==============================] - 261s - loss: 239.9274    \n",
      "Epoch 190/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 266.0651\n",
      "Testing loss: 196.458282471, test_err:14.016352351, train_err:15.0086048894 \n",
      "\n",
      "3040/3040 [==============================] - 260s - loss: 266.1452    \n",
      "Epoch 191/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 311.8842\n",
      "Testing loss: 193.391812134, test_err:13.9065389525, train_err:14.9239950784 \n",
      "\n",
      "3040/3040 [==============================] - 250s - loss: 322.4941    \n",
      "Epoch 192/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 447.6315\n",
      "Testing loss: 333.717659719, test_err:18.2679302259, train_err:19.0311808821 \n",
      "\n",
      "3040/3040 [==============================] - 253s - loss: 434.9687    \n",
      "Epoch 193/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 412.3734\n",
      "Testing loss: 703.106005859, test_err:26.5161188976, train_err:27.1042696985 \n",
      "\n",
      "3040/3040 [==============================] - 249s - loss: 438.4491    \n",
      "Epoch 194/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 573.5071\n",
      "Testing loss: 620.895941805, test_err:24.9178184348, train_err:25.6194554873 \n",
      "\n",
      "3040/3040 [==============================] - 252s - loss: 552.8679    \n",
      "Epoch 195/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 339.0887\n",
      "Testing loss: 200.549362424, test_err:14.1615504924, train_err:15.1632661081 \n",
      "\n",
      "3040/3040 [==============================] - 252s - loss: 335.0787    \n",
      "Epoch 196/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 277.8061\n",
      "Testing loss: 250.488641518, test_err:15.8268469833, train_err:16.6973601464 \n",
      "\n",
      "3040/3040 [==============================] - 250s - loss: 273.1184    \n",
      "Epoch 197/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 241.7238\n",
      "Testing loss: 192.383264722, test_err:13.8702333034, train_err:14.8905197201 \n",
      "\n",
      "3040/3040 [==============================] - 257s - loss: 241.0578    \n",
      "Epoch 198/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 244.4544\n",
      "Testing loss: 219.926929514, test_err:14.8299485824, train_err:15.8336653148 \n",
      "\n",
      "3040/3040 [==============================] - 253s - loss: 240.7257    \n",
      "Epoch 199/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 263.8796\n",
      "Testing loss: 221.100101029, test_err:14.8694524455, train_err:15.9011773799 \n",
      "\n",
      "3040/3040 [==============================] - 254s - loss: 262.6894    \n",
      "Epoch 200/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 256.6119\n",
      "Testing loss: 388.774356882, test_err:19.717373118, train_err:20.309784475 \n",
      "\n",
      "3040/3040 [==============================] - 254s - loss: 262.5208    \n",
      "Epoch 201/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 293.4082\n",
      "Testing loss: 221.21603249, test_err:14.8733387121, train_err:15.9527682843 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 287.7711    \n",
      "Epoch 202/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 290.7863\n",
      "Testing loss: 314.479239855, test_err:17.7335768221, train_err:18.6228766949 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 285.3697    \n",
      "Epoch 203/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 321.3570\n",
      "Testing loss: 198.515868016, test_err:14.0895714816, train_err:15.0676512252 \n",
      "\n",
      "3040/3040 [==============================] - 254s - loss: 316.1136    \n",
      "Epoch 204/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 376.5164\n",
      "Testing loss: 526.689383416, test_err:22.9497432595, train_err:23.6842053601 \n",
      "\n",
      "3040/3040 [==============================] - 253s - loss: 391.7368    \n",
      "Epoch 205/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 315.3167\n",
      "Testing loss: 215.518942984, test_err:14.6805674254, train_err:15.7163306719 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 308.6192    \n",
      "Epoch 206/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 234.0806\n",
      "Testing loss: 193.491153838, test_err:13.9101045889, train_err:14.9605587709 \n",
      "\n",
      "3040/3040 [==============================] - 257s - loss: 235.6853    \n",
      "Epoch 207/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 289.6546\n",
      "Testing loss: 201.35889234, test_err:14.1901036754, train_err:15.2286049143 \n",
      "\n",
      "3040/3040 [==============================] - 254s - loss: 290.7602    \n",
      "Epoch 208/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 299.1488\n",
      "Testing loss: 195.921739116, test_err:13.9972128537, train_err:15.0573715837 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 294.3134    \n",
      "Epoch 209/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 229.4221\n",
      "Testing loss: 270.426084498, test_err:16.4446516498, train_err:17.370364023 \n",
      "\n",
      "3040/3040 [==============================] - 260s - loss: 229.5357    \n",
      "Epoch 210/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 257.6292\n",
      "Testing loss: 207.136504484, test_err:14.3922483622, train_err:15.405096417 \n",
      "\n",
      "3040/3040 [==============================] - 258s - loss: 263.0957    \n",
      "Epoch 211/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 269.2327\n",
      "Testing loss: 206.974352706, test_err:14.3866137935, train_err:15.2953488999 \n",
      "\n",
      "3040/3040 [==============================] - 260s - loss: 266.8819    \n",
      "Epoch 212/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 244.8343\n",
      "Testing loss: 249.46762872, test_err:15.7945631538, train_err:16.7767143343 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 243.3530    \n",
      "Epoch 213/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 252.7117\n",
      "Testing loss: 214.284183703, test_err:14.6384576565, train_err:15.5758191553 \n",
      "\n",
      "3040/3040 [==============================] - 262s - loss: 252.5542    \n",
      "Epoch 214/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 239.7760\n",
      "Testing loss: 250.297367779, test_err:15.8208022819, train_err:16.6745574368 \n",
      "\n",
      "3040/3040 [==============================] - 296s - loss: 239.1272    \n",
      "Epoch 215/500\n",
      "2048/3040 [===================>..........] - ETA: 66s - loss: 247.5745"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:34: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:34: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(256, return_sequences=False, recurrent_initializer=<keras.ini..., kernel_initializer=<keras.ini..., input_shape=(None, 1))`\n",
      "ERROR: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 1118, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 300, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1044, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python2.7/inspect.py\", line 1004, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 454, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 490, in getmodule\n",
      "    for modname, module in sys.modules.items():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[0;32m   1828\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1829\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 1830\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   1831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1832\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1390\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1392\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1298\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1300\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1301\u001b[0m             )\n\u001b[0;32m   1302\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "# plain discriminative \n",
    "\n",
    "# network set-up\n",
    "in_out_neurons = 1\n",
    "hidden_neurons = 256\n",
    "win_size = 200\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "disc_xtrain = np.reshape( xtrain, [-1, win_size, in_out_neurons] )\n",
    "disc_xtest  = np.reshape( xtest,  [-1, win_size, in_out_neurons] )\n",
    "\n",
    "disc_ytrain = ytrain\n",
    "disc_ytest  = ytest\n",
    "\n",
    "print np.shape(disc_xtrain),np.shape(disc_ytrain),np.shape(disc_xtest),\\\n",
    "np.shape(disc_ytest)\n",
    "\n",
    "# optimizer\n",
    "sgd  = SGD(lr = 0.01, momentum = 0.9, nesterov = True)\n",
    "rms  = RMSprop(lr = 0.05,  rho = 0.9, epsilon  = 1e-08, decay = 0.0)\n",
    "\n",
    "\n",
    "adam = Adam(lr = 0.005, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add( LSTM(hidden_neurons, input_dim = in_out_neurons, return_sequences = False, \\\n",
    "                input_shape = [batch_size, win_size, in_out_neurons ], \\\n",
    "#               input_length = win_size, \\\n",
    "#               activation ='tanh',\\\n",
    "#               dropout = 0.1,\\\n",
    "#               kernel_regularizer = l2(0.2), \n",
    "#               recurrent_regularizer = l2(0.1),\\\n",
    "#               !change!\n",
    "                kernel_initializer    = glorot_normal(), \\\n",
    "                recurrent_initializer = glorot_normal() ))\n",
    "# change: activiation\n",
    "\n",
    "# model.add( LSTM(hidden_neurons, return_sequences = False, \\\n",
    "# #               input_shape = [batch_size, win_size, in_out_neurons ], \\\n",
    "# #               input_length = win_size, \\\n",
    "#               activation ='tanh',\\\n",
    "# #               dropout = 0.1,\\\n",
    "# #               kernel_regularizer = l2(0.2), \n",
    "# #               recurrent_regularizer = l2(0.1),\\\n",
    "# #               !change!\n",
    "#                 kernel_initializer    = glorot_normal(), \\\n",
    "#                 recurrent_initializer = glorot_normal() ))\n",
    "\n",
    "\n",
    "# identity ini\n",
    "# he's ini:   he_normal()\n",
    "# xavier ini: glorot_normal()\n",
    "# orthogonal: Orthogonal()\n",
    "\n",
    "\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(128,\\\n",
    "#                   kernel_regularizer = l2(0.01), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(64, \\\n",
    "#                   kernel_regularizer = l2(0.01), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(32, \\\n",
    "#                   kernel_regularizer = l2(0.01), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(1,  \\\n",
    "#                   kernel_regularizer = l2(0.01), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"linear\"))\n",
    "\n",
    "\n",
    "# training\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = adam)\n",
    "\n",
    "\n",
    "model.fit( disc_xtrain, disc_ytrain, shuffle = True, \\\n",
    "           callbacks = [ \\\n",
    "           TestCallback( (disc_xtest, disc_ytest), (disc_xtrain, disc_ytrain) ) ], \\\n",
    "           batch_size = batch_size, epochs = 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generative model\n",
    "\n",
    "# set-up\n",
    "in_out_neurons = 1\n",
    "hidden_neurons = 256\n",
    "win_size = 200    \n",
    "    \n",
    "# prepare data\n",
    "ytrain = expand_y( xtrain_df.as_matrix(), ytrain_df.as_matrix() )\n",
    "ytest  = expand_y( xtest_df.as_matrix(),  ytest_df.as_matrix()  )\n",
    "\n",
    "gen_xtrain = np.reshape( xtrain, [-1, win_size, 1] )\n",
    "gen_ytrain = np.reshape( ytrain, [-1, win_size, 1] )\n",
    "\n",
    "gen_xtest  = np.reshape( xtest, [-1, win_size, 1] )\n",
    "gen_ytest  = np.reshape( ytest, [-1, win_size, 1] )\n",
    "\n",
    "print np.shape(gen_xtrain), np.shape(gen_ytrain), np.shape(gen_xtest), np.shape(gen_ytest)\n",
    "\n",
    "\n",
    "# optimizer \n",
    "adam  = Adam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add( LSTM(hidden_neurons, input_dim = in_out_neurons, return_sequences = True, \\\n",
    "                input_length = win_size,\\\n",
    "                activation   = 'tanh',\\\n",
    "#                dropout = 0.1,\\\n",
    "#                kernel_regularizer = l2(0.15), \\\n",
    "#                recurrent_regularizer = l2(0.15), \\\n",
    "                kernel_initializer    = glorot_normal(), \\\n",
    "                recurrent_initializer = glorot_normal() ) )\n",
    "\n",
    "\n",
    "# identity ini\n",
    "# he's ini:   he_normal()\n",
    "# xavier ini: glorot_normal()\n",
    "# orthogonal: Orthogonal() \n",
    "\n",
    "\n",
    "# model.add(Dropout(0.1))\n",
    "model.add(TimeDistributed(Dense(128, activation = 'relu',\\\n",
    "#                                     kernel_regularizer = l2(0.1), \\\n",
    "                                    kernel_initializer = he_normal() ) ))\n",
    "\n",
    "model.add(TimeDistributed(Dense(64, activation = 'relu',\\\n",
    "#                                     kernel_regularizer = l2(0.01), \\\n",
    "                                    kernel_initializer = he_normal() ) ))\n",
    "\n",
    "model.add(TimeDistributed(Dense(1,  activation = 'linear',\\\n",
    "#                                     kernel_regularizer = l2(0.01), \\\n",
    "                                    kernel_initializer = he_normal() ) ))\n",
    "\n",
    "\n",
    "# training\n",
    "model.compile( loss = \"mean_squared_error\", optimizer = adam )\n",
    "\n",
    "\n",
    "model.fit( gen_xtrain, gen_ytrain, shuffle=True,  \\\n",
    "           callbacks = [ TestCallback_Generative( \\\n",
    "                         (gen_xtest, gen_ytest), (gen_xtrain, gen_ytrain) ) ], \\\n",
    "                         batch_size = 128, epochs = 500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# discriminative with non-overlapping local data\n",
    "\n",
    "#  network set-up\n",
    "local_size = 5\n",
    "in_out_neurons = local_size\n",
    "hidden_neurons = 512\n",
    "win_size = 200/5\n",
    "\n",
    "# # validation on each epoch \n",
    "# class TestCallback_insight(Callback):\n",
    "#     def __init__(self, test_data):\n",
    "#         self.test_data = test_data\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         x, y = self.test_data\n",
    "#         loss, mse = self.model.evaluate(x, y, verbose=0)\n",
    "        \n",
    "#         py = self.model.predict(x, verbose=0)\n",
    "#         cnt = len(y)\n",
    "#         err = [ (y[i][0]-py[i][0])**2 for i in range(cnt)]\n",
    "        \n",
    "#         print('\\nTesting loss: {}, acc: {}, mean: {} \\n'.format(\\\n",
    "#                loss, sqrt(mse), mean(err) ))\n",
    "\n",
    "def expand_x_local_non_overlapping( local_size, list_data):\n",
    "    \n",
    "    cnt = len(list_data)\n",
    "    steps = len(list_data[0])\n",
    "    tmp_dta = []\n",
    "    \n",
    "    for i in range(cnt):\n",
    "        tmp_dta.append([])\n",
    "        for j in range( local_size-1, steps, local_size ):\n",
    "            tmp_dta[-1].append( list_data[i][ j-local_size+1:j+1 ] )\n",
    "    \n",
    "    return tmp_dta\n",
    "\n",
    "\n",
    "disc_xtrain = np.array( expand_x_local_non_overlapping( local_size, list(xtrain) ) )\n",
    "disc_xtest  = np.array( expand_x_local_non_overlapping( local_size, list(xtest) ) )\n",
    "\n",
    "disc_ytrain = ytrain\n",
    "disc_ytest  = ytest\n",
    "\n",
    "print np.shape(disc_xtrain), np.shape(disc_ytrain), np.shape(disc_xtest), \\\n",
    "np.shape(disc_ytest)\n",
    "\n",
    "# optimizer \n",
    "adam = Adam(lr = 0.007, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_neurons, input_dim = in_out_neurons, return_sequences = False, \\\n",
    "               input_length = win_size, \\\n",
    "               activation = 'tanh', \\\n",
    "#                dropout = 0.1,\\\n",
    "#                kernel_regularizer = l2(0.1),      recurrent_regularizer = l2(0.1), \\\n",
    "               kernel_initializer    = glorot_normal(), \\\n",
    "               recurrent_initializer = glorot_normal() ))\n",
    "# identity ini\n",
    "# he's ini:   he_normal()\n",
    "# xavier ini: glorot_normal()\n",
    "# orthogonal: Orthogonal() \n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# kernel_regularizer = l2(0.1), \\\n",
    "model.add(Dense(128,\\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(64, \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(32, \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(1,\\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"linear\"))\n",
    "\n",
    "\n",
    "# training \n",
    "model.compile(loss=\"mean_squared_error\", optimizer = adam)\n",
    "\n",
    "\n",
    "model.fit( disc_xtrain, disc_ytrain, shuffle=True, \\\n",
    "           callbacks = [ TestCallback( (disc_xtest, disc_ytest), (disc_xtrain, disc_ytrain) ) ], \\\n",
    "           batch_size = 256, epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.merge import *\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "in_out_neurons = 1\n",
    "hidden_neurons = 256\n",
    "win_size = 200\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "input_raw  = Input(shape = ( win_size, 1 ),    dtype='float32', name='input_raw')\n",
    "input_diff = Input(shape = ( win_size -1, 1 ), dtype='float32', name='input_diff')\n",
    "input_last = Input(shape = ( 1, ), dtype='float32', name='input_last')\n",
    "\n",
    "hidden_raw  = LSTM(32)( input_raw )\n",
    "output_raw  = Dense(1, activation='relu', name='aux_output')(hidden_raw)\n",
    "\n",
    "hidden_diff = LSTM(32)( input_diff )\n",
    "output_diff = Dense(1, activation='relu', name='aux_output')(hidden_diff)\n",
    "output_diff = Add( [output_diff, input_last] )\n",
    "\n",
    "attention_diff = Add( [output_diff, -1*output_raw] )\n",
    "attention_prob = Dense(1, activation='sigmoid')( attention_diff )\n",
    "attention_diff = merge([output_diff, attention_prob], name='attention_diff', mode='mul')\n",
    "\n",
    "\n",
    "tmp_batch_sizee = int(input_raw.shape[0])\n",
    "ones = K.ones((tmp_batch_sizee,1))\n",
    "\n",
    "attention_prob = Add( [one, -1*attention_prob] )\n",
    "attention_raw  = merge([output_raw, attention_prob], name='attention_raw', mode='mul')\n",
    "\n",
    "# x = keras.layers.concatenate([lstm_out, auxiliary_input])\n",
    "# x = Dense(64, activation='relu')(x)\n",
    "\n",
    "output_main = Add( [attention_diff, attention_raw] )\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "# main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=[ input_raw, input_diff, input_last], outputs=[ output_main ])\n",
    "\n",
    "model.compile(optimizer = adam, loss='mean_squared_error',\n",
    "#               loss_weights=[1., 0.2]\\\n",
    "             )\n",
    "\n",
    "model.fit([headline_data, additional_data], [labels], epochs = 500, batch_size = batch_size )\n",
    "\n",
    "\n",
    "\n",
    "# def attention_3d_block(inputs):\n",
    "#     # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "#     input_dim = int(inputs.shape[2])\n",
    "#     a = Permute((2, 1))(inputs)\n",
    "#     a = Reshape((input_dim, TIME_STEPS))(a)\n",
    "#     a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "#     if SINGLE_ATTENTION_VECTOR:\n",
    "#         a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "#         a = RepeatVector(input_dim)(a)\n",
    "#     a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "#     output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "#     return output_attention_mul\n",
    "\n",
    "\n",
    "# def model_attention_applied_after_lstm():\n",
    "#     inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "#     lstm_units = 32\n",
    "#     lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "#     attention_mul = attention_3d_block(lstm_out)\n",
    "#     attention_mul = Flatten()(attention_mul)\n",
    "#     output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "#     model = Model(input=[inputs], output=output)\n",
    "#     return model\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
