{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# baseline\n",
    "\n",
    "#  GP\n",
    "#  S-ARIMA\n",
    "#  SVR\n",
    "\n",
    "\n",
    "#  RF\n",
    "#  GBT\n",
    "#  xgboosted \n",
    "\n",
    "\n",
    "# bayeisan regression\n",
    "\n",
    "# LSTM:\n",
    "#   1. initialization\n",
    "#   2. batch normalization\n",
    "#   3. weight normalization\n",
    "#   4. variable length\n",
    "#   5. attention mechanism \n",
    "\n",
    "# LSTM:  discrimitive, generative\n",
    "# perodic in data\n",
    "# attention \n",
    "\n",
    "# http://bugra.github.io/work/notes/2014-04-26/outlier-detection-markov-chain-monte-carlo-via-pymc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "#  lstm regularization\n",
    "#  skewness of dependent variable \n",
    "\n",
    "# http://smerity.com/articles/2016/orthogonal_init.html\n",
    "\n",
    "\n",
    "\n",
    "# RMSE, MAE, MAPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# data processing packages\n",
    "import numpy as np   \n",
    "import pandas as pd \n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    " \n",
    "%matplotlib inline    \n",
    "import matplotlib as mplt\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.style.use('ggplot')\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "\n",
    "# machine leanring packages\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "\n",
    "from utils_keras import *\n",
    "from utils_dataPrepro import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features:\n",
    "    \n",
    "# Feature based approach: Here the time series are mapped to another, \n",
    "#     possibly lower dimensional, representation. \n",
    "#     This means that the feature extraction algorithm calculates characteristics\n",
    "#     such as the average or maximal value of the time series. The features are then \n",
    "#     passed as a feature matrix to a \"normal\" machine learning such as a neural network, \n",
    "#     random forest or support vector machine. This approach has the advantage of a better\n",
    "#     explainability of the results. Further it enables us to use a well developed theory of\n",
    "# supervised machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3800, 398) (3800, 1) (1300, 398) (1300, 1)\n"
     ]
    }
   ],
   "source": [
    "# stock data\n",
    "\n",
    "files_list=[\"../dataset/dataset_ts/stock_xtrain.dat\", \\\n",
    "            \"../dataset/dataset_ts/stock_xtest.dat\",\\\n",
    "            \"../dataset/dataset_ts/stock_ytrain.dat\", \\\n",
    "            \"../dataset/dataset_ts/stock_ytest.dat\"]\n",
    "\n",
    "xtrain_df = pd.DataFrame( flatten_features(np.load(files_list[0])) )\n",
    "xtest_df  = pd.DataFrame( flatten_features(np.load(files_list[1])) )\n",
    "ytrain_df = pd.DataFrame( flatten_features(np.load(files_list[2])) )\n",
    "ytest_df  = pd.DataFrame( flatten_features(np.load(files_list[3])) )\n",
    "\n",
    "xtrain, ytrain, xtest, ytest = prepare_train_test_data(200, True, \\\n",
    "                                                       xtrain_df, xtest_df, ytrain_df, ytest_df)\n",
    "    \n",
    "    \n",
    "xtrain = flatten_features( xtrain )\n",
    "ytrain = flatten_features( ytrain )\n",
    "xtest  = flatten_features( xtest )\n",
    "ytest  = flatten_features( ytest )\n",
    "\n",
    "print np.shape(xtrain), np.shape(ytrain), np.shape(xtest), np.shape(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8300, 398) (8300, 1) (1297, 398) (1297, 1)\n"
     ]
    }
   ],
   "source": [
    "# power data\n",
    "\n",
    "files_list=[\"../dataset/dataset_ts/power_xtrain.dat\", \\\n",
    "            \"../dataset/dataset_ts/power_xtest.dat\",\\\n",
    "            \"../dataset/dataset_ts/power_ytrain.dat\", \\\n",
    "            \"../dataset/dataset_ts/power_ytest.dat\"]\n",
    "\n",
    "xtrain_df = pd.DataFrame( flatten_features(np.load(files_list[0])) )\n",
    "xtest_df  = pd.DataFrame( flatten_features(np.load(files_list[1])) )\n",
    "ytrain_df = pd.DataFrame( flatten_features(np.load(files_list[2])) )\n",
    "ytest_df  = pd.DataFrame( flatten_features(np.load(files_list[3])) )\n",
    "\n",
    "xtrain, ytrain, xtest, ytest = prepare_train_test_data(200, True, \\\n",
    "                                                       xtrain_df, xtest_df, ytrain_df, ytest_df)\n",
    "\n",
    "xtrain = flatten_features( xtrain )\n",
    "ytrain = flatten_features( ytrain )\n",
    "xtest  = flatten_features( xtest )\n",
    "ytest  = flatten_features( ytest )\n",
    "    \n",
    "print np.shape(xtrain), np.shape(ytrain), np.shape(xtest), np.shape(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GBT\n",
    "\n",
    "# https://www.analyticsvidhya.com/blog/2016/02/\n",
    "# complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "#----Boosting parameters:\n",
    "#   learnning rate: 0.05 - 0.2\n",
    "#   n_estimators: 40-70\n",
    "\n",
    "#----Tree parameters:\n",
    "#   max_depth: 3-10\n",
    "#   max_leaf_nodes\n",
    "#   num_samples_split: 0.5-1% of total number \n",
    "#   min_samples_leaf\n",
    "#   max_features\n",
    "\n",
    "#   subsample: 0.8\n",
    "#   min_weight_fraction_leaf\n",
    "\n",
    "#----Order of tuning: max_depth and num_samples_split, min_samples_leaf, max_features\n",
    "\n",
    "def gbt_n_estimatior(maxnum, X, Y, xtest, ytest, fix_lr ):\n",
    "    \n",
    "    tmpy = Y.reshape( (len(Y),) )\n",
    "    score = []\n",
    "    \n",
    "    cnt = len(xtest)\n",
    "    \n",
    "    for i in range(10,maxnum+1,10):\n",
    "        clf = GradientBoostingRegressor(n_estimators = i,learning_rate = fix_lr)\n",
    "        \n",
    "        clf.fit( X, tmpy )\n",
    "        \n",
    "        pytest = clf.predict(xtest)\n",
    "\n",
    "        score.append(\\\n",
    "        (i, sqrt(mean([( pytest[i]-ytest[i] )**2 for i in range(cnt) ]))) )\n",
    "    \n",
    "    return min(score, key = lambda x: x[1]), score\n",
    "\n",
    "\n",
    "def gbt_tree_para( X, Y, xtest, ytest, depth_range, fix_lr, fix_n_est):\n",
    "    \n",
    "    tmpy = Y.reshape( (len(Y),) )\n",
    "    score = []\n",
    "    \n",
    "    cnt = len(xtest)\n",
    "    \n",
    "    for i in depth_range:\n",
    "        \n",
    "        clf = GradientBoostingRegressor(n_estimators = fix_n_est,learning_rate = fix_lr,\\\n",
    "                                        max_depth = i )\n",
    "        \n",
    "        clf.fit( X, tmpy )\n",
    "        \n",
    "        pytest = clf.predict(xtest)\n",
    "\n",
    "        score.append(\\\n",
    "        (i, sqrt(mean([( pytest[i]-ytest[i] )**2 for i in range(cnt) ]))) )\n",
    "    \n",
    "    return min(score, key = lambda x: x[1]), score\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimator, RMSE: (40, 1.0143259393877819)\n",
      "depth, RMSE: (3, 1.014234534209864)\n"
     ]
    }
   ],
   "source": [
    "# GBT performance\n",
    "\n",
    "fix_lr = 0.25\n",
    "\n",
    "n_err, n_err_list = gbt_n_estimatior(301, xtrain, ytrain, xtest, ytest, fix_lr)\n",
    "\n",
    "print \"n_estimator, RMSE:\", n_err\n",
    "\n",
    "depth_err, depth_err_list = gbt_tree_para( xtrain, ytrain, xtest, ytest, range(3,16), fix_lr, n_err[0] )\n",
    "\n",
    "print \"depth, RMSE:\", depth_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # GBT test\n",
    "# clf = GradientBoostingRegressor(n_estimators=20, learning_rate=0.25, max_depth = 3)\n",
    "        \n",
    "# clf.fit( xtrain, ytrain )\n",
    "        \n",
    "# pytest = clf.predict(xtest)\n",
    "\n",
    "# err = []\n",
    "# for i in range(len(pytest)):\n",
    "#     err.append( ytest[i] - pytest[i] )\n",
    "    \n",
    "\n",
    "# print mean(err), var(err), sqrt(mean([i**2 for i in err]))\n",
    "\n",
    "# print zip(ytrain, pytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoosted\n",
    "\n",
    "# https://www.analyticsvidhya.com/blog/2016/03/\n",
    "#     complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "\n",
    "#----General Parameters\n",
    "\n",
    "#   eta(learning rate): 0.05 - 0.3\n",
    "#   number of rounds: \n",
    "\n",
    "#----Booster Parameters\n",
    "\n",
    "#   max_depth 3-10\n",
    "#   max_leaf_nodes\n",
    "#   gamma: mininum loss reduction\n",
    "#   min_child_weight: 1 by default\n",
    "\n",
    "#   max_delta_step: not needed in general, for unbalance in logistic regression\n",
    "#   subsample: 0.5-1\n",
    "#   colsample_bytree: 0.5-1\n",
    "#   colsample_bylevel: \n",
    "#   lambda: l2 regularization \n",
    "#   alpha: l1 regularization\n",
    "#   scale_pos_weight: >>1, for high class imbalance\n",
    "\n",
    "# Learning Task Parameters\n",
    "\n",
    "\n",
    "def xgt_n_depth( lr, max_depth, max_round, X, Y, xtest, ytest ):\n",
    "    \n",
    "    score = []\n",
    "    xg_train = xgb.DMatrix(xtrain, label = ytrain)\n",
    "    xg_test  = xgb.DMatrix(xtest,  label = ytest)\n",
    "\n",
    "# setup parameters for xgboost\n",
    "    param = {}\n",
    "# use softmax multi-class classification\n",
    "    param['objective'] = \"reg:linear\" \n",
    "#   'multi:softmax'\n",
    "    \n",
    "# scale weight of positive examples\n",
    "    param['eta'] = lr\n",
    "    param['max_depth'] = 0\n",
    "    param['silent'] = 1\n",
    "    param['nthread'] = 8\n",
    "#     param['num_class'] = 8\n",
    "#     param['gamma']\n",
    "    \n",
    "    for depth_trial in range(2, max_depth):\n",
    "        for num_round_trial in range(2, max_round):\n",
    "            \n",
    "            param['max_depth'] = depth_trial\n",
    "            bst  = xgb.train( param, xg_train, num_round_trial )\n",
    "            pred = bst.predict( xg_test )\n",
    "            \n",
    "            tmp_accur = sqrt(mean( [(pred[i] - ytest[i])**2 for i in range(len(ytest))] )) \n",
    "            \n",
    "            score.append( (depth_trial, num_round_trial, tmp_accur) )\n",
    "            \n",
    "    return min(score, key = lambda x: x[2]), score\n",
    "\n",
    "\n",
    "def xgt_l2( fix_lr, fix_depth, fix_round, X, Y, xtest, ytest, l2_range ):\n",
    "    \n",
    "    score = []\n",
    "    xg_train = xgb.DMatrix(xtrain, label = ytrain)\n",
    "    xg_test  = xgb.DMatrix(xtest,  label = ytest)\n",
    "\n",
    "# setup parameters for xgboost\n",
    "    param = {}\n",
    "# use softmax multi-class classification\n",
    "    param['objective'] = 'reg:linear' \n",
    "#   'multi:softmax'\n",
    "    \n",
    "# scale weight of positive examples\n",
    "    param['eta'] = fix_lr\n",
    "    param['max_depth'] = fix_depth\n",
    "    param['silent'] = 1\n",
    "    param['nthread'] = 8\n",
    "#     param['num_class'] = 8\n",
    "    \n",
    "    param['lambda'] = 0.0\n",
    "#     param['alpha']\n",
    "    \n",
    "    \n",
    "    for l2_trial in l2_range:\n",
    "        \n",
    "        param['lambda'] = l2_trial\n",
    "        \n",
    "        bst = xgb.train(param, xg_train, fix_round )\n",
    "        pred = bst.predict( xg_test )\n",
    "            \n",
    "        tmp_accur = sqrt(mean( [(pred[i] - ytest[i])**2 for i in range(len(ytest))] )) \n",
    "            \n",
    "        score.append( (l2_trial, tmp_accur) )\n",
    "            \n",
    "    return min(score, key = lambda x: x[1]), score\n",
    "\n",
    "    \n",
    "#  def xgt_l1 for very high dimensional features    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " depth, number of rounds, RMSE: (3, 44, 1.0455573756115384)\n",
      " l2, RMSE: (0.01, 1.0192915193168073)\n"
     ]
    }
   ],
   "source": [
    "# XGBoosted performance\n",
    "fix_lr = 0.2\n",
    "\n",
    "n_depth_err, n_depth_err_list = xgt_n_depth( fix_lr, 16, 51, xtrain, ytrain, xtest, ytest)\n",
    "\n",
    "print \" depth, number of rounds, RMSE:\", n_depth_err\n",
    "\n",
    "l2_err, l2_err_list = xgt_l2( fix_lr, n_depth_err[0], n_depth_err[1], xtrain, ytrain, xtest, ytest,\\\n",
    "                    [0.0001, 0.001, 0.01, 0.1, 1, 10, 100])\n",
    "\n",
    "print \" l2, RMSE:\", l2_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random forest\n",
    "\n",
    "# max_features:\n",
    "# n_estimators\n",
    "# max_depth\n",
    "\n",
    "def rf_n_depth_estimatior(maxnum, maxdep, X, Y, xtest, ytest ):\n",
    "    \n",
    "    tmpy = Y.reshape( (len(Y),) )\n",
    "    score = []\n",
    "    \n",
    "    cnt = len(xtest)\n",
    "        \n",
    "    for n_trial in range(10,maxnum+1,10):\n",
    "        for dep_trial in range(2, maxdep+1):\n",
    "            \n",
    "            clf = RandomForestRegressor(n_estimators = n_trial, max_depth = dep_trial, max_features = \"sqrt\")\n",
    "            clf.fit( X, tmpy )\n",
    "        \n",
    "            pytest = clf.predict(xtest)\n",
    "            \n",
    "            score.append(\\\n",
    "            (n_trial, dep_trial, sqrt(mean([( pytest[i]-ytest[i] )**2 for i in range(cnt) ]))) )\n",
    "    \n",
    "    return min(score, key = lambda x: x[2]), score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimator, RMSE: (90, 13, 1.2387303782002563)\n"
     ]
    }
   ],
   "source": [
    "# Random forest performance\n",
    "\n",
    "n_err, n_err_list = rf_n_depth_estimatior( 100, 25, xtrain, ytrain, xtest, ytest )\n",
    "\n",
    "print \"n_estimator, RMSE:\", n_err\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
