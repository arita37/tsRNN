{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# baseline\n",
    "\n",
    "#  GP\n",
    "#  S-ARIMA\n",
    "#  SVR\n",
    "\n",
    "\n",
    "#  RF\n",
    "#  GBT\n",
    "#  xgboosted \n",
    "\n",
    "\n",
    "# bayeisan regression\n",
    "\n",
    "# LSTM:\n",
    "#   1. initialization\n",
    "#   2. batch normalization\n",
    "#   3. weight normalization\n",
    "#   4. variable length\n",
    "#   5. attention mechanism \n",
    "\n",
    "# LSTM:  discrimitive, generative\n",
    "# perodic in data\n",
    "# attention \n",
    "\n",
    "# http://bugra.github.io/work/notes/2014-04-26/outlier-detection-markov-chain-monte-carlo-via-pymc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "#  lstm regularization\n",
    "#  skewness of dependent variable \n",
    "\n",
    "# http://smerity.com/articles/2016/orthogonal_init.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data processing packages\n",
    "import numpy as np   \n",
    "import pandas as pd \n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    " \n",
    "%matplotlib inline    \n",
    "import matplotlib as mplt\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.style.use('ggplot')\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "\n",
    "# machine leanring packages\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features:\n",
    "    \n",
    "# Feature based approach: Here the time series are mapped to another, \n",
    "#     possibly lower dimensional, representation. \n",
    "#     This means that the feature extraction algorithm calculates characteristics\n",
    "#     such as the average or maximal value of the time series. The features are then \n",
    "#     passed as a feature matrix to a \"normal\" machine learning such as a neural network, \n",
    "#     random forest or support vector machine. This approach has the advantage of a better\n",
    "#     explainability of the results. Further it enables us to use a well developed theory of\n",
    "# supervised machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7908, 100) (7908, 1) (879, 100) (879, 1)\n"
     ]
    }
   ],
   "source": [
    "def conti_normalization_train_dta(dta_df):\n",
    "    \n",
    "    return preprocessing.scale(dta_df)\n",
    "\n",
    "def conti_normalization_test_dta(dta_df, train_df):\n",
    "    \n",
    "    mean_dim = np.mean(train_df, axis=0)\n",
    "    std_dim = np.std(train_df, axis=0)\n",
    "        \n",
    "    df=pd.DataFrame()\n",
    "    cols = train_df.columns\n",
    "    idx=0\n",
    "    \n",
    "    for i in cols:\n",
    "        df[i] = (dta_df[i]- mean_dim[idx])*1.0/std_dim[idx]\n",
    "        idx=idx+1\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------data prepro-----------------------\n",
    "\n",
    "files_list=[\"../dataset/dataset_ts/stock_xtrain.csv\", \\\n",
    "            \"../dataset/dataset_ts/stock_xtest.csv\",\\\n",
    "            \"../dataset/dataset_ts/stock_ytrain.csv\", \\\n",
    "            \"../dataset/dataset_ts/stock_ytest.csv\"]\n",
    "\n",
    "xtrain_df =pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "xtest_df  =pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "ytrain_df =pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "ytest_df  =pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "\n",
    "\n",
    "# normalize x in training and testing datasets\n",
    "xtest = conti_normalization_test_dta(xtest_df, xtrain_df)\n",
    "xtrain= conti_normalization_train_dta(xtrain_df)\n",
    "\n",
    "# xtrain = xtrain.as_matrix()\n",
    "xtest  = xtest.as_matrix()\n",
    "\n",
    "\n",
    "ytrain = ytrain_df.as_matrix()\n",
    "ytest = ytest_df.as_matrix()\n",
    "\n",
    "print np.shape(xtrain), np.shape(ytrain), np.shape(xtest), np.shape(ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GBT\n",
    "\n",
    "# https://www.analyticsvidhya.com/blog/2016/02/\n",
    "# complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "#----Boosting parameters:\n",
    "#   learnning rate: 0.05 - 0.2\n",
    "#   n_estimators: 40-70\n",
    "\n",
    "#----Tree parameters:\n",
    "#   max_depth: 3-10\n",
    "#   max_leaf_nodes\n",
    "#   num_samples_split: 0.5-1% of total number \n",
    "#   min_samples_leaf\n",
    "#   max_features\n",
    "\n",
    "#   subsample: 0.8\n",
    "#   min_weight_fraction_leaf\n",
    "\n",
    "#----Order of tuning: max_depth and num_samples_split, min_samples_leaf, max_features\n",
    "\n",
    "def gbt_n_estimatior(maxnum, X, Y, xtest, ytest):\n",
    "    \n",
    "    tmpy = Y.reshape( (len(Y),) )\n",
    "    score = []\n",
    "    \n",
    "    cnt = len(xtest)\n",
    "    \n",
    "    for i in range(10,maxnum+1,10):\n",
    "        clf = GradientBoostingRegressor(n_estimators=i,learning_rate = 0.25)\n",
    "        \n",
    "        clf.fit( X, tmpy )\n",
    "        \n",
    "        pytest = clf.predict(xtest)\n",
    "\n",
    "        score.append(\\\n",
    "        (i, sqrt(mean([( pytest[i]-ytest[i] )**2 for i in range(cnt) ]))) )\n",
    "    \n",
    "    return min(score, key = lambda x: x[1])\n",
    "\n",
    "\n",
    "def gbt_tree_para( X, Y, xtest, ytest, depth_range, fix_lr, fix_n_est):\n",
    "    \n",
    "    tmpy = Y.reshape( (len(Y),) )\n",
    "    score = []\n",
    "    \n",
    "    cnt = len(xtest)\n",
    "    \n",
    "    for i in depth_range:\n",
    "        \n",
    "        clf = GradientBoostingRegressor(n_estimators = fix_n_est,learning_rate = fix_lr,\\\n",
    "                                        max_depth = i )\n",
    "        \n",
    "        clf.fit( X, tmpy )\n",
    "        \n",
    "        pytest = clf.predict(xtest)\n",
    "\n",
    "        score.append(\\\n",
    "        (i, sqrt(mean([( pytest[i]-ytest[i] )**2 for i in range(cnt) ]))) )\n",
    "    \n",
    "    return min(score, key = lambda x: x[1])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimator, RMSE: (240, 4.7729188313641622)\n",
      "depth, RMSE: (7, 3.3156377378346091)\n"
     ]
    }
   ],
   "source": [
    "# GBT performance\n",
    "\n",
    "n_err = gbt_n_estimatior(300, xtrain, ytrain, xtest, ytest)\n",
    "\n",
    "print \"n_estimator, RMSE:\", n_err\n",
    "\n",
    "depth_err = gbt_tree_para( xtrain, ytrain, xtest, ytest, range(3,10), 0.2, n_err[0] )\n",
    "\n",
    "print \"depth, RMSE:\", depth_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.107026742201 20.2878758177 4.50547783717\n"
     ]
    }
   ],
   "source": [
    "# GBT test\n",
    "clf = GradientBoostingRegressor(n_estimators=750,learning_rate=0.1)\n",
    "        \n",
    "clf.fit( xtrain, ytrain )\n",
    "        \n",
    "pytest = clf.predict(xtest)\n",
    "\n",
    "err = []\n",
    "for i in range(len(pytest)):\n",
    "    err.append( ytest[i] - pytest[i] )\n",
    "    \n",
    "\n",
    "print mean(err), var(err), sqrt(mean([i**2 for i in err]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoosted\n",
    "\n",
    "# https://www.analyticsvidhya.com/blog/2016/03/\n",
    "#     complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "\n",
    "#----General Parameters\n",
    "\n",
    "#   eta(learning rate): 0.05 - 0.3\n",
    "#   number of rounds: \n",
    "\n",
    "#----Booster Parameters\n",
    "\n",
    "#   max_depth 3-10\n",
    "#   max_leaf_nodes\n",
    "#   gamma: mininum loss reduction\n",
    "#   min_child_weight: 1 by default\n",
    "\n",
    "#   max_delta_step: not needed in general, for unbalance in logistic regression\n",
    "#   subsample: 0.5-1\n",
    "#   colsample_bytree: 0.5-1\n",
    "#   colsample_bylevel: \n",
    "#   lambda: l2 regularization \n",
    "#   alpha: l1 regularization\n",
    "#   scale_pos_weight: >>1, for high class imbalance\n",
    "\n",
    "# Learning Task Parameters\n",
    "\n",
    "\n",
    "def xgt_n_depth( max_depth, max_round, X, Y, xtest, ytest ):\n",
    "    \n",
    "    score = []\n",
    "    xg_train = xgb.DMatrix(xtrain, label = ytrain)\n",
    "    xg_test  = xgb.DMatrix(xtest,  label = ytest)\n",
    "\n",
    "# setup parameters for xgboost\n",
    "    param = {}\n",
    "# use softmax multi-class classification\n",
    "    param['objective'] = \"reg:linear\" \n",
    "#   'multi:softmax'\n",
    "    \n",
    "# scale weight of positive examples\n",
    "    param['eta'] = 0.15\n",
    "    param['max_depth'] = 0\n",
    "    param['silent'] = 1\n",
    "    param['nthread'] = 8\n",
    "#     param['num_class'] = 8\n",
    "#     param['gamma']\n",
    "    \n",
    "    for depth_trial in range(2, max_depth):\n",
    "        for num_round_trial in range(2, max_round):\n",
    "            \n",
    "            param['max_depth'] = depth_trial\n",
    "            bst  = xgb.train( param, xg_train, num_round_trial )\n",
    "            pred = bst.predict( xg_test )\n",
    "            \n",
    "            tmp_accur = sqrt(mean( [(pred[i] - ytest[i])**2 for i in range(len(ytest))] )) \n",
    "            \n",
    "            score.append( (depth_trial, num_round_trial, tmp_accur) )\n",
    "            \n",
    "    return min(score, key = lambda x: x[2])\n",
    "\n",
    "\n",
    "def xgt_l2( fix_lr, fix_depth, fix_round, X, Y, xtest, ytest, l2_range ):\n",
    "    \n",
    "    score = []\n",
    "    xg_train = xgb.DMatrix(xtrain, label = ytrain)\n",
    "    xg_test  = xgb.DMatrix(xtest,  label = ytest)\n",
    "\n",
    "# setup parameters for xgboost\n",
    "    param = {}\n",
    "# use softmax multi-class classification\n",
    "    param['objective'] = 'reg:linear' \n",
    "#   'multi:softmax'\n",
    "    \n",
    "# scale weight of positive examples\n",
    "    param['eta'] = fix_lr\n",
    "    param['max_depth'] = fix_depth\n",
    "    param['silent'] = 1\n",
    "    param['nthread'] = 8\n",
    "#     param['num_class'] = 8\n",
    "    \n",
    "    param['lambda'] = 0.0\n",
    "#     param['alpha']\n",
    "    \n",
    "    \n",
    "    for l2_trial in l2_range:\n",
    "        \n",
    "        param['lambda'] = l2_trial\n",
    "        \n",
    "        bst = xgb.train(param, xg_train, fix_round )\n",
    "        pred = bst.predict( xg_test )\n",
    "            \n",
    "        tmp_accur = sqrt(mean( [(pred[i] - ytest[i])**2 for i in range(len(ytest))] )) \n",
    "            \n",
    "        score.append( (l2_trial, tmp_accur) )\n",
    "            \n",
    "    return min(score, key = lambda x: x[1])\n",
    "\n",
    "    \n",
    "#  def xgt_l1 for very high dimensional features    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " depth, number of rounds, RMSE: (5, 46, 3.2475943642139655)\n",
      " l2, RMSE: (0.01, 3.104052830436308)\n"
     ]
    }
   ],
   "source": [
    "# XGBoosted performance\n",
    "\n",
    "n_depth_err = xgt_n_depth(11, 50, xtrain, ytrain, xtest, ytest)\n",
    "\n",
    "print \" depth, number of rounds, RMSE:\", n_depth_err\n",
    "\n",
    "l2_err = xgt_l2(0.15, n_depth_err[0], n_depth_err[1], xtrain, ytrain, xtest, ytest,\\\n",
    "                    [0.001, 0.01, 0.1, 1, 10, 100])\n",
    "\n",
    "print \" l2, RMSE:\", l2_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoosted test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discriminative\n",
    "\n",
    "class tsLSTM_discriminative():\n",
    "    \n",
    "    def __init__(self, n_lstm_dim, n_steps, n_data_dim, n_lstm_layers, session,\\\n",
    "                 lr, l2, max_norm ):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "        self.L2 =  l2\n",
    "        \n",
    "        self.N_LSTM_LAYERS = n_lstm_layers\n",
    "        self.N_LSTM_DIM    = n_lstm_dim\n",
    "        \n",
    "        self.N_STEPS    = n_steps\n",
    "        self.N_DATA_DIM = n_data_dim\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [None, self.N_STEPS, self.N_DATA_DIM])\n",
    "        self.y = tf.placeholder(tf.float32, [None, 1])\n",
    "        \n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        with tf.variable_scope(\"lstm\"):\n",
    "            \n",
    "#           !!change\n",
    "            lstm_cell    = tf.contrib.rnn.GRUCell(self.N_LSTM_DIM, activation = tf.nn.relu)\n",
    "#           state_is_tuple = True\n",
    "            \n",
    "            stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell]* self.N_LSTM_LAYERS,\\\n",
    "                                                      state_is_tuple=True )\n",
    "            \n",
    "            self.hiddens, self.state = tf.nn.dynamic_rnn(cell = stacked_lstm,\\\n",
    "                                                         inputs = self.x,\\\n",
    "                                                         dtype = tf.float32)\n",
    "            \n",
    "        tmp_hiddens = tf.transpose( self.hiddens, [1,0,2]  )\n",
    "        last_hidden = tmp_hiddens[-1]\n",
    "        \n",
    "        with tf.variable_scope(\"hidden\"):\n",
    "                \n",
    "#           change  orthogonal ini\n",
    "            w = tf.Variable(tf.random_normal([self.N_LSTM_DIM, 128],\\\n",
    "                            stddev=math.sqrt(2.0/self.N_LSTM_DIM)))\n",
    "            b = tf.Variable(tf.zeros( [128] ))\n",
    "            \n",
    "            self.regularization = tf.nn.l2_loss(w)\n",
    "            \n",
    "            h = tf.matmul(last_hidden, w) + b\n",
    "            h = tf.nn.relu( h )\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w = tf.Variable(tf.random_normal([128, 1],\\\n",
    "                            stddev=math.sqrt(2.0/128)))\n",
    "            b = tf.Variable(tf.zeros( [ 1 ] ))\n",
    "            \n",
    "            self.regularization += tf.nn.l2_loss(w)\n",
    "            self.py = tf.matmul(h, w) + b\n",
    "        \n",
    "    \n",
    "    def train_ini(self):\n",
    "        self.cost = tf.nn.l2_loss( self.y - self.py ) + self.L2*self.regularization\n",
    "        \n",
    "        self.optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)  \n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)  \n",
    "#         !! same lr, converge faster\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "        \n",
    "    def train_batch(self, x_batch, y_batch, keep_prob ):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer, self.cost],\\\n",
    "                        feed_dict={self.x:x_batch,\\\n",
    "                                   self.y:y_batch,\\\n",
    "                                   self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "            \n",
    "        return c\n",
    "\n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "\n",
    "#       denormalzied RMSE  \n",
    "        self.rmse = \\\n",
    "        tf.sqrt( tf.reduce_mean( tf.square( self.y - self.py ) ) )\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, x_test, y_test, keep_prob):\n",
    "        return sess.run([self.rmse], feed_dict={self.x:x_test,\\\n",
    "                                                self.y:y_test,\\\n",
    "                                                self.keep_prob:keep_prob\\\n",
    "                                                })\n",
    "        \n",
    "   \n",
    "    def test(self, x_test, y_test ):\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        tmpshape = tf.shape(self.state)\n",
    "        sess.run( self.init )\n",
    "        \n",
    "        return sess.run( [ tmpshape ], \\\n",
    "                 feed_dict={self.x:x_test, self.y:y_test})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# discriminative\n",
    "\n",
    "disc_xtrain = np.reshape( xtrain, [-1, 100, 1] )\n",
    "disc_xtest  = np.reshape( xtest, [-1, 100, 1] )\n",
    "\n",
    "disc_ytrain = ytrain\n",
    "disc_ytest  = ytest\n",
    "\n",
    "print np.shape(disc_xtrain), np.shape(disc_ytrain), np.shape(disc_xtest), \\\n",
    "np.shape(disc_ytest)\n",
    "\n",
    "\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_lr = 0.001\n",
    "para_lstm_dim = 256\n",
    "para_lstm_layers = 1\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_l2 = 0.1\n",
    "\n",
    "para_max_norm  = 4\n",
    "para_keep_prob = 0.8\n",
    "\n",
    "#  fixed parameters\n",
    "para_data_dim = 1\n",
    "para_steps = 100\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    reg = tsLSTM_discriminative( para_lstm_dim, para_steps, para_data_dim, \\\n",
    "                                para_lstm_layers, sess, \\\n",
    "                                para_lr, para_l2, para_max_norm)\n",
    "    \n",
    "    reg.train_ini()\n",
    "    reg.inference_ini()\n",
    "    \n",
    "    total_cnt   = np.shape(disc_xtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx = range(total_cnt)\n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc = 0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "    \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_x = disc_xtrain[ batch_idx ]\n",
    "            batch_y = disc_ytrain[ batch_idx ]            \n",
    "            \n",
    "            tmpc += reg.train_batch( batch_x, batch_y, para_keep_prob,)\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "    \n",
    "        tmp_test_acc  = reg.inference( disc_xtest, disc_ytest,  para_keep_prob) \n",
    "        tmp_train_acc = reg.inference( disc_xtrain,disc_ytrain, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generative\n",
    "\n",
    "def expand_y( x, y ):\n",
    "    cnt = len(x)\n",
    "    expand_y = []\n",
    "    \n",
    "    for i in range(cnt):\n",
    "        tmp = x[i][1:]\n",
    "        tmp = np.append( tmp, y[i][0] )\n",
    "        \n",
    "        expand_y.append( tmp )\n",
    "    \n",
    "    return np.array( expand_y )\n",
    "\n",
    "class tsLSTM_generative():\n",
    "    \n",
    "    def __init__(self, n_lstm_dim, n_steps, n_data_dim, n_lstm_layers, session,\\\n",
    "                 lr, l2, max_norm ):\n",
    "        \n",
    "        self.LEARNING_RATE = lr\n",
    "        self.L2 =  l2\n",
    "        \n",
    "        self.N_LSTM_LAYERS = n_lstm_layers\n",
    "        self.N_LSTM_DIM    = n_lstm_dim\n",
    "        \n",
    "        self.N_STEPS    = n_steps\n",
    "        self.N_DATA_DIM = n_data_dim\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [None, self.N_STEPS, self.N_DATA_DIM])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.N_STEPS])\n",
    "        self.test_y    = tf.placeholder(tf.float32, [None, 1]\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        with tf.variable_scope(\"lstm\"):\n",
    "            \n",
    "            lstm_cell    = tf.contrib.rnn.GRUCell(self.N_LSTM_DIM)\n",
    "#           , state_is_tuple = True\n",
    "            \n",
    "            stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell]* self.N_LSTM_LAYERS,\\\n",
    "                                                      state_is_tuple=True )\n",
    "            \n",
    "            self.hiddens, self.state = tf.nn.dynamic_rnn(cell = stacked_lstm, \\\n",
    "                                                         inputs = self.x,\\\n",
    "                                                    dtype = tf.float32)\n",
    "            \n",
    "        with tf.variable_scope(\"output\"):\n",
    "            \n",
    "            w = tf.Variable( tf.random_normal([self.N_LSTM_DIM, 1],\\\n",
    "                             stddev=math.sqrt(2.0/self.N_LSTM_DIM)) )\n",
    "            \n",
    "            b = tf.Variable(tf.zeros( [ 1 ] ))\n",
    "            \n",
    "            \n",
    "            train_h  =  tf.reshape( self.hiddens, [ -1, self.N_LSTM_DIM ] )\n",
    "            train_py =  tf.matmul( train_h, w ) + b\n",
    "            self.py  =  tf.reshape( train_py, [-1, self.N_STEPS ] ) \n",
    "            \n",
    "            \n",
    "            test_h = tf.transpose( self.hiddens, [1,0,2] )\n",
    "            self.test_py = tf.matmul( test_h[-1], w ) + b\n",
    "        \n",
    "\n",
    "    def train_ini(self):\n",
    "        \n",
    "        self.cost = tf.nn.l2_loss( self.y - self.py )\n",
    "        \n",
    "        self.optimizer = \\\n",
    "        tf.train.AdamOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.RMSPropOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdadeltaOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "#         tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.cost)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        sess.run( self.init )\n",
    "        \n",
    "        \n",
    "    def train_batch(self, x_batch, y_batch, keep_prob ):\n",
    "        \n",
    "        _,c = sess.run([self.optimizer, self.cost],\\\n",
    "                        feed_dict={self.x:x_batch,\\\n",
    "                                   self.y:y_batch,\\\n",
    "                                   self.keep_prob:keep_prob\\\n",
    "                                 })\n",
    "            \n",
    "        return c\n",
    "    \n",
    "#   initialize inference         \n",
    "    def inference_ini(self):\n",
    "\n",
    "#       denormalzied RMSE\n",
    "        self.rmse = tf.sqrt( tf.reduce_mean(\\\n",
    "                             tf.square( self.test_y - self.test_py ) ) )\n",
    "        \n",
    "#   infer givn testing data    \n",
    "    def inference(self, x_test, y_test, keep_prob):\n",
    "        return sess.run([self.rmse], feed_dict={self.x:x_test,\\\n",
    "                                                self.test_y:y_test,\\\n",
    "                                                self.keep_prob:keep_prob\\\n",
    "                                                   })\n",
    "        \n",
    "   \n",
    "    def test(self, x_test, y_test):\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        tmpshape  = tf.shape(self.state)\n",
    "        sess.run( self.init )\n",
    "        \n",
    "        return sess.run( [ tmpshape ], \\\n",
    "                         feed_dict={self.x:x_test, self.y:y_test})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generative\n",
    "\n",
    "gen_ytrain_test =  ytrain\n",
    "gen_ytrain = expand_y( xtrain, ytrain )\n",
    "gen_ytest  = ytest\n",
    "\n",
    "gen_xtrain = np.reshape( xtrain, [-1, 100, 1] )\n",
    "gen_xtest  = np.reshape( xtest, [-1, 100, 1] )\n",
    "\n",
    "print np.shape(gen_xtrain), np.shape(gen_ytrain), np.shape(gen_xtest), np.shape(gen_ytest)\n",
    "\n",
    "\n",
    "\n",
    "para_n_epoch = 1200\n",
    "\n",
    "# tunable parameters\n",
    "\n",
    "#   representation ability\n",
    "para_lr = 0.01\n",
    "para_lstm_dim = 256\n",
    "para_lstm_layers = 1\n",
    "#   regularization\n",
    "para_batch_size = 32\n",
    "para_l2 = 0.1\n",
    "\n",
    "para_max_norm = 4\n",
    "para_keep_prob = 0.8\n",
    "\n",
    "# fixed parameters\n",
    "para_data_dim = 1\n",
    "para_steps = 100\n",
    "\n",
    "# evaluation parameters\n",
    "para_eval_byepoch = 10\n",
    "\n",
    "# clean the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    reg = tsLSTM_generative( para_lstm_dim, para_steps, para_data_dim, \\\n",
    "                                para_lstm_layers, sess, \\\n",
    "                                para_lr, para_l2, para_max_norm)\n",
    "    \n",
    "    reg.train_ini()\n",
    "    reg.inference_ini()\n",
    "    \n",
    "    total_cnt   = np.shape(gen_xtrain)[0]\n",
    "    total_batch = int(total_cnt/para_batch_size)\n",
    "    \n",
    "    total_idx = range(total_cnt)\n",
    "    \n",
    " \n",
    "    \n",
    "#   training cycle\n",
    "    for epoch in range(para_n_epoch):\n",
    "        \n",
    "        tmpc = 0.0\n",
    "        \n",
    "#       shuffle traning instances each epoch  \n",
    "        np.random.shuffle(total_idx)\n",
    "    \n",
    "#       Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_idx = total_idx[ i*para_batch_size: (i+1)*para_batch_size ] \n",
    "            \n",
    "            batch_x = gen_xtrain[ batch_idx ]\n",
    "            batch_y = gen_ytrain[ batch_idx ]            \n",
    "            \n",
    "            tmpc += reg.train_batch( batch_x, batch_y, para_keep_prob,)\n",
    "        \n",
    "        if epoch%para_eval_byepoch != 0:\n",
    "            continue\n",
    "    \n",
    "        tmp_test_acc  = reg.inference( gen_xtest, gen_ytest,       para_keep_prob) \n",
    "        tmp_train_acc = reg.inference( gen_xtrain,gen_ytrain_test, para_keep_prob) \n",
    "        \n",
    "        print \"loss on epoch \", epoch, \" : \", 1.0*tmpc/total_batch, tmp_test_acc,\\\n",
    "        tmp_train_acc\n",
    "    \n",
    "    print \"Optimization Finished!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://github.com/fchollet/keras/issues/2548\n",
    "    \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation, TimeDistributedDense,Dropout,Reshape,Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import *\n",
    "# RMSprop, Adadelta\n",
    "from keras.regularizers import l2, activity_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multi-layer LSTM, seq2Point\n",
    "\n",
    "#  network set-up\n",
    "in_out_neurons = 1\n",
    "hidden_neurons = 200\n",
    "\n",
    "# optimizer \n",
    "learning_rate = 0.005\n",
    "decay_rate = learning_rate / 20\n",
    "momentum = 0.9 # , decay=decay_rate\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, nesterov=True)\n",
    "\n",
    "adam=Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# data preparation parameter \n",
    "seq2seq = False\n",
    "replicate = False\n",
    "\n",
    "\n",
    "x,y = feature_target_build(seq2seq, dur)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_neurons, input_dim=in_out_neurons, return_sequences=True, \\\n",
    "               input_length=win_size  ))\n",
    "# model.add(LSTM(hidden_neurons, input_dim=hidden_neurons, return_sequences=True, \\\n",
    "#                input_length=win_size  ))\n",
    "model.add(LSTM(hidden_neurons, input_dim=hidden_neurons, return_sequences=False, \\\n",
    "               input_length=win_size  ))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"linear\"))\n",
    "\n",
    "\n",
    "# training \n",
    "model.compile(loss=\"mean_squared_error\", optimizer=adam)\n",
    "    \n",
    "(X_train, y_train), (X_test, y_test) = train_test_split(x,y, seq2seq)  # retrieve data\n",
    "\n",
    "print X_train.shape, y_train.shape\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=600, nb_epoch=10)\n",
    "#           validation_split=0.05)\n",
    "test_phase(X_test, y_test, seq2seq)\n",
    "\n",
    "# and maybe plot it\n",
    "# pd.DataFrame(predicted).to_csv(\"predicted.csv\")\n",
    "# pd.DataFrame(y_test).to_csv(\"test_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
