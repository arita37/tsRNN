{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# data processing packages\n",
    "import numpy as np   \n",
    "import pandas as pd \n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    " \n",
    "%matplotlib inline    \n",
    "import matplotlib as mplt\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.style.use('ggplot')\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "\n",
    "# machine leanring packages\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation, Dropout, Reshape, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "# RMSprop, Adadelta\n",
    "from keras.regularizers import *\n",
    "from keras.initializers import *\n",
    "from keras.activations import *\n",
    "\n",
    "from utils_keras import *\n",
    "from utils_dataPrepro import *\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notes:\n",
    "\n",
    "# ---Ini:            \n",
    "#     orthogonal: https://smerity.com/articles/2016/orthogonal_init.html\n",
    "#     identity, Xavier, He's\n",
    "\n",
    "# ---Activation:     \n",
    "#     tan, sigmod by default\n",
    "\n",
    "# ---Regularization: \n",
    "#     dropout on non-recurrent connections \n",
    "#     batch normalization\n",
    "\n",
    "# unstable or not decreasing training loss: lr, representability, e.g., number of neursons, layers, etc..\n",
    "# watch out for the amount difference between regularization and loss  \n",
    "# large minibatch -> large lr\n",
    "# large network -> large lr\n",
    "\n",
    "\n",
    "# learning speed, repren, regular\n",
    "\n",
    "# TO DO:\n",
    "    \n",
    "# more component\n",
    "# muliti-input-muliti-output method\n",
    "# log transformation\n",
    "# integrate trend \n",
    "# attention for peridoic time series\n",
    "\n",
    "# multistep ahead:  multi-output rnn\n",
    "#                   seq2seq rnn \n",
    "\n",
    "# Stat-space model: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3040, 200) (3040, 1) (760, 200) (760, 1)\n"
     ]
    }
   ],
   "source": [
    "# stock data\n",
    "\n",
    "files_list=[\"../dataset/dataset_ts/stock_xtrain.dat\", \\\n",
    "            \"../dataset/dataset_ts/stock_xtest.dat\",\\\n",
    "            \"../dataset/dataset_ts/stock_ytrain.dat\", \\\n",
    "            \"../dataset/dataset_ts/stock_ytest.dat\"]\n",
    "\n",
    "xtrain_df = pd.DataFrame( np.load(files_list[0]) )\n",
    "xtest_df  = pd.DataFrame( np.load(files_list[1]) )\n",
    "ytrain_df = pd.DataFrame( np.load(files_list[2]) )\n",
    "ytest_df  = pd.DataFrame( np.load(files_list[3]) )\n",
    "\n",
    "\n",
    "# files_list=[\"../dataset/dataset_ts/power_xtrain.csv\", \\\n",
    "#             \"../dataset/dataset_ts/power_xtest.csv\",\\\n",
    "#             \"../dataset/dataset_ts/power_ytrain.csv\", \\\n",
    "#             \"../dataset/dataset_ts/power_ytest.csv\"]\n",
    "\n",
    "# xtrain_df = pd.read_csv( files_list[0] ,sep=',', header=None)\n",
    "# xtest_df  = pd.read_csv( files_list[1] ,sep=',', header=None)\n",
    "# ytrain_df = pd.read_csv( files_list[2] ,sep=',', header=None)\n",
    "# ytest_df  = pd.read_csv( files_list[3] ,sep=',', header=None)\n",
    "\n",
    "xtrain, ytrain, xtest, ytest = prepare_train_test_data( 200, False, xtrain_df, xtest_df, ytrain_df, ytest_df)\n",
    "    \n",
    "print np.shape(xtrain), np.shape(ytrain), np.shape(xtest), np.shape(ytest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3040, 200, 1) (3040, 1) (760, 200, 1) (760, 1)\n",
      "Epoch 1/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 1692548.9432\n",
      "Testing loss: 1274001.30658, test_err:1128.7166653, train_err:1134.52304366 \n",
      "\n",
      "3040/3040 [==============================] - 238s - loss: 1670629.6632    \n",
      "Epoch 2/500\n",
      "2816/3040 [==========================>...] - ETA: 10s - loss: 577618.1882\n",
      "Testing loss: 170779.337007, test_err:413.25455469, train_err:408.130061653 \n",
      "\n",
      "3040/3040 [==============================] - 216s - loss: 549117.8102    \n",
      "Epoch 3/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 93828.6282\n",
      "Testing loss: 56495.6588405, test_err:237.688153522, train_err:246.989565854 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 91936.7611    \n",
      "Epoch 4/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 56690.6946\n",
      "Testing loss: 51846.9339227, test_err:227.699219889, train_err:224.548743943 \n",
      "\n",
      "3040/3040 [==============================] - 226s - loss: 55688.9144    \n",
      "Epoch 5/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 49910.5433\n",
      "Testing loss: 48206.0161595, test_err:219.558685799, train_err:224.026274127 \n",
      "\n",
      "3040/3040 [==============================] - 224s - loss: 49886.3930    \n",
      "Epoch 6/500\n",
      "2816/3040 [==========================>...] - ETA: 10s - loss: 50798.1669\n",
      "Testing loss: 51004.1605674, test_err:225.841009962, train_err:236.137836201 \n",
      "\n",
      "3040/3040 [==============================] - 218s - loss: 51434.1927    \n",
      "Epoch 7/500\n",
      "2816/3040 [==========================>...] - ETA: 10s - loss: 64793.1992\n",
      "Testing loss: 56458.3134457, test_err:237.609572897, train_err:249.654592157 \n",
      "\n",
      "3040/3040 [==============================] - 216s - loss: 65188.2971    \n",
      "Epoch 8/500\n",
      "2816/3040 [==========================>...] - ETA: 10s - loss: 71863.8217\n",
      "Testing loss: 78311.0651316, test_err:279.841142722, train_err:292.861521957 \n",
      "\n",
      "3040/3040 [==============================] - 218s - loss: 71799.5103    \n",
      "Epoch 9/500\n",
      "2816/3040 [==========================>...] - ETA: 10s - loss: 81830.1342\n",
      "Testing loss: 70895.6825658, test_err:266.262425299, train_err:279.919904416 \n",
      "\n",
      "3040/3040 [==============================] - 213s - loss: 82223.9578    \n",
      "Epoch 10/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 74921.0110\n",
      "Testing loss: 58966.7945313, test_err:242.83079866, train_err:255.226393942 \n",
      "\n",
      "3040/3040 [==============================] - 221s - loss: 74250.5961    \n",
      "Epoch 11/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 56946.6744\n",
      "Testing loss: 40867.1847451, test_err:202.156349526, train_err:212.248130214 \n",
      "\n",
      "3040/3040 [==============================] - 239s - loss: 55798.4954    \n",
      "Epoch 12/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 61293.6461\n",
      "Testing loss: 25243.6459498, test_err:158.882490736, train_err:167.556436959 \n",
      "\n",
      "3040/3040 [==============================] - 262s - loss: 61687.7296    \n",
      "Epoch 13/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 33416.6999\n",
      "Testing loss: 13630.8905325, test_err:116.751421503, train_err:121.290209795 \n",
      "\n",
      "3040/3040 [==============================] - 262s - loss: 32196.0400    \n",
      "Epoch 14/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 9081.5775\n",
      "Testing loss: 3104.67259971, test_err:55.7196081051, train_err:58.587057687 \n",
      "\n",
      "3040/3040 [==============================] - 254s - loss: 8743.1926    \n",
      "Epoch 15/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 1758.7204\n",
      "Testing loss: 686.957874178, test_err:26.2098899162, train_err:27.6792614576 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 1693.5572    \n",
      "Epoch 16/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 632.8336\n",
      "Testing loss: 579.187985711, test_err:24.0663087391, train_err:24.5565357417 \n",
      "\n",
      "3040/3040 [==============================] - 259s - loss: 618.2510    \n",
      "Epoch 17/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 447.5005\n",
      "Testing loss: 340.703533293, test_err:18.4581524077, train_err:19.9240643771 \n",
      "\n",
      "3040/3040 [==============================] - 263s - loss: 441.3128    \n",
      "Epoch 18/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 382.0725\n",
      "Testing loss: 341.403486392, test_err:18.4770960118, train_err:19.638733139 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 381.2650    \n",
      "Epoch 19/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 396.4724\n",
      "Testing loss: 336.856852963, test_err:18.3536620326, train_err:19.3194968205 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 392.6052    \n",
      "Epoch 20/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 360.3428\n",
      "Testing loss: 300.360037392, test_err:17.3309079476, train_err:18.5202595949 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 361.8175    \n",
      "Epoch 21/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 358.8409\n",
      "Testing loss: 293.676522827, test_err:17.1369947837, train_err:18.5751374238 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 358.8745    \n",
      "Epoch 22/500\n",
      "2816/3040 [==========================>...] - ETA: 14s - loss: 370.6037\n",
      "Testing loss: 340.708367277, test_err:18.4582795798, train_err:19.8246469958 \n",
      "\n",
      "3040/3040 [==============================] - 316s - loss: 364.5654    \n",
      "Epoch 23/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 371.8998\n",
      "Testing loss: 297.516356619, test_err:17.2486502153, train_err:18.6642209509 \n",
      "\n",
      "3040/3040 [==============================] - 256s - loss: 364.2591    \n",
      "Epoch 24/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 348.4544\n",
      "Testing loss: 302.473144049, test_err:17.391762412, train_err:18.8954536892 \n",
      "\n",
      "3040/3040 [==============================] - 247s - loss: 346.5957    \n",
      "Epoch 25/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 348.6552\n",
      "Testing loss: 275.510203151, test_err:16.5985002127, train_err:17.9856424271 \n",
      "\n",
      "3040/3040 [==============================] - 249s - loss: 344.1412    \n",
      "Epoch 26/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 320.4709\n",
      "Testing loss: 270.442692004, test_err:16.4451443588, train_err:17.7395431133 \n",
      "\n",
      "3040/3040 [==============================] - 246s - loss: 325.3525    \n",
      "Epoch 27/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 317.2515\n",
      "Testing loss: 315.641042609, test_err:17.7662696164, train_err:19.1790962751 \n",
      "\n",
      "3040/3040 [==============================] - 247s - loss: 323.5113    \n",
      "Epoch 28/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 333.4323\n",
      "Testing loss: 270.093091302, test_err:16.434514549, train_err:17.8477787017 \n",
      "\n",
      "3040/3040 [==============================] - 248s - loss: 330.8723    \n",
      "Epoch 29/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 327.6919\n",
      "Testing loss: 293.345355706, test_err:17.1273289348, train_err:18.5104208889 \n",
      "\n",
      "3040/3040 [==============================] - 245s - loss: 331.3874    \n",
      "Epoch 30/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 319.9462\n",
      "Testing loss: 268.978842806, test_err:16.4005759141, train_err:17.7608538274 \n",
      "\n",
      "3040/3040 [==============================] - 245s - loss: 317.5853    \n",
      "Epoch 31/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 319.2015\n",
      "Testing loss: 271.450907336, test_err:16.4757634274, train_err:17.73400733 \n",
      "\n",
      "3040/3040 [==============================] - 245s - loss: 319.5326    \n",
      "Epoch 32/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 316.9760\n",
      "Testing loss: 272.506402427, test_err:16.5077793854, train_err:17.6043482255 \n",
      "\n",
      "3040/3040 [==============================] - 244s - loss: 310.6991    \n",
      "Epoch 33/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 315.5329\n",
      "Testing loss: 261.889252994, test_err:16.1829928295, train_err:17.4351874594 \n",
      "\n",
      "3040/3040 [==============================] - 243s - loss: 320.8214    \n",
      "Epoch 34/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 316.6218\n",
      "Testing loss: 258.320943411, test_err:16.0723673608, train_err:17.3628512556 \n",
      "\n",
      "3040/3040 [==============================] - 242s - loss: 314.0412    \n",
      "Epoch 35/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 296.4598\n",
      "Testing loss: 356.231642231, test_err:18.8740928431, train_err:19.6556726104 \n",
      "\n",
      "3040/3040 [==============================] - 242s - loss: 300.6423    \n",
      "Epoch 36/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 327.6816\n",
      "Testing loss: 264.045454326, test_err:16.2494774672, train_err:17.4840357931 \n",
      "\n",
      "3040/3040 [==============================] - 241s - loss: 326.0461    \n",
      "Epoch 37/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 325.2726\n",
      "Testing loss: 302.336480552, test_err:17.3878088961, train_err:18.6845229414 \n",
      "\n",
      "3040/3040 [==============================] - 241s - loss: 325.1007    \n",
      "Epoch 38/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 320.6729\n",
      "Testing loss: 265.976442839, test_err:16.3087713427, train_err:17.6506384589 \n",
      "\n",
      "3040/3040 [==============================] - 239s - loss: 324.7185    \n",
      "Epoch 39/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 300.2908\n",
      "Testing loss: 249.372314132, test_err:15.7915292121, train_err:17.0095576119 \n",
      "\n",
      "3040/3040 [==============================] - 239s - loss: 302.5138    \n",
      "Epoch 40/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 314.4506\n",
      "Testing loss: 350.977474494, test_err:18.7343693626, train_err:19.4895718274 \n",
      "\n",
      "3040/3040 [==============================] - 240s - loss: 316.5998    \n",
      "Epoch 41/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 361.2545\n",
      "Testing loss: 296.971144024, test_err:17.2328598399, train_err:18.5186294116 \n",
      "\n",
      "3040/3040 [==============================] - 239s - loss: 371.4470    \n",
      "Epoch 42/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 316.9725\n",
      "Testing loss: 287.317977423, test_err:16.9504471683, train_err:18.0728004667 \n",
      "\n",
      "3040/3040 [==============================] - 240s - loss: 318.9074    \n",
      "Epoch 43/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 350.5060\n",
      "Testing loss: 332.247406809, test_err:18.2276465271, train_err:19.465971547 \n",
      "\n",
      "3040/3040 [==============================] - 239s - loss: 347.2694    \n",
      "Epoch 44/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 338.4307\n",
      "Testing loss: 278.776427259, test_err:16.6965875372, train_err:17.6787180425 \n",
      "\n",
      "3040/3040 [==============================] - 241s - loss: 341.9740    \n",
      "Epoch 45/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 305.2902\n",
      "Testing loss: 272.806606092, test_err:16.5168618015, train_err:17.8383204742 \n",
      "\n",
      "3040/3040 [==============================] - 242s - loss: 308.8174    \n",
      "Epoch 46/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 302.7911\n",
      "Testing loss: 298.698898476, test_err:17.2829032808, train_err:18.249950756 \n",
      "\n",
      "3040/3040 [==============================] - 245s - loss: 304.9058    \n",
      "Epoch 47/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 326.7969\n",
      "Testing loss: 317.916178011, test_err:17.8301906569, train_err:19.1142816466 \n",
      "\n",
      "3040/3040 [==============================] - 251s - loss: 328.0458    \n",
      "Epoch 48/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 308.0563\n",
      "Testing loss: 253.068229595, test_err:15.9081241017, train_err:16.9829820284 \n",
      "\n",
      "3040/3040 [==============================] - 253s - loss: 306.9351    \n",
      "Epoch 49/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 300.5569\n",
      "Testing loss: 250.791484311, test_err:15.8364045789, train_err:16.9746370026 \n",
      "\n",
      "3040/3040 [==============================] - 240s - loss: 294.6446    \n",
      "Epoch 50/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 310.6704\n",
      "Testing loss: 241.706154432, test_err:15.5468974996, train_err:16.7104871813 \n",
      "\n",
      "3040/3040 [==============================] - 245s - loss: 319.4418    \n",
      "Epoch 51/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 290.6121\n",
      "Testing loss: 271.764120162, test_err:16.4852803493, train_err:17.7090773269 \n",
      "\n",
      "3040/3040 [==============================] - 245s - loss: 287.9890    \n",
      "Epoch 52/500\n",
      "2816/3040 [==========================>...] - ETA: 15s - loss: 288.2199\n",
      "Testing loss: 261.091080194, test_err:16.1583192063, train_err:17.353957389 \n",
      "\n",
      "3040/3040 [==============================] - 338s - loss: 291.6590    \n",
      "Epoch 53/500\n",
      "2816/3040 [==========================>...] - ETA: 17s - loss: 308.7866\n",
      "Testing loss: 301.659586696, test_err:17.3683393587, train_err:18.2318179145 \n",
      "\n",
      "3040/3040 [==============================] - 338s - loss: 309.5412    \n",
      "Epoch 54/500\n",
      "2816/3040 [==========================>...] - ETA: 15s - loss: 315.5919\n",
      "Testing loss: 326.355602064, test_err:18.065313925, train_err:19.2975000366 \n",
      "\n",
      "3040/3040 [==============================] - 303s - loss: 310.0220    \n",
      "Epoch 55/500\n",
      "2816/3040 [==========================>...] - ETA: 17s - loss: 307.3914\n",
      "Testing loss: 236.646093429, test_err:15.383305927, train_err:16.5116706974 \n",
      "\n",
      "3040/3040 [==============================] - 392s - loss: 305.0727    \n",
      "Epoch 56/500\n",
      "2816/3040 [==========================>...] - ETA: 17s - loss: 300.2018\n",
      "Testing loss: 243.460392681, test_err:15.6032195263, train_err:16.7571065834 \n",
      "\n",
      "3040/3040 [==============================] - 382s - loss: 308.4647    \n",
      "Epoch 57/500\n",
      "2816/3040 [==========================>...] - ETA: 16s - loss: 330.8917\n",
      "Testing loss: 268.09029284, test_err:16.3734567008, train_err:17.600025807 \n",
      "\n",
      "3040/3040 [==============================] - 352s - loss: 326.8484    \n",
      "Epoch 58/500\n",
      "2816/3040 [==========================>...] - ETA: 21s - loss: 282.4741\n",
      "Testing loss: 234.040931139, test_err:15.2983965749, train_err:16.4890839295 \n",
      "\n",
      "3040/3040 [==============================] - 429s - loss: 287.1842    \n",
      "Epoch 59/500\n",
      "2816/3040 [==========================>...] - ETA: 23s - loss: 278.8339\n",
      "Testing loss: 242.213474314, test_err:15.563205915, train_err:16.7490070674 \n",
      "\n",
      "3040/3040 [==============================] - 422s - loss: 279.0070    \n",
      "Epoch 60/500\n",
      "2816/3040 [==========================>...] - ETA: 17s - loss: 283.7250\n",
      "Testing loss: 235.719530768, test_err:15.3531651192, train_err:16.406863545 \n",
      "\n",
      "3040/3040 [==============================] - 358s - loss: 283.1107    \n",
      "Epoch 61/500\n",
      "2816/3040 [==========================>...] - ETA: 20s - loss: 302.2320\n",
      "Testing loss: 239.949089934, test_err:15.4902886475, train_err:16.5838680775 \n",
      "\n",
      "3040/3040 [==============================] - 434s - loss: 300.6438    \n",
      "Epoch 62/500\n",
      "2816/3040 [==========================>...] - ETA: 22s - loss: 281.8936\n",
      "Testing loss: 266.127594476, test_err:16.3134355442, train_err:17.2591719378 \n",
      "\n",
      "3040/3040 [==============================] - 465s - loss: 283.1522    \n",
      "Epoch 63/500\n",
      "2816/3040 [==========================>...] - ETA: 14s - loss: 313.2643\n",
      "Testing loss: 263.460214394, test_err:16.2314429937, train_err:17.290952373 \n",
      "\n",
      "3040/3040 [==============================] - 301s - loss: 311.6946    \n",
      "Epoch 64/500\n",
      "2816/3040 [==========================>...] - ETA: 16s - loss: 290.1951\n",
      "Testing loss: 240.107424927, test_err:15.4953925524, train_err:16.465816801 \n",
      "\n",
      "3040/3040 [==============================] - 347s - loss: 298.6897    \n",
      "Epoch 65/500\n",
      "2816/3040 [==========================>...] - ETA: 18s - loss: 379.1737\n",
      "Testing loss: 268.947269962, test_err:16.3996196811, train_err:17.6082806913 \n",
      "\n",
      "3040/3040 [==============================] - 419s - loss: 373.5812    \n",
      "Epoch 66/500\n",
      "2816/3040 [==========================>...] - ETA: 18s - loss: 294.0103\n",
      "Testing loss: 265.448193038, test_err:16.2925705367, train_err:17.3271394173 \n",
      "\n",
      "3040/3040 [==============================] - 351s - loss: 290.0542    \n",
      "Epoch 67/500\n",
      "2816/3040 [==========================>...] - ETA: 16s - loss: 318.2517\n",
      "Testing loss: 419.583224005, test_err:20.4837039462, train_err:21.0473439238 \n",
      "\n",
      "3040/3040 [==============================] - 340s - loss: 325.4650    \n",
      "Epoch 68/500\n",
      "2816/3040 [==========================>...] - ETA: 17s - loss: 319.1539\n",
      "Testing loss: 232.514609889, test_err:15.248431278, train_err:16.506182407 \n",
      "\n",
      "3040/3040 [==============================] - 357s - loss: 314.0335    \n",
      "Epoch 69/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 286.1419\n",
      "Testing loss: 295.992332699, test_err:17.2044140468, train_err:18.0185936559 \n",
      "\n",
      "3040/3040 [==============================] - 250s - loss: 286.4080    \n",
      "Epoch 70/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 276.0303\n",
      "Testing loss: 255.782144326, test_err:15.9931954067, train_err:17.176638128 \n",
      "\n",
      "3040/3040 [==============================] - 239s - loss: 280.5886    \n",
      "Epoch 71/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 303.1132\n",
      "Testing loss: 228.082028038, test_err:15.1023857191, train_err:16.2028487298 \n",
      "\n",
      "3040/3040 [==============================] - 234s - loss: 304.6370    \n",
      "Epoch 72/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 350.9553\n",
      "Testing loss: 597.563200941, test_err:24.445084502, train_err:25.0349576551 \n",
      "\n",
      "3040/3040 [==============================] - 239s - loss: 346.1269    \n",
      "Epoch 73/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 447.2020\n",
      "Testing loss: 307.357423481, test_err:17.5316017939, train_err:18.2132091687 \n",
      "\n",
      "3040/3040 [==============================] - 238s - loss: 441.4635    \n",
      "Epoch 74/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 287.6760\n",
      "Testing loss: 250.574413741, test_err:15.8295424607, train_err:16.7923824691 \n",
      "\n",
      "3040/3040 [==============================] - 240s - loss: 286.5944    \n",
      "Epoch 75/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 269.3803\n",
      "Testing loss: 302.914442524, test_err:17.4044314473, train_err:18.2312088169 \n",
      "\n",
      "3040/3040 [==============================] - 246s - loss: 270.0354    \n",
      "Epoch 76/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 321.6463\n",
      "Testing loss: 275.084371466, test_err:16.5856652526, train_err:17.6047408688 \n",
      "\n",
      "3040/3040 [==============================] - 240s - loss: 319.3184    \n",
      "Epoch 77/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 324.1215\n",
      "Testing loss: 235.136047685, test_err:15.3341479521, train_err:16.3671643467 \n",
      "\n",
      "3040/3040 [==============================] - 244s - loss: 331.4238    \n",
      "Epoch 78/500\n",
      "2816/3040 [==========================>...] - ETA: 13s - loss: 279.6389\n",
      "Testing loss: 277.77174273, test_err:16.6664753471, train_err:17.7921736445 \n",
      "\n",
      "3040/3040 [==============================] - 255s - loss: 280.7824    \n",
      "Epoch 79/500\n",
      "2816/3040 [==========================>...] - ETA: 12s - loss: 290.2765\n",
      "Testing loss: 234.508824238, test_err:15.3136755329, train_err:16.4507474955 \n",
      "\n",
      "3040/3040 [==============================] - 241s - loss: 295.7295    \n",
      "Epoch 80/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 295.8880\n",
      "Testing loss: 327.062494218, test_err:18.084876491, train_err:18.8479456491 \n",
      "\n",
      "3040/3040 [==============================] - 238s - loss: 291.6832    \n",
      "Epoch 81/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 324.5666\n",
      "Testing loss: 229.863500013, test_err:15.1612546615, train_err:16.1953209049 \n",
      "\n",
      "3040/3040 [==============================] - 236s - loss: 322.2859    \n",
      "Epoch 82/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 266.8899\n",
      "Testing loss: 224.133745053, test_err:14.971095295, train_err:16.0541713255 \n",
      "\n",
      "3040/3040 [==============================] - 237s - loss: 271.6040    \n",
      "Epoch 83/500\n",
      "2816/3040 [==========================>...] - ETA: 11s - loss: 283.2018\n",
      "Testing loss: 334.336737382, test_err:18.2848503456, train_err:19.3408146195 \n",
      "\n",
      "3040/3040 [==============================] - 359s - loss: 289.7786    \n",
      "Epoch 84/500\n",
      "2816/3040 [==========================>...] - ETA: 24s - loss: 340.3971\n",
      "Testing loss: 244.463683118, test_err:15.6353216218, train_err:16.7865409587 \n",
      "\n",
      "3040/3040 [==============================] - 556s - loss: 346.8395    \n",
      "Epoch 85/500\n",
      "2816/3040 [==========================>...] - ETA: 22s - loss: 314.0639\n",
      "Testing loss: 382.401296836, test_err:19.5551015009, train_err:20.1059184526 \n",
      "\n",
      "3040/3040 [==============================] - 407s - loss: 310.5340    \n",
      "Epoch 86/500\n",
      "2816/3040 [==========================>...] - ETA: 19s - loss: 320.7493\n",
      "Testing loss: 312.677904952, test_err:17.6827144457, train_err:18.4565227944 \n",
      "\n",
      "3040/3040 [==============================] - 421s - loss: 321.0826    \n",
      "Epoch 87/500\n",
      "2816/3040 [==========================>...] - ETA: 19s - loss: 285.4413\n",
      "Testing loss: 231.890909456, test_err:15.2279550484, train_err:16.4149592189 \n",
      "\n",
      "3040/3040 [==============================] - 364s - loss: 285.3717    \n",
      "Epoch 88/500\n",
      "2816/3040 [==========================>...] - ETA: 16s - loss: 272.1994"
     ]
    }
   ],
   "source": [
    "# plain discriminative \n",
    "\n",
    "# network set-up\n",
    "in_out_neurons = 1\n",
    "hidden_neurons = 256\n",
    "win_size = 200\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "disc_xtrain = np.reshape( xtrain, [-1, win_size, in_out_neurons] )\n",
    "disc_xtest  = np.reshape( xtest,  [-1, win_size, in_out_neurons] )\n",
    "\n",
    "disc_ytrain = ytrain\n",
    "disc_ytest  = ytest\n",
    "\n",
    "print np.shape(disc_xtrain),np.shape(disc_ytrain),np.shape(disc_xtest),\\\n",
    "np.shape(disc_ytest)\n",
    "\n",
    "# optimizer\n",
    "sgd  = SGD(lr = 0.01, momentum = 0.9, nesterov = True)\n",
    "rms  = RMSprop(lr = 0.05,  rho = 0.9, epsilon  = 1e-08, decay = 0.0)\n",
    "\n",
    "\n",
    "adam = Adam(lr = 0.005, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add( LSTM(hidden_neurons, input_dim = in_out_neurons, return_sequences = False, \\\n",
    "                input_shape = [batch_size, win_size, in_out_neurons ], \\\n",
    "#               input_length = win_size, \\\n",
    "#               activation ='tanh',\\\n",
    "#               dropout = 0.1,\\\n",
    "#               kernel_regularizer = l2(0.2), \n",
    "#               recurrent_regularizer = l2(0.1),\\\n",
    "#               !change!\n",
    "                kernel_initializer    = glorot_normal(), \\\n",
    "                recurrent_initializer = glorot_normal() ))\n",
    "# change: activiation\n",
    "\n",
    "# model.add( LSTM(hidden_neurons, return_sequences = False, \\\n",
    "# #               input_shape = [batch_size, win_size, in_out_neurons ], \\\n",
    "# #               input_length = win_size, \\\n",
    "#               activation ='tanh',\\\n",
    "# #               dropout = 0.1,\\\n",
    "# #               kernel_regularizer = l2(0.2), \n",
    "# #               recurrent_regularizer = l2(0.1),\\\n",
    "# #               !change!\n",
    "#                 kernel_initializer    = glorot_normal(), \\\n",
    "#                 recurrent_initializer = glorot_normal() ))\n",
    "\n",
    "\n",
    "# identity ini\n",
    "# he's ini:   he_normal()\n",
    "# xavier ini: glorot_normal()\n",
    "# orthogonal: Orthogonal()\n",
    "\n",
    "\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(128,\\\n",
    "#                   kernel_regularizer = l2(0.01), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(64, \\\n",
    "#                   kernel_regularizer = l2(0.01), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(32, \\\n",
    "#                   kernel_regularizer = l2(0.01), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(1,  \\\n",
    "#                   kernel_regularizer = l2(0.01), \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"linear\"))\n",
    "\n",
    "\n",
    "# training\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = adam)\n",
    "\n",
    "\n",
    "model.fit( disc_xtrain, disc_ytrain, shuffle = True, \\\n",
    "           callbacks = [ \\\n",
    "           TestCallback( (disc_xtest, disc_ytest), (disc_xtrain, disc_ytrain) ) ], \\\n",
    "           batch_size = batch_size, epochs = 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generative model\n",
    "\n",
    "# set-up\n",
    "in_out_neurons = 1\n",
    "hidden_neurons = 256\n",
    "win_size = 200    \n",
    "    \n",
    "# prepare data\n",
    "ytrain = expand_y( xtrain_df.as_matrix(), ytrain_df.as_matrix() )\n",
    "ytest  = expand_y( xtest_df.as_matrix(),  ytest_df.as_matrix()  )\n",
    "\n",
    "gen_xtrain = np.reshape( xtrain, [-1, win_size, 1] )\n",
    "gen_ytrain = np.reshape( ytrain, [-1, win_size, 1] )\n",
    "\n",
    "gen_xtest  = np.reshape( xtest, [-1, win_size, 1] )\n",
    "gen_ytest  = np.reshape( ytest, [-1, win_size, 1] )\n",
    "\n",
    "print np.shape(gen_xtrain), np.shape(gen_ytrain), np.shape(gen_xtest), np.shape(gen_ytest)\n",
    "\n",
    "\n",
    "# optimizer \n",
    "adam  = Adam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add( LSTM(hidden_neurons, input_dim = in_out_neurons, return_sequences = True, \\\n",
    "                input_length = win_size,\\\n",
    "                activation   = 'tanh',\\\n",
    "#                dropout = 0.1,\\\n",
    "#                kernel_regularizer = l2(0.15), \\\n",
    "#                recurrent_regularizer = l2(0.15), \\\n",
    "                kernel_initializer    = glorot_normal(), \\\n",
    "                recurrent_initializer = glorot_normal() ) )\n",
    "\n",
    "\n",
    "# identity ini\n",
    "# he's ini:   he_normal()\n",
    "# xavier ini: glorot_normal()\n",
    "# orthogonal: Orthogonal() \n",
    "\n",
    "\n",
    "# model.add(Dropout(0.1))\n",
    "model.add(TimeDistributed(Dense(128, activation = 'relu',\\\n",
    "#                                     kernel_regularizer = l2(0.1), \\\n",
    "                                    kernel_initializer = he_normal() ) ))\n",
    "\n",
    "model.add(TimeDistributed(Dense(64, activation = 'relu',\\\n",
    "#                                     kernel_regularizer = l2(0.01), \\\n",
    "                                    kernel_initializer = he_normal() ) ))\n",
    "\n",
    "model.add(TimeDistributed(Dense(1,  activation = 'linear',\\\n",
    "#                                     kernel_regularizer = l2(0.01), \\\n",
    "                                    kernel_initializer = he_normal() ) ))\n",
    "\n",
    "\n",
    "# training\n",
    "model.compile( loss = \"mean_squared_error\", optimizer = adam )\n",
    "\n",
    "\n",
    "model.fit( gen_xtrain, gen_ytrain, shuffle=True,  \\\n",
    "           callbacks = [ TestCallback_Generative( \\\n",
    "                         (gen_xtest, gen_ytest), (gen_xtrain, gen_ytrain) ) ], \\\n",
    "                         batch_size = 128, epochs = 500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# discriminative with non-overlapping local data\n",
    "\n",
    "#  network set-up\n",
    "local_size = 5\n",
    "in_out_neurons = local_size\n",
    "hidden_neurons = 512\n",
    "win_size = 200/5\n",
    "\n",
    "# # validation on each epoch \n",
    "# class TestCallback_insight(Callback):\n",
    "#     def __init__(self, test_data):\n",
    "#         self.test_data = test_data\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         x, y = self.test_data\n",
    "#         loss, mse = self.model.evaluate(x, y, verbose=0)\n",
    "        \n",
    "#         py = self.model.predict(x, verbose=0)\n",
    "#         cnt = len(y)\n",
    "#         err = [ (y[i][0]-py[i][0])**2 for i in range(cnt)]\n",
    "        \n",
    "#         print('\\nTesting loss: {}, acc: {}, mean: {} \\n'.format(\\\n",
    "#                loss, sqrt(mse), mean(err) ))\n",
    "\n",
    "def expand_x_local_non_overlapping( local_size, list_data):\n",
    "    \n",
    "    cnt = len(list_data)\n",
    "    steps = len(list_data[0])\n",
    "    tmp_dta = []\n",
    "    \n",
    "    for i in range(cnt):\n",
    "        tmp_dta.append([])\n",
    "        for j in range( local_size-1, steps, local_size ):\n",
    "            tmp_dta[-1].append( list_data[i][ j-local_size+1:j+1 ] )\n",
    "    \n",
    "    return tmp_dta\n",
    "\n",
    "\n",
    "disc_xtrain = np.array( expand_x_local_non_overlapping( local_size, list(xtrain) ) )\n",
    "disc_xtest  = np.array( expand_x_local_non_overlapping( local_size, list(xtest) ) )\n",
    "\n",
    "disc_ytrain = ytrain\n",
    "disc_ytest  = ytest\n",
    "\n",
    "print np.shape(disc_xtrain), np.shape(disc_ytrain), np.shape(disc_xtest), \\\n",
    "np.shape(disc_ytest)\n",
    "\n",
    "# optimizer \n",
    "adam = Adam(lr = 0.007, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_neurons, input_dim = in_out_neurons, return_sequences = False, \\\n",
    "               input_length = win_size, \\\n",
    "               activation = 'tanh', \\\n",
    "#                dropout = 0.1,\\\n",
    "#                kernel_regularizer = l2(0.1),      recurrent_regularizer = l2(0.1), \\\n",
    "               kernel_initializer    = glorot_normal(), \\\n",
    "               recurrent_initializer = glorot_normal() ))\n",
    "# identity ini\n",
    "# he's ini:   he_normal()\n",
    "# xavier ini: glorot_normal()\n",
    "# orthogonal: Orthogonal() \n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# kernel_regularizer = l2(0.1), \\\n",
    "model.add(Dense(128,\\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(64, \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(32, \\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(1,\\\n",
    "                    kernel_initializer = he_normal()))\n",
    "model.add(Activation(\"linear\"))\n",
    "\n",
    "\n",
    "# training \n",
    "model.compile(loss=\"mean_squared_error\", optimizer = adam)\n",
    "\n",
    "\n",
    "model.fit( disc_xtrain, disc_ytrain, shuffle=True, \\\n",
    "           callbacks = [ TestCallback( (disc_xtest, disc_ytest), (disc_xtrain, disc_ytrain) ) ], \\\n",
    "           batch_size = 256, epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.merge import *\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "in_out_neurons = 1\n",
    "hidden_neurons = 256\n",
    "win_size = 200\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "input_raw  = Input(shape = ( win_size, 1 ),    dtype='float32', name='input_raw')\n",
    "input_diff = Input(shape = ( win_size -1, 1 ), dtype='float32', name='input_diff')\n",
    "input_last = Input(shape = ( 1, ), dtype='float32', name='input_last')\n",
    "\n",
    "hidden_raw  = LSTM(32)( input_raw )\n",
    "output_raw  = Dense(1, activation='relu', name='aux_output')(hidden_raw)\n",
    "\n",
    "hidden_diff = LSTM(32)( input_diff )\n",
    "output_diff = Dense(1, activation='relu', name='aux_output')(hidden_diff)\n",
    "output_diff = Add( [output_diff, input_last] )\n",
    "\n",
    "attention_diff = Add( [output_diff, -1*output_raw] )\n",
    "attention_prob = Dense(1, activation='sigmoid')( attention_diff )\n",
    "attention_diff = merge([output_diff, attention_prob], name='attention_diff', mode='mul')\n",
    "\n",
    "\n",
    "tmp_batch_sizee = int(input_raw.shape[0])\n",
    "ones = K.ones((tmp_batch_sizee,1))\n",
    "\n",
    "attention_prob = Add( [one, -1*attention_prob] )\n",
    "attention_raw  = merge([output_raw, attention_prob], name='attention_raw', mode='mul')\n",
    "\n",
    "# x = keras.layers.concatenate([lstm_out, auxiliary_input])\n",
    "# x = Dense(64, activation='relu')(x)\n",
    "\n",
    "output_main = Add( [attention_diff, attention_raw] )\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "# main_output = Dense(1, activation='sigmoid', name='main_output')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=[ input_raw, input_diff, input_last], outputs=[ output_main ])\n",
    "\n",
    "model.compile(optimizer = adam, loss='mean_squared_error',\n",
    "#               loss_weights=[1., 0.2]\\\n",
    "             )\n",
    "\n",
    "model.fit([headline_data, additional_data], [labels], epochs = 500, batch_size = batch_size )\n",
    "\n",
    "\n",
    "\n",
    "# def attention_3d_block(inputs):\n",
    "#     # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "#     input_dim = int(inputs.shape[2])\n",
    "#     a = Permute((2, 1))(inputs)\n",
    "#     a = Reshape((input_dim, TIME_STEPS))(a)\n",
    "#     a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "#     if SINGLE_ATTENTION_VECTOR:\n",
    "#         a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "#         a = RepeatVector(input_dim)(a)\n",
    "#     a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "#     output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "#     return output_attention_mul\n",
    "\n",
    "\n",
    "# def model_attention_applied_after_lstm():\n",
    "#     inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "#     lstm_units = 32\n",
    "#     lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "#     attention_mul = attention_3d_block(lstm_out)\n",
    "#     attention_mul = Flatten()(attention_mul)\n",
    "#     output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "#     model = Model(input=[inputs], output=output)\n",
    "#     return model\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
